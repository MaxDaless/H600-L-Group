{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled Dataset exploration, meta-data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder configuration\n",
    "\n",
    "Run the bash script in order to check if the folder and the data are well configured. If you get an error, please follow the instructions or run the script download_metadata.sh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ READY TO START -----------\r\n"
     ]
    }
   ],
   "source": [
    "# Check if the files and forlder are on the good folder to exectue the following code.\n",
    "!./check_files.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute basic statistics about the number of files in this sub-dataset, their size, and the number of records (lines) in each file. For length and number of records, give the min, max, mean, 25, 50, 75, 90 percentiles values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of green tripdata files:\n",
      "76\n",
      "count of yellow tripdata files:\n",
      "131\n",
      "count of fhv tripdata files:\n",
      "64\n",
      "count of fhvhv tripdata files:\n",
      "10\n",
      "count of all tripdata files:\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "print ('count of green tripdata files:')\n",
    "!find data/sampled/green_tripdata_*.csv -type f | wc -l \n",
    "print ('count of yellow tripdata files:')\n",
    "!find data/sampled/yellow_tripdata_*.csv -type f | wc -l\n",
    "print ('count of fhv tripdata files:')\n",
    "!find data/sampled/fhv_tripdata_*.csv -type f | wc -l \n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find data/sampled/fhvhv_tripdata_*.csv -type f | wc -l\n",
    "print ('count of all tripdata files:')\n",
    "!find data/sampled/*.csv -type f | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files stats in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the file type called yellow there are 131 files.\n",
      "     Here are some metrics based on their size in bytes:\n",
      "       -min value           :  43103\n",
      "       -max value           :  5959352\n",
      "       -mean value          :  3750759.69\n",
      "       -25 percentile value :  1756967.0\n",
      "       -50 percentile value :  4442047.0\n",
      "       -75 percentile value :  5123591.5\n",
      "       -90 percentile value :  5491438.0\n",
      "       -total file sum size :  491349519\n",
      "For the file type called green there are 76 files.\n",
      "     Here are some metrics based on their size in bytes:\n",
      "       -min value           :  2512\n",
      "       -max value           :  570765\n",
      "       -mean value          :  262437.29\n",
      "       -25 percentile value :  121494.75\n",
      "       -50 percentile value :  190194.5\n",
      "       -75 percentile value :  456955.0\n",
      "       -90 percentile value :  499751.0\n",
      "       -total file sum size :  19945234\n",
      "For the file type called fhv there are 74 files.\n",
      "     Here are some metrics based on their size in bytes:\n",
      "       -min value           :  52060\n",
      "       -max value           :  3339455\n",
      "       -mean value          :  1263540.66\n",
      "       -25 percentile value :  239082.25\n",
      "       -50 percentile value :  725289.0\n",
      "       -75 percentile value :  2545631.75\n",
      "       -90 percentile value :  3003289.6\n",
      "       -total file sum size :  93502009\n",
      "For the file type called fhvhv there are 10 files.\n",
      "     Here are some metrics based on their size in bytes:\n",
      "       -min value           :  535789\n",
      "       -max value           :  2978931\n",
      "       -mean value          :  2007750.7\n",
      "       -25 percentile value :  1121047.25\n",
      "       -50 percentile value :  2542280.0\n",
      "       -75 percentile value :  2687857.25\n",
      "       -90 percentile value :  2804332.8\n",
      "       -total file sum size :  20077507\n"
     ]
    }
   ],
   "source": [
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    list_files = {}\n",
    "    list_files[taxi_brand] = []\n",
    "    nb_files = 0\n",
    "    size_b = []\n",
    "    # List the file from the same taxi company brand \n",
    "    for file in glob.glob(\"data/sampled/%s*.csv\" %(taxi_brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the file name\n",
    "        list_files[taxi_brand].append(file)\n",
    "        size_b.append(os.path.getsize(file))\n",
    "    print(\"For the file type called %s there are %i files.\" %(taxi_brand, nb_files))\n",
    "    # Get size basic stat\n",
    "    print(\"     Here are some metrics based on their size in bytes:\")\n",
    "    print(\"       -min value           : \", np.min(size_b))\n",
    "    print(\"       -max value           : \", np.max(size_b))\n",
    "    print(\"       -mean value          : \", np.round(np.mean(size_b),2))\n",
    "    print(\"       -25 percentile value : \", np.quantile(size_b, .25)) \n",
    "    print(\"       -50 percentile value : \", np.quantile(size_b, .50)) \n",
    "    print(\"       -75 percentile value : \", np.quantile(size_b, .75)) \n",
    "    print(\"       -90 percentile value : \", np.quantile(size_b, .90))\n",
    "    print(\"       -total file sum size : \", np.sum(size_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files stats in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the taxi brand called yellow there are still 10 files.\n",
      "     Here are some metrics based on their size in rows count:\n",
      "       -min value           :  477\n",
      "       -max value           :  32301\n",
      "       -mean value          :  24204.51\n",
      "       -25 percentile value :  19990.0\n",
      "       -50 percentile value :  26295.0\n",
      "       -75 percentile value :  29081.0\n",
      "       -90 percentile value :  30203.0\n",
      "       -total row number    :  3170791\n",
      "For the taxi brand called green there are still 10 files.\n",
      "     Here are some metrics based on their size in rows count:\n",
      "       -min value           :  16\n",
      "       -max value           :  3547\n",
      "       -mean value          :  2027.5\n",
      "       -25 percentile value :  1359.75\n",
      "       -50 percentile value :  2073.5\n",
      "       -75 percentile value :  2893.25\n",
      "       -90 percentile value :  3127.0\n",
      "       -total row number    :  154090\n",
      "For the taxi brand called fhv there are still 10 files.\n",
      "     Here are some metrics based on their size in rows count:\n",
      "       -min value           :  960\n",
      "       -max value           :  47704\n",
      "       -mean value          :  23128.39\n",
      "       -25 percentile value :  7975.25\n",
      "       -50 percentile value :  22391.5\n",
      "       -75 percentile value :  39336.5\n",
      "       -90 percentile value :  44120.3\n",
      "       -total row number    :  1711501\n",
      "For the taxi brand called fhvhv there are still 10 files.\n",
      "     Here are some metrics based on their size in rows count:\n",
      "       -min value           :  8626\n",
      "       -max value           :  47704\n",
      "       -mean value          :  32182.9\n",
      "       -25 percentile value :  18023.0\n",
      "       -50 percentile value :  40715.0\n",
      "       -75 percentile value :  43057.25\n",
      "       -90 percentile value :  44923.0\n",
      "       -total row number    :  321829\n"
     ]
    }
   ],
   "source": [
    "for taxi_brand in list_taxi :\n",
    "    names={}\n",
    "    size_r=[]\n",
    "    for fn in glob.glob(\"data/sampled/%s*.csv\" %(taxi_brand)):\n",
    "        with open(fn) as f:\n",
    "            names[fn]=sum(1 for line in f if line.strip())      \n",
    "        # Save in list files sizes in rows\n",
    "        size_r=list(names.values())\n",
    "    print(\"For the taxi brand called %s there are still %i files.\" %(taxi_brand, nb_files))\n",
    "    # Get size basic stat\n",
    "    print(\"     Here are some metrics based on their size in rows count:\")\n",
    "    print(\"       -min value           : \", np.min(size_r))\n",
    "    print(\"       -max value           : \", np.max(size_r))\n",
    "    print(\"       -mean value          : \", np.round(np.mean(size_r),2))\n",
    "    print(\"       -25 percentile value : \", np.quantile(size_r, .25)) \n",
    "    print(\"       -50 percentile value : \", np.quantile(size_r, .50)) \n",
    "    print(\"       -75 percentile value : \", np.quantile(size_r, .75)) \n",
    "    print(\"       -90 percentile value : \", np.quantile(size_r, .90))\n",
    "    print(\"       -total row number    : \", np.sum(size_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the schema evolution.\n",
    "\n",
    "Over time, the relational schema associated to each type of trip data (yellow, green, fhv, hvfhv) has changed. Let us analyze the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ [YELLOW] ------------\n",
      "For the taxi brand called yellow there are 131 files.\n",
      "    --> In 2009 - 1 : save 1st reference schema\n",
      "    --> In 2010 - 1 : 12 diff in new schema compared to reference schema on a total of 18 columns\n",
      "               New col. not in ref: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
      "               Ref col. not in new ['vendor_name', 'trip_pickup_datetime', 'trip_dropoff_datetime', 'start_lon', 'start_lat', 'store_and_forward', 'end_lon', 'end_lat', 'fare_amt', 'tip_amt', 'tolls_amt', 'total_amt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 154: expected 18 fields, saw 19\\nSkipping line 2380: expected 18 fields, saw 19\\nSkipping line 8408: expected 18 fields, saw 19\\nSkipping line 11210: expected 18 fields, saw 19\\nSkipping line 11353: expected 18 fields, saw 19\\nSkipping line 11663: expected 18 fields, saw 19\\nSkipping line 13047: expected 18 fields, saw 19\\nSkipping line 13900: expected 18 fields, saw 19\\nSkipping line 14577: expected 18 fields, saw 19\\nSkipping line 15041: expected 18 fields, saw 19\\nSkipping line 15844: expected 18 fields, saw 19\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    --> In 2015 - 1 : 6 diff in new schema compared to reference schema on a total of 19 columns\n",
      "               New col. not in ref: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
      "               Ref col. not in new ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'rate_code', 'surcharge']\n",
      "    --> In 2016 - 7 : 2 diff in new schema compared to reference schema on a total of 17 columns\n",
      "               New col. not in ref: ['pulocationid', 'dolocationid']\n",
      "               Ref col. not in new ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 18 columns\n",
      "               New col. not in ref: ['congestion_surcharge']\n",
      "               Ref col. not in new []\n",
      "Records from 2009-1 to 2009-12 use the same schema\n",
      "Records from 2010-1 to 2014-12 use the same schema\n",
      "Records from 2015-1 to 2016-7 use the same schema\n",
      "Records from 2016-7 to 2018-12 use the same schema\n",
      "Records from 2019-1 to 2020-6 use the same schema\n",
      "Reference schema: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
      "------------------------------\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "0   2009-01-01\n",
      "1   2010-01-01\n",
      "2   2015-01-01\n",
      "3   2016-07-01\n",
      "4   2019-01-01\n",
      "5   2020-06-01\n",
      "dtype: datetime64[ns]\n",
      "------------ [GREEN] ------------\n",
      "For the taxi brand called green there are 76 files.\n",
      "    --> In 2013 - 8 : save 1st reference schema\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seb/miniconda2/envs/BDGA/lib/python3.7/site-packages/ipykernel_launcher.py:89: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    --> In 2015 - 1 : 1 diff in new schema compared to reference schema on a total of 21 columns\n",
      "               New col. not in ref: ['improvement_surcharge']\n",
      "               Ref col. not in new []\n",
      "    --> In 2016 - 7 : 2 diff in new schema compared to reference schema on a total of 19 columns\n",
      "               New col. not in ref: ['pulocationid', 'dolocationid']\n",
      "               Ref col. not in new ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
      "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 20 columns\n",
      "               New col. not in ref: ['congestion_surcharge']\n",
      "               Ref col. not in new []\n",
      "Records from 2013-8 to 2014-12 use the same schema\n",
      "Records from 2015-1 to 2016-7 use the same schema\n",
      "Records from 2016-7 to 2018-12 use the same schema\n",
      "Records from 2019-1 to 2020-6 use the same schema\n",
      "Reference schema: ['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
      "------------------------------\n",
      "[1. 1. 1. 1. 1.]\n",
      "0   2013-08-01\n",
      "1   2015-01-01\n",
      "2   2016-07-01\n",
      "3   2019-01-01\n",
      "4   2020-06-01\n",
      "dtype: datetime64[ns]\n",
      "------------ [FHV] ------------\n",
      "For the taxi brand called fhv there are 64 files.\n",
      "    --> In 2015 - 1 : save 1st reference schema\n",
      "    --> In 2017 - 1 : 4 diff in new schema compared to reference schema on a total of 5 columns\n",
      "               New col. not in ref: ['pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid']\n",
      "               Ref col. not in new ['pickup_date', 'locationid']\n",
      "    --> In 2017 - 7 : 1 diff in new schema compared to reference schema on a total of 6 columns\n",
      "               New col. not in ref: ['sr_flag']\n",
      "               Ref col. not in new []\n",
      "    --> In 2018 - 1 : 1 diff in new schema compared to reference schema on a total of 7 columns\n",
      "               New col. not in ref: ['dispatching_base_number']\n",
      "               Ref col. not in new []\n",
      "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 6 columns\n",
      "               New col. not in ref: []\n",
      "               Ref col. not in new ['dispatching_base_number']\n",
      "Records from 2015-1 to 2016-12 use the same schema\n",
      "Records from 2017-1 to 2017-7 use the same schema\n",
      "Records from 2017-7 to 2017-12 use the same schema\n",
      "Records from 2018-1 to 2018-12 use the same schema\n",
      "Records from 2019-1 to 2020-6 use the same schema\n",
      "Reference schema: ['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n",
      "------------------------------\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "0   2015-01-01\n",
      "1   2017-01-01\n",
      "2   2017-07-01\n",
      "3   2018-01-01\n",
      "4   2019-01-01\n",
      "5   2020-06-01\n",
      "dtype: datetime64[ns]\n",
      "------------ [FHVHV] ------------\n",
      "For the taxi brand called fhvhv there are 10 files.\n",
      "    --> In 2019 - 2 : save 1st reference schema\n",
      "     There is no diff between files\n",
      "Records from 2019-2 to 2020-6 use the same schema\n",
      "Reference schema: ['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n",
      "------------------------------\n",
      "[1. 1.]\n",
      "0   2019-02-01\n",
      "1   2020-06-01\n",
      "dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Code to help analyze the schema changes goes here\n",
    "# Get the list of file\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    list_files = {}\n",
    "    yr_list = {}\n",
    "    mth_list = {}\n",
    "    list_files[taxi_brand] = []\n",
    "    yr_list[taxi_brand] = []\n",
    "    mth_list[taxi_brand] = []\n",
    "    nb_files = 0\n",
    "    size = []\n",
    "    count = 0\n",
    "    # List the file from the same taxi company brand \n",
    "    for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files[taxi_brand].append(file)\n",
    "    # Order by date the file list\n",
    "    list_files[taxi_brand].sort()\n",
    "    print(\"------------ [%s] ------------\" %(taxi_brand.upper()))\n",
    "    print(\"For the taxi brand called %s there are %i files.\" %(taxi_brand , nb_files))\n",
    "    for yr in range(0,nb_files):\n",
    "        # Read file by file\n",
    "        df = pd.read_csv(list_files[taxi_brand][yr],error_bad_lines=False, sep=',')\n",
    "        # Extract the head of the file\n",
    "        head = list(df)\n",
    "        head_lower = [ x.lower() for x in head ]\n",
    "        # Remove the blank space in the col name\n",
    "        head_lower_clean = [x.strip(' ') for x in head_lower]\n",
    "        # counter for check file\n",
    "        count = count + 1\n",
    "        # Save first file schema as reference\n",
    "        if yr == 0 :\n",
    "            nb_col_ref = len(head)\n",
    "            col_name_ref = head_lower_clean\n",
    "            time_period = int(list_files[taxi_brand][yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "            year = int(list_files[taxi_brand][yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "            month = int(list_files[taxi_brand][yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "            yr_list[taxi_brand].append(year)\n",
    "            mth_list[taxi_brand].append(month)\n",
    "            print(\"    --> In %i - %i : save 1st reference schema\" %(year,month))\n",
    "        else :\n",
    "            # Compare reference schema with the shema of all the others files\n",
    "            if head_lower_clean != col_name_ref :\n",
    "                diff_name_col_add = [ x for x in head_lower_clean if x not in set(col_name_ref) ]\n",
    "                diff_name_col_rm = [ x for x in col_name_ref if x not in set (head_lower_clean) ]\n",
    "                if len(diff_name_col_add) > 0 :\n",
    "                    diff_name_col = diff_name_col_add\n",
    "                elif len(diff_name_col_add) == 0 :\n",
    "                    diff_name_col = diff_name_col_rm\n",
    "                year = int(list_files[taxi_brand][yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "                month = int(list_files[taxi_brand][yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "                yr_list[taxi_brand].append(year)\n",
    "                mth_list[taxi_brand].append(month)\n",
    "                print(\"    --> In %i - %i : %i diff in new schema compared to reference schema on a total of %i columns\" %(year,month,len(diff_name_col),len(head_lower_clean)))\n",
    "                print(\"               New col. not in ref:\",diff_name_col_add)\n",
    "                print(\"               Ref col. not in new\",diff_name_col_rm)\n",
    "                col_name_ref = head_lower_clean\n",
    "                nb_col_ref = len(head_lower_clean)\n",
    "                count = count - 1\n",
    "        if yr == nb_files-1 :\n",
    "            year = int(list_files[taxi_brand][yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "            month = int(list_files[taxi_brand][yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "            yr_list[taxi_brand].append(year)\n",
    "            mth_list[taxi_brand].append(month)\n",
    "    if count == nb_files:\n",
    "        print(\"     There is no diff between files\")\n",
    "        \n",
    "    for i in range(0,len(yr_list[taxi_brand])-1) :\n",
    "        date_in = str(yr_list[taxi_brand][i])+'-'+str(mth_list[taxi_brand][i])\n",
    "        if mth_list[taxi_brand][i+1] == 1 :\n",
    "            date_out =  str(yr_list[taxi_brand][i+1]-1)+'-12'\n",
    "        else :\n",
    "            date_out =  str(yr_list[taxi_brand][i+1])+'-'+str(mth_list[taxi_brand][i+1])\n",
    "        print(\"Records from %s to %s use the same schema\" %(date_in,date_out))\n",
    "    print(\"Reference schema:\",col_name_ref)\n",
    "    print(\"------------------------------\")\n",
    "    # Save date of schema change for data cleaning)\n",
    "    days = np.ones(len(yr_list[taxi_brand]))\n",
    "    print(days)\n",
    "    d = {\n",
    "        'year' : yr_list[taxi_brand],\n",
    "        'month': mth_list[taxi_brand],\n",
    "        'day'  : days\n",
    "    }\n",
    "    df_wrt = pd.DataFrame(data=d)\n",
    "    df_wrt_time = pd.to_datetime(df_wrt)\n",
    "    df_wrt_time.to_csv('data/Change_date_%s.csv' %(taxi_brand,), sep=',', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for fhv cab data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FHV taxi (64 files) reacords go from January 2015 till June 2020 (December 2019 and February 2020 are missing).\n",
    "\n",
    "#### Schema changes:\n",
    "\n",
    "- In January 2017, the name of the pickup date column changed from 'pickup_date' to 'pickup_datetime' and the drop-off time are also now recorded. From this date, both pick-up and drop-off location id are saved, instead of pick-up location only.\n",
    "\n",
    "- In Jully 2017, a new column is add 'sr_flag'. If sr_flag=1 then the trip was a part of a shared ride chain (e.g. Uber Pool, Lyft Line).\n",
    "\n",
    "- In January 2018, the column nammed 'dispatching_base_number' was duplacated and removed in January 2019.\n",
    "\n",
    "------- \n",
    "- Records from 2015-1 to 2016-12 use the same schema\n",
    "- Records from 2017-1 to 2017-6 use the same schema\n",
    "- Records from 2017-7 to 2017-12 use the same schema\n",
    "- Records from 2018-1 to 2018-12 use the same schema\n",
    "- Records from 2019-1 to 2020-6 use the same schema\n",
    "\n",
    "#### Final columns schema\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n",
    "\n",
    "#### Deduce removed/changed/added column:\n",
    "\n",
    "It is not possible to deduce removed or added columns from other ones. Drop-off time and location are save only from 01/2017 and shared ride are instore in 07/2017.\n",
    "\n",
    "#### Conclusion extract from the previous python code execution:\n",
    "\n",
    "    --> In 2015 - 1 : save 1st reference schema\n",
    "    --> In 2017 - 1 : 4 diff in new schema compared to reference schema on a total of 5 columns\n",
    "               New col. not in ref: ['pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid']\n",
    "               Ref col. not in new ['pickup_date', 'locationid']\n",
    "    --> In 2017 - 7 : 1 diff in new schema compared to reference schema on a total of 6 columns\n",
    "               New col. not in ref: ['sr_flag']\n",
    "               Ref col. not in new []\n",
    "    --> In 2018 - 1 : 1 diff in new schema compared to reference schema on a total of 7 columns\n",
    "               New col. not in ref: ['dispatching_base_number']\n",
    "               Ref col. not in new []\n",
    "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 6 columns\n",
    "               New col. not in ref: []\n",
    "               Ref col. not in new ['dispatching_base_number']\n",
    "    Records from 2015-1 to 2016-12 use the same schema\n",
    "    Records from 2017-1 to 2017-7 use the same schema\n",
    "    Records from 2017-7 to 2017-12 use the same schema\n",
    "    Records from 2018-1 to 2018-12 use the same schema\n",
    "    Records from 2019-1 to 2020-6 use the same schema\n",
    "    Reference schema: ['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for fhvhv data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "FHVHV taxi (10 files) records from February 2019 till June 2020, 7 files of records are missing (from July 2019 till December 2019 and March 2020).\n",
    "\n",
    "#### There is no change of the schema:\n",
    "\n",
    "- Records from 2019-2 to 2020-6 use the same schema\n",
    "\n",
    "#### Final columns schema\n",
    "\n",
    "['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']\n",
    "\n",
    "#### Conclusion extract from the previous python code execution:\n",
    "\n",
    "    --> In 2019 - 2 : save 1st reference schema\n",
    "     There is no diff between files\n",
    "    Records from 2019-2 to 2020-6 use the same schema\n",
    "    Reference schema: ['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for green cab data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Green taxi (76 files) records from August 2013 till June 2020, 7 files of records are missing (from July 2019 till December 2019 and March 2020).\n",
    "\n",
    "#### Schema changes:\n",
    "\n",
    "- In January 2015, a new column is add 'improvement_surcharge'. Add $0.30 surcharge on hailed trips at the flag\n",
    "drop.\n",
    "\n",
    "- In Jully 2016, both pick-up and drop-off location id are saved, instead of pick-up and drop-off latitude and longitude location.\n",
    "\n",
    "- In January 2019, a new column is add 'congestion_surcharge'. Add $2.75 surcharge if traffic congestion during the trip.\n",
    "\n",
    "--------\n",
    "- Records from 2013-8 to 2014-12 use the same schema\n",
    "- Records from 2015-1 to 2016-7 use the same schema\n",
    "- Records from 2016-7 to 2018-12 use the same schema\n",
    "- Records from 2019-1 to 2020-6 use the same schema\n",
    "\n",
    "#### Final columns schema:\n",
    "\n",
    "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    "\n",
    "#### Deduce removed/changed/added column:\n",
    "\n",
    "Before July 2016 it is possible to deduce the pick-up and drop-off location id using the real latitude and longitude position for the pick-up and the drop-off.\n",
    "\n",
    "The 'improvement_surcharge' and 'congestion_surcharge' are new taxes that can be deduce for the missing period. In order to compare the trip price over all the period it is possible to remove these taxes to the 'total_amount'  when there are applied. Otherwise the taxes are set equal to $0 .\n",
    "\n",
    "#### Conclusion extract from the previous python code execution:\n",
    "\n",
    "    --> In 2013 - 8 : save 1st reference schema\n",
    "    --> In 2015 - 1 : 1 diff in new schema compared to reference schema on a total of 21 columns\n",
    "               New col. not in ref: ['improvement_surcharge']\n",
    "               Ref col. not in new []\n",
    "    --> In 2016 - 7 : 2 diff in new schema compared to reference schema on a total of 19 columns\n",
    "               New col. not in ref: ['pulocationid', 'dolocationid']\n",
    "               Ref col. not in new ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 20 columns\n",
    "               New col. not in ref: ['congestion_surcharge']\n",
    "               Ref col. not in new []\n",
    "    Records from 2013-8 to 2014-12 use the same schema\n",
    "    Records from 2015-1 to 2016-7 use the same schema\n",
    "    Records from 2016-7 to 2018-12 use the same schema\n",
    "    Records from 2019-1 to 2020-6 use the same schema\n",
    "    Reference schema: ['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of schema changes for yellow cab data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellow taxi (131 files) records from January 2009 till June 2020, 7 files of records are missing (from July 2019 till December 2019 and March 2020).\n",
    "\n",
    "#### Schema changes:\n",
    "\n",
    "- In January 2010, 12 columns are renamed:\n",
    "        - 'vendor_name'          --> 'vendor_id'\n",
    "        - 'trip_pickup_datetime' --> 'pickup_datetime'\n",
    "        - 'trip_dropoff_datetime'--> 'dropoff_datetime'\n",
    "        - 'start_lon'            --> 'pickup_longitude'\n",
    "        - 'start_lat'            --> 'pickup_latitude'\n",
    "        - 'store_and_forward'    --> 'store_and_fwd_flag'\n",
    "        - 'end_lon'              --> 'dropoff_longitude'\n",
    "        - 'end_lat'              --> 'dropoff_latitude'\n",
    "        - 'fare_amt'             --> 'fare_amount'\n",
    "        - 'tip_amt               --> 'tip_amount'\n",
    "        - 'tolls_amt'            --> 'tolls_amount'\n",
    "        - 'total_amt'            --> 'total_amount'\n",
    "\n",
    "- In January 2015, a new columns is add 'extra' for rush hour and overnight charges: $0.5 ; $1.0. And 5 columns are renamed:\n",
    "        - 'vendor_id'       --> 'vendorid'\n",
    "        - 'pickup_datetime' --> 'tpep_pickup_datetime'\n",
    "        - 'dropoff_datetime'--> 'tpep_dropoff_datetime'\n",
    "        - 'rate_code'       --> 'ratecodeid'\n",
    "        - 'surcharge'       --> 'improvement_surcharge' --> the amount of the surcharge change from $.5 to $0.3.\n",
    "\n",
    "- In July 2016, both pick-up and drop-off location id are saved, instead of pick-up and drop-off latitude and longitude location.\n",
    "\n",
    "- In January 2019, a new column is add 'congestion_surcharge'. Add $2.75 surcharge if traffic congestion during the trip.\n",
    "\n",
    "------------\n",
    "\n",
    "- Records from 2009-1 to 2009-12 use the same schema\n",
    "- Records from 2010-1 to 2014-12 use the same schema\n",
    "- Records from 2015-1 to 2016-7 use the same schema\n",
    "- Records from 2016-7 to 2018-12 use the same schema\n",
    "- Records from 2019-1 to 2020-6 use the same schema\n",
    "\n",
    "#### Final columns schema:\n",
    "\n",
    "['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
    "\n",
    "\n",
    "#### Deduce removed/changed/added column:\n",
    "\n",
    "Before July 2016 it is possible to deduce the pick-up and drop-off location id using the real latitude and longitude position for the pick-up and the drop-off.\n",
    "\n",
    "The 'improvement_surcharge' and 'congestion_surcharge' are new taxes that cannot be deduce for the missing period. In order to compare the trip price over all the period it is possible to remove these taxes to the 'total_amount'  when there are applied for the period after 01/2015 and 07/2016. Otherwise the taxes are set equal to $0.\n",
    "\n",
    "\n",
    "#### Conclusion extract from the previous python code execution:\n",
    "\n",
    "    --> In 2009 - 1 : save 1st reference schema\n",
    "    --> In 2010 - 1 : 12 diff in new schema compared to reference schema on a total of 18 columns\n",
    "               New col. not in ref: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "               Ref col. not in new ['vendor_name', 'trip_pickup_datetime', 'trip_dropoff_datetime', 'start_lon', 'start_lat', 'store_and_forward', 'end_lon', 'end_lat', 'fare_amt', 'tip_amt', 'tolls_amt', 'total_amt']\n",
    "    --> In 2015 - 1 : 6 diff in new schema compared to reference schema on a total of 19 columns\n",
    "               New col. not in ref: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
    "               Ref col. not in new ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'rate_code', 'surcharge']\n",
    "    --> In 2016 - 7 : 2 diff in new schema compared to reference schema on a total of 17 columns\n",
    "               New col. not in ref: ['pulocationid', 'dolocationid']\n",
    "               Ref col. not in new ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    --> In 2019 - 1 : 1 diff in new schema compared to reference schema on a total of 18 columns\n",
    "               New col. not in ref: ['congestion_surcharge']\n",
    "               Ref col. not in new []\n",
    "    Records from 2009-1 to 2009-12 use the same schema\n",
    "    Records from 2010-1 to 2014-12 use the same schema\n",
    "    Records from 2015-1 to 2016-7 use the same schema\n",
    "    Records from 2016-7 to 2018-12 use the same schema\n",
    "    Records from 2019-1 to 2020-6 use the same schema\n",
    "    Reference schema: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
