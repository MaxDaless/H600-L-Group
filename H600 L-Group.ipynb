{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Project H600 / L-Group\n",
    "***\n",
    "## Real world Data Exploration, Integration, Cleasning, Transformation and Analysis\n",
    "\n",
    "Data:\n",
    "The New York City Taxi and Limousine Commission (or TLC for short) has been publishing\n",
    "records about taxi trips in New York since 2009. \n",
    "\n",
    "The TLC trip dataset actually consists of 4 sub-datasets:\n",
    "\n",
    "    1.Yellow taxi records are records that record trip information of New York's famous yellow taxi cars\n",
    "\n",
    "    2.Green taxi records are records that record trip information by so-called 'boro' taxis, a newer service introduced in August of 2013 to improve taxi service and availability in the boroughs. \n",
    "\n",
    "    3.FHV records (short for 'For Hire Vehicles') record information from services that offered for-hire vehicles (such as Uber, Lyft, Via, and Juno), but also luxury limousine bases.\n",
    "\n",
    "    4.High volume FHV (FHVHV for short) are FHV records offered by services that make more than 10,000 trips per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Files & Data Exploration \n",
    "### 1.1 Count of files per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of green tripdata files:\n",
      "76\n",
      "count of yellow tripdata files:\n",
      "131\n",
      "count of fhv tripdata files:\n",
      "64\n",
      "count of fhvhv tripdata files:\n",
      "10\n",
      "count of all tripdata files:\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "print ('count of green tripdata files:')\n",
    "!find /home/bigdata/Desktop/Project/Data/green_tripdata_*.csv -type f | wc -l \n",
    "print ('count of yellow tripdata files:')\n",
    "!find /home/bigdata/Desktop/Project/Data/yellow_tripdata_*.csv -type f | wc -l\n",
    "print ('count of fhv tripdata files:')\n",
    "!find /home/bigdata/Desktop/Project/Data/fhv_tripdata_*.csv -type f | wc -l \n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find /home/bigdata/Desktop/Project/Data/fhvhv_tripdata_*.csv -type f | wc -l\n",
    "print ('count of all tripdata files:')\n",
    "!find /home/bigdata/Desktop/Project/Data/*.csv -type f | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Files size (bytes) and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the taxi brand called fhv there are 74 files.\n",
      "-min value           :  52060\n",
      "-max value           :  3339455\n",
      "-mean value          :  1263540.6621621621\n",
      "-25 percentile value :  239082.25\n",
      "-50 percentile value :  725289.0\n",
      "-75 percentile value :  2545631.75\n",
      "-90 percentile value :  3003289.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.chdir(\"/home/bigdata/Desktop/Project/Data/\")\n",
    "taxi_brand=\"fhv\"\n",
    "list_files = {}\n",
    "list_files[taxi_brand] = []\n",
    "nb_files = 0\n",
    "size_b = []\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"%s*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the file name\n",
    "    list_files[taxi_brand].append(file)\n",
    "    size_b.append(os.path.getsize(file))\n",
    "\n",
    "print(\"For the taxi brand called %s there are %i files.\" %(taxi_brand, nb_files))\n",
    "# Get size basic stat\n",
    "print(\"Here are some metrics based on their size in bytes:\")\n",
    "print(\"-min value           : \", np.min(size_b))\n",
    "print(\"-max value           : \", np.max(size_b))\n",
    "print(\"-mean value          : \", np.mean(size_b))\n",
    "print(\"-25 percentile value : \", np.quantile(size_b, .25)) \n",
    "print(\"-50 percentile value : \", np.quantile(size_b, .50)) \n",
    "print(\"-75 percentile value : \", np.quantile(size_b, .75)) \n",
    "print(\"-90 percentile value : \", np.quantile(size_b, .90)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Files size (rows) and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the taxi brand called fhv there are still 74 files.\n",
      "Here are some metrics based on their size in rows count:\n",
      "-min value           :  960\n",
      "-max value           :  47704\n",
      "-mean value          :  23128.391891891893\n",
      "-25 percentile value :  7975.25\n",
      "-50 percentile value :  22391.5\n",
      "-75 percentile value :  39336.5\n",
      "-90 percentile value :  44120.3\n"
     ]
    }
   ],
   "source": [
    "names={}\n",
    "size_r=[]\n",
    "\n",
    "for fn in glob.glob(\"%s*.csv\" %(taxi_brand)):\n",
    "    with open(fn) as f:\n",
    "        names[fn]=sum(1 for line in f if line.strip())     \n",
    "        \n",
    "    # Save in list files sizes in rows\n",
    "    size_r=list(names.values())\n",
    "\n",
    "print(\"For the taxi brand called %s there are still %i files.\" %(taxi_brand, nb_files))\n",
    "# Get size basic stat\n",
    "print(\"Here are some metrics based on their size in rows count:\")\n",
    "print(\"-min value           : \", np.min(size_r))\n",
    "print(\"-max value           : \", np.max(size_r))\n",
    "print(\"-mean value          : \", np.mean(size_r))\n",
    "print(\"-25 percentile value : \", np.quantile(size_r, .25)) \n",
    "print(\"-50 percentile value : \", np.quantile(size_r, .50)) \n",
    "print(\"-75 percentile value : \", np.quantile(size_r, .75)) \n",
    "print(\"-90 percentile value : \", np.quantile(size_r, .90)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the contents of a file into an RDD. Note - when run on the cluster this load from HDFS (inside /user/$USER/)\n",
    "# if you really want to load from HDFS, you can also put the full HDFS url, e.g.\n",
    "# hdfs://public00:8020/user/<your_user_id_here>/data/books/pg20417.txt\n",
    "fileName = 'Data/green_tripdata_2020-01.csv'\n",
    "TaxiRDD = sc.textFile(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,RatecodeID,PULocationID,DOLocationID,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge',\n",
       " ',2020-01-27 06:40:00,2020-01-27 07:25:00,,,159,61,,19.84,40.95,2.75,0,0,6.12,,0.3,50.12,,,',\n",
       " '2,2020-01-18 01:01:42,2020-01-18 01:04:57,N,1,7,146,1,.56,4.5,0.5,0.5,1.45,0,,0.3,7.25,1,1,0',\n",
       " '2,2020-01-15 13:52:01,2020-01-15 14:13:09,N,1,134,77,1,5.87,22,0,0.5,0,0,,0.3,22.8,1,1,0',\n",
       " '2,2020-01-30 23:49:37,2020-01-31 00:12:17,N,1,42,143,1,5.40,20,0.5,0.5,4.81,0,,0.3,28.86,1,1,2.75']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaxiRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['VendorID,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,RatecodeID,PULocationID,DOLocationID,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge'],\n",
       " [',2020-01-27',\n",
       "  '06:40:00,2020-01-27',\n",
       "  '07:25:00,,,159,61,,19.84,40.95,2.75,0,0,6.12,,0.3,50.12,,,'],\n",
       " ['2,2020-01-18',\n",
       "  '01:01:42,2020-01-18',\n",
       "  '01:04:57,N,1,7,146,1,.56,4.5,0.5,0.5,1.45,0,,0.3,7.25,1,1,0']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tupleRDD = TaxiRDD.map(lambda line: line.split())\n",
    "tupleRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID,lpep_pickup_datetime,lpep_dropoff_datetime,store_and_fwd_flag,RatecodeID,PULocationID,DOLocationID,passenger_count,trip_distance,fare_amount,extra,mta_tax,tip_amount,tolls_amount,ehail_fee,improvement_surcharge,total_amount,payment_type,trip_type,congestion_surcharge',\n",
       " ',2020-01-27',\n",
       " '06:40:00,2020-01-27',\n",
       " '07:25:00,,,159,61,,19.84,40.95,2.75,0,0,6.12,,0.3,50.12,,,',\n",
       " '2,2020-01-18']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD = TaxiRDD.flatMap(lambda line: line.split())\n",
    "wordsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2686"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.map(lambda x: 1).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a word count on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTaxiRDD = sc.textFile('Data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5036382"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTaxiRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5036382"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTaxiRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow taxi records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
