{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory data/cleaned\n",
      "Successfully created the directory data/cleaned/yellow \n",
      "Successfully created the directory data/cleaned/green \n",
      "Successfully created the directory data/cleaned/fhv \n",
      "Successfully created the directory data/cleaned/fhvhv \n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "try :  \n",
    "    os.path.isdir(\"data/cleaned\")\n",
    "except OSError:\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "    print (\"Creation of the directory data/cleaned failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/cleaned\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.path.isdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We can then create a unified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Documents/Master_BDGA\n",
      "list of fhvhv tripdata files:\n",
      "fhvhv_tripdata_2019-02.csv  fhvhv_tripdata_2020-01.csv\n",
      "fhvhv_tripdata_2019-03.csv  fhvhv_tripdata_2020-03.csv\n",
      "fhvhv_tripdata_2019-04.csv  fhvhv_tripdata_2020-04.csv\n",
      "fhvhv_tripdata_2019-05.csv  fhvhv_tripdata_2020-05.csv\n",
      "fhvhv_tripdata_2019-06.csv  fhvhv_tripdata_2020-06.csv\n",
      "count of fhvhv tripdata files:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#export new csv file\n",
    "#fhvhv_DF.write.save(path='data/modified/fhvhv.csv', format='csv', mode='append', sep='\\t')\n",
    "#Ã§a fait un dossier ac plusieurs fichiers pourris\n",
    "\n",
    "#move files to t2 directory\n",
    "\n",
    "source_dir= '/data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleanned/fhvhv')\n",
    "\n",
    "print ('list of fhvhv tripdata files:')\n",
    "!ls data/cleanned/fhvhv\n",
    "\n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find data/cleanned/fhvhv -type f | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 : \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 : \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 :\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Change schema 5 :\n",
    "            NO change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01\n",
      "schema 1\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-02-01\n",
      "schema 1\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-03-01\n",
      "schema 1\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-04-01\n",
      "schema 1\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-05-01\n",
      "schema 1\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-06-01\n",
      "schema 1\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-07-01\n",
      "schema 1\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-08-01\n",
      "schema 1\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-09-01\n",
      "schema 1\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-10-01\n",
      "schema 1\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-11-01\n",
      "schema 1\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-12-01\n",
      "schema 1\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2016-01-01\n",
      "schema 1\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-02-01\n",
      "schema 1\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-03-01\n",
      "schema 1\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-04-01\n",
      "schema 1\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-05-01\n",
      "schema 1\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-06-01\n",
      "schema 1\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-07-01\n",
      "schema 1\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-08-01\n",
      "schema 1\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-09-01\n",
      "schema 1\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-10-01\n",
      "schema 1\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-11-01\n",
      "schema 1\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-12-01\n",
      "schema 1\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "schema 2\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "schema 2\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "schema 2\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "schema 2\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "schema 2\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "schema 2\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "schema 3\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "schema 3\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "schema 3\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "schema 3\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "schema 3\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "schema 3\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "schema 4\n",
      "2018-01-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "schema 4\n",
      "2018-02-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "schema 4\n",
      "2018-03-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "schema 4\n",
      "2018-04-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "schema 4\n",
      "2018-05-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "schema 4\n",
      "2018-06-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "schema 4\n",
      "2018-07-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "schema 4\n",
      "2018-08-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "schema 4\n",
      "2018-09-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "schema 4\n",
      "2018-10-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "schema 4\n",
      "2018-11-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "schema 4\n",
      "2018-12-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "schema 5\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "schema 5\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "schema 5\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "schema 5\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "schema 5\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "schema 5\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "schema 5\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "schema 5\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "schema 5\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "schema 5\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "schema 5\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "schema 5\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "schema 5\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "schema 5\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "schema 5\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "    month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "    date_file = date(year,month,1)\n",
    "    fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files[yr]) )\n",
    "    for nb_schema in range(0,len(dating_schema)-1):\n",
    "        print(date_file)\n",
    "        if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "            if nb_schema+1 == 1 :\n",
    "                print(\"schema\",nb_schema+1)\n",
    "                fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                       .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                       .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                       .select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                        \"dropoff_datetime\",\n",
    "                        col(\"locationID\").alias(\"PULocationID\"),\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "                fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 2 :\n",
    "                print(\"schema\",nb_schema+1)\n",
    "                fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                        .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 3 :\n",
    "                print(\"schema\",nb_schema+1)\n",
    "                fhv3_DF = fhv_DF.select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 4 :\n",
    "                print(\"schema\",nb_schema+1)\n",
    "                fhv4_DF = fhv_DF.select(\n",
    "                            col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 5 :\n",
    "                print(\"schema\",nb_schema+1)\n",
    "                fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "    if date_file == dating_schema[5].date() :\n",
    "        fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/fhv'))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "fhv1_DF, fhv2_DF, fhv3_DF, fhv4_DF, fhv5_DF = 0,0,0,0,0\n",
    "for yr in range(0,nb_files):\n",
    "    year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "    month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "    date_file = date(year,month,1)\n",
    "    for nb_schema in range(0,len(dating_schema)-1):\n",
    "        if date_file >= dating_schema[nb_schema].date() and date_file < dating_schema[nb_schema+1].date():\n",
    "            if nb_schema+1 == 1 :\n",
    "                fhv1_DF = fhv1_DF + 1\n",
    "            elif nb_schema+1 == 2 :\n",
    "                fhv2_DF = fhv2_DF + 1\n",
    "            elif nb_schema+1 == 3 :\n",
    "                fhv3_DF = fhv3_DF + 1\n",
    "            elif nb_schema+1 == 4 :\n",
    "                fhv4_DF = fhv4_DF + 1\n",
    "            elif nb_schema+1 == 5 :\n",
    "                fhv5_DF = fhv5_DF + 1\n",
    "    if date_file == dating_schema[5].date() :\n",
    "        fhv5_DF = fhv5_DF + 1\n",
    "#print(list_files[0:fhv1_DF])\n",
    "#print(list_files[fhv1_DF:fhv1_DF+fhv2_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF:fhv1_DF+fhv2_DF+fhv3_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF:fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF+fhv5_DF])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "For green there are 76 files:\n",
    "\n",
    "In 2015 - 1 :\n",
    "\n",
    "   1 diff on a total of 21 col: ['improvement_surcharge']\n",
    "           1/1 col add\n",
    "\n",
    "In 2016 - 7 :\n",
    "\n",
    "   2 diff on a total of 19 col: ['pulocationid', 'dolocationid']\n",
    "           2/2 col remove\n",
    "\n",
    "In 2019 - 1 :\n",
    "\n",
    "   1 diff on a total of 20 col: ['congestion_surcharge']\n",
    "           1/1 col add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of yellow tripdata files:\n",
      "data/sampled/green_tripdata_2013-08.csv\r\n",
      "data/sampled/green_tripdata_2013-09.csv\r\n",
      "data/sampled/green_tripdata_2013-10.csv\r\n",
      "data/sampled/green_tripdata_2013-11.csv\r\n",
      "data/sampled/green_tripdata_2013-12.csv\r\n",
      "data/sampled/green_tripdata_2014-01.csv\r\n",
      "data/sampled/green_tripdata_2014-02.csv\r\n",
      "data/sampled/green_tripdata_2014-03.csv\r\n",
      "data/sampled/green_tripdata_2014-04.csv\r\n",
      "data/sampled/green_tripdata_2014-05.csv\r\n",
      "data/sampled/green_tripdata_2014-06.csv\r\n",
      "data/sampled/green_tripdata_2014-07.csv\r\n",
      "data/sampled/green_tripdata_2014-08.csv\r\n",
      "data/sampled/green_tripdata_2014-09.csv\r\n",
      "data/sampled/green_tripdata_2014-10.csv\r\n",
      "data/sampled/green_tripdata_2014-11.csv\r\n",
      "data/sampled/green_tripdata_2014-12.csv\r\n",
      "data/sampled/green_tripdata_2015-01.csv\r\n",
      "data/sampled/green_tripdata_2015-02.csv\r\n",
      "data/sampled/green_tripdata_2015-03.csv\r\n",
      "data/sampled/green_tripdata_2015-04.csv\r\n",
      "data/sampled/green_tripdata_2015-05.csv\r\n",
      "data/sampled/green_tripdata_2015-06.csv\r\n",
      "data/sampled/green_tripdata_2015-07.csv\r\n",
      "data/sampled/green_tripdata_2015-08.csv\r\n",
      "data/sampled/green_tripdata_2015-09.csv\r\n",
      "data/sampled/green_tripdata_2015-10.csv\r\n",
      "data/sampled/green_tripdata_2015-11.csv\r\n",
      "data/sampled/green_tripdata_2015-12.csv\r\n",
      "data/sampled/green_tripdata_2016-01.csv\r\n",
      "data/sampled/green_tripdata_2016-02.csv\r\n",
      "data/sampled/green_tripdata_2016-03.csv\r\n",
      "data/sampled/green_tripdata_2016-04.csv\r\n",
      "data/sampled/green_tripdata_2016-05.csv\r\n",
      "data/sampled/green_tripdata_2016-06.csv\r\n",
      "data/sampled/green_tripdata_2016-07.csv\r\n",
      "data/sampled/green_tripdata_2016-08.csv\r\n",
      "data/sampled/green_tripdata_2016-09.csv\r\n",
      "data/sampled/green_tripdata_2016-10.csv\r\n",
      "data/sampled/green_tripdata_2016-11.csv\r\n",
      "data/sampled/green_tripdata_2016-12.csv\r\n",
      "data/sampled/green_tripdata_2017-01.csv\r\n",
      "data/sampled/green_tripdata_2017-02.csv\r\n",
      "data/sampled/green_tripdata_2017-03.csv\r\n",
      "data/sampled/green_tripdata_2017-04.csv\r\n",
      "data/sampled/green_tripdata_2017-05.csv\r\n",
      "data/sampled/green_tripdata_2017-06.csv\r\n",
      "data/sampled/green_tripdata_2017-07.csv\r\n",
      "data/sampled/green_tripdata_2017-08.csv\r\n",
      "data/sampled/green_tripdata_2017-09.csv\r\n",
      "data/sampled/green_tripdata_2017-10.csv\r\n",
      "data/sampled/green_tripdata_2017-11.csv\r\n",
      "data/sampled/green_tripdata_2017-12.csv\r\n",
      "data/sampled/green_tripdata_2018-01.csv\r\n",
      "data/sampled/green_tripdata_2018-02.csv\r\n",
      "data/sampled/green_tripdata_2018-03.csv\r\n",
      "data/sampled/green_tripdata_2018-04.csv\r\n",
      "data/sampled/green_tripdata_2018-05.csv\r\n",
      "data/sampled/green_tripdata_2018-06.csv\r\n",
      "data/sampled/green_tripdata_2018-07.csv\r\n",
      "data/sampled/green_tripdata_2018-08.csv\r\n",
      "data/sampled/green_tripdata_2018-09.csv\r\n",
      "data/sampled/green_tripdata_2018-10.csv\r\n",
      "data/sampled/green_tripdata_2018-11.csv\r\n",
      "data/sampled/green_tripdata_2018-12.csv\r\n",
      "data/sampled/green_tripdata_2019-01.csv\r\n",
      "data/sampled/green_tripdata_2019-02.csv\r\n",
      "data/sampled/green_tripdata_2019-03.csv\r\n",
      "data/sampled/green_tripdata_2019-04.csv\r\n",
      "data/sampled/green_tripdata_2019-05.csv\r\n",
      "data/sampled/green_tripdata_2019-06.csv\r\n",
      "data/sampled/green_tripdata_2020-01.csv\r\n",
      "data/sampled/green_tripdata_2020-02.csv\r\n",
      "data/sampled/green_tripdata_2020-04.csv\r\n",
      "data/sampled/green_tripdata_2020-05.csv\r\n",
      "data/sampled/green_tripdata_2020-06.csv\r\n"
     ]
    }
   ],
   "source": [
    "print ('list of yellow tripdata files:')\n",
    "!ls data/sampled/green*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- Pickup_longitude: double (nullable = true)\n",
      " |-- Pickup_latitude: double (nullable = true)\n",
      " |-- Dropoff_longitude: double (nullable = true)\n",
      " |-- Dropoff_latitude: double (nullable = true)\n",
      " |-- Passenger_count: integer (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- Fare_amount: double (nullable = true)\n",
      " |-- Extra: double (nullable = true)\n",
      " |-- MTA_tax: double (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Tolls_amount: double (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      " |-- Payment_type: integer (nullable = true)\n",
      " |-- Trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|Lpep_dropoff_datetime|Store_and_fwd_flag|RateCodeID|  Pickup_longitude|   Pickup_latitude| Dropoff_longitude| Dropoff_latitude|Passenger_count|Trip_distance|Fare_amount|Extra|MTA_tax|Tip_amount|Tolls_amount|Ehail_fee|Total_amount|Payment_type|Trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "|       2| 2014-12-19 18:56:47|  2014-12-19 19:16:21|                 N|         1|-73.87925720214844|40.740623474121094|-73.91719055175781|40.74149703979492|              2|         2.18|       13.5|  1.0|    0.5|       0.0|         0.0|     null|        15.0|           2|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- Pickup_longitude: double (nullable = true)\n",
      " |-- Pickup_latitude: double (nullable = true)\n",
      " |-- Dropoff_longitude: double (nullable = true)\n",
      " |-- Dropoff_latitude: double (nullable = true)\n",
      " |-- Passenger_count: integer (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- Fare_amount: double (nullable = true)\n",
      " |-- Extra: double (nullable = true)\n",
      " |-- MTA_tax: double (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Tolls_amount: double (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      " |-- Payment_type: integer (nullable = true)\n",
      " |-- Trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|Lpep_dropoff_datetime|Store_and_fwd_flag|RateCodeID|  Pickup_longitude|  Pickup_latitude| Dropoff_longitude|Dropoff_latitude|Passenger_count|Trip_distance|Fare_amount|Extra|MTA_tax|Tip_amount|Tolls_amount|Ehail_fee|improvement_surcharge|Total_amount|Payment_type|Trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|       2| 2015-01-21 20:07:58|  2015-01-21 20:19:47|                 N|         1|-73.95250701904297|40.81092071533203|-73.94770812988281|40.7846794128418|              1|         2.48|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|           1|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|       2| 2016-07-24 18:50:57|  2016-07-24 18:56:09|                 N|         1|          95|         102|              1|         1.49|        6.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         7.3|           2|        1|\n",
      "|       2| 2016-07-27 08:40:44|  2016-07-27 09:06:41|                 N|         1|         244|         236|              1|         4.34|       19.0|  0.0|    0.5|       4.0|         0.0|     null|                  0.3|        23.8|           1|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read fhv files\n",
    "green1_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2014-12.csv'))\n",
    "green1_DF.printSchema()\n",
    "green1_DF.show(1)\n",
    "green2_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2015-01.csv'))\n",
    "green2_DF.printSchema()\n",
    "green2_DF.show(1)\n",
    "green3_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2016-07.csv'))\n",
    "green3_DF.printSchema()\n",
    "green3_DF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2015-01-21 20:07:58|  2015-01-21 20:19:47|                 N|         1|              1|         2.48|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|           1|        1|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##1.modifing schema as needed\n",
    "green_201412 = green1_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"VendorID\").alias(\"VendorID\"),\n",
    "                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                            col(\"RateCodeID\").alias(\"RatecodeID\"),\n",
    "                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                            col(\"Extra\").alias(\"extra\"),\n",
    "                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                            col(\"improvement_surcharge\").alias(\"improvement_surcharge\"),\n",
    "                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                            \"congestion_surcharge\")\n",
    "#green_201412.show(1)\n",
    "#\n",
    "#2.modifing schema as needed (with pu/do location id)\n",
    "green_201501 = green2_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"VendorID\").alias(\"VendorID\"),\n",
    "                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                            col(\"RateCodeID\").alias(\"RatecodeID\"),\n",
    "                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                            col(\"Extra\").alias(\"extra\"),\n",
    "                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                            col(\"improvement_surcharge\").alias(\"improvement_surcharge\"),\n",
    "                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                            \"congestion_surcharge\")\n",
    "\n",
    "green_201501.show(1)\n",
    "\n",
    "#3.modifing schema as needed\n",
    "green_201707 = green3_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#green_201707.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for geo location\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this subdf into a GeoDF, adding a \"geometry\" column to pinpoint the pickup location\n",
    "geometry = [Point(xy) for xy in zip(subdf['pickup_longitude'], subdf['pickup_latitude'])]\n",
    "geo_subdf = gpd.GeoDataFrame(subdf, geometry=geometry)\n",
    "geo_subdf\n",
    "# A geopanda dataframe has the possibility to create an R-tree index on it's geometry\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 236, 42]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By means of the intersection() method we can query for all the entries in the zones dataframe that \n",
    "# *can* intersect with a query point\n",
    "# Note: this mentod can return false positives; the actual zone is part of the result.\n",
    "# The method returns a generator. We use the list(.) constructor to convert this to a list.\n",
    "\n",
    "query_point = Point( df1.iloc[0].pickup_longitude, df1.iloc[0].pickup_latitude)\n",
    "possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "possible_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>141</td>\n",
       "      <td>0.041514</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>141</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.96178 40.75988, -73.96197 40.759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>Upper East Side South</td>\n",
       "      <td>237</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.96613 40.76218, -73.96658 40.761...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>0.099739</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>Central Park</td>\n",
       "      <td>43</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.97255 40.76490, -73.97301 40.764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                   zone  LocationID  \\\n",
       "140       141    0.041514    0.000077        Lenox Hill West         141   \n",
       "236       237    0.042213    0.000096  Upper East Side South         237   \n",
       "42         43    0.099739    0.000380           Central Park          43   \n",
       "\n",
       "       borough                                           geometry  \n",
       "140  Manhattan  POLYGON ((-73.96178 40.75988, -73.96197 40.759...  \n",
       "236  Manhattan  POLYGON ((-73.96613 40.76218, -73.96658 40.761...  \n",
       "42   Manhattan  POLYGON ((-73.97255 40.76490, -73.97301 40.764...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones.iloc[ possible_matches ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow files\n",
    "\n",
    "For yellow there are 131 files:\n",
    "\n",
    " In 2010 - 1 :\n",
    " \n",
    "   12 diff on a total of 18 col: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "         12/12 column name have changed:\n",
    "         \n",
    " In 2015 - 1 :\n",
    " \n",
    "   6 diff on a total of 19 col: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
    "         1/6 col add\n",
    "         5/6 name change\n",
    "         \n",
    " In 2016 - 7 :\n",
    " \n",
    "   2 diff on a total of 17 col: ['pulocationid', 'dolocationid']\n",
    "         2/2 col remove\n",
    "         \n",
    " In 2019 - 1 :\n",
    " \n",
    "   1 diff on a total of 18 col: ['congestion_surcharge']\n",
    "         1/1 col add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of fhv tripdata files:\n",
      "data/sampled/green_tripdata_2013-08.csv\r\n",
      "data/sampled/green_tripdata_2013-09.csv\r\n",
      "data/sampled/green_tripdata_2013-10.csv\r\n",
      "data/sampled/green_tripdata_2013-11.csv\r\n",
      "data/sampled/green_tripdata_2013-12.csv\r\n",
      "data/sampled/green_tripdata_2014-01.csv\r\n",
      "data/sampled/green_tripdata_2014-02.csv\r\n",
      "data/sampled/green_tripdata_2014-03.csv\r\n",
      "data/sampled/green_tripdata_2014-04.csv\r\n",
      "data/sampled/green_tripdata_2014-05.csv\r\n",
      "data/sampled/green_tripdata_2014-06.csv\r\n",
      "data/sampled/green_tripdata_2014-07.csv\r\n",
      "data/sampled/green_tripdata_2014-08.csv\r\n",
      "data/sampled/green_tripdata_2014-09.csv\r\n",
      "data/sampled/green_tripdata_2014-10.csv\r\n",
      "data/sampled/green_tripdata_2014-11.csv\r\n",
      "data/sampled/green_tripdata_2014-12.csv\r\n",
      "data/sampled/green_tripdata_2015-01.csv\r\n",
      "data/sampled/green_tripdata_2015-02.csv\r\n",
      "data/sampled/green_tripdata_2015-03.csv\r\n",
      "data/sampled/green_tripdata_2015-04.csv\r\n",
      "data/sampled/green_tripdata_2015-05.csv\r\n",
      "data/sampled/green_tripdata_2015-06.csv\r\n",
      "data/sampled/green_tripdata_2015-07.csv\r\n",
      "data/sampled/green_tripdata_2015-08.csv\r\n",
      "data/sampled/green_tripdata_2015-09.csv\r\n",
      "data/sampled/green_tripdata_2015-10.csv\r\n",
      "data/sampled/green_tripdata_2015-11.csv\r\n",
      "data/sampled/green_tripdata_2015-12.csv\r\n",
      "data/sampled/green_tripdata_2016-01.csv\r\n",
      "data/sampled/green_tripdata_2016-02.csv\r\n",
      "data/sampled/green_tripdata_2016-03.csv\r\n",
      "data/sampled/green_tripdata_2016-04.csv\r\n",
      "data/sampled/green_tripdata_2016-05.csv\r\n",
      "data/sampled/green_tripdata_2016-06.csv\r\n",
      "data/sampled/green_tripdata_2016-07.csv\r\n",
      "data/sampled/green_tripdata_2016-08.csv\r\n",
      "data/sampled/green_tripdata_2016-09.csv\r\n",
      "data/sampled/green_tripdata_2016-10.csv\r\n",
      "data/sampled/green_tripdata_2016-11.csv\r\n",
      "data/sampled/green_tripdata_2016-12.csv\r\n",
      "data/sampled/green_tripdata_2017-01.csv\r\n",
      "data/sampled/green_tripdata_2017-02.csv\r\n",
      "data/sampled/green_tripdata_2017-03.csv\r\n",
      "data/sampled/green_tripdata_2017-04.csv\r\n",
      "data/sampled/green_tripdata_2017-05.csv\r\n",
      "data/sampled/green_tripdata_2017-06.csv\r\n",
      "data/sampled/green_tripdata_2017-07.csv\r\n",
      "data/sampled/green_tripdata_2017-08.csv\r\n",
      "data/sampled/green_tripdata_2017-09.csv\r\n",
      "data/sampled/green_tripdata_2017-10.csv\r\n",
      "data/sampled/green_tripdata_2017-11.csv\r\n",
      "data/sampled/green_tripdata_2017-12.csv\r\n",
      "data/sampled/green_tripdata_2018-01.csv\r\n",
      "data/sampled/green_tripdata_2018-02.csv\r\n",
      "data/sampled/green_tripdata_2018-03.csv\r\n",
      "data/sampled/green_tripdata_2018-04.csv\r\n",
      "data/sampled/green_tripdata_2018-05.csv\r\n",
      "data/sampled/green_tripdata_2018-06.csv\r\n",
      "data/sampled/green_tripdata_2018-07.csv\r\n",
      "data/sampled/green_tripdata_2018-08.csv\r\n",
      "data/sampled/green_tripdata_2018-09.csv\r\n",
      "data/sampled/green_tripdata_2018-10.csv\r\n",
      "data/sampled/green_tripdata_2018-11.csv\r\n",
      "data/sampled/green_tripdata_2018-12.csv\r\n",
      "data/sampled/green_tripdata_2019-01.csv\r\n",
      "data/sampled/green_tripdata_2019-02.csv\r\n",
      "data/sampled/green_tripdata_2019-03.csv\r\n",
      "data/sampled/green_tripdata_2019-04.csv\r\n",
      "data/sampled/green_tripdata_2019-05.csv\r\n",
      "data/sampled/green_tripdata_2019-06.csv\r\n",
      "data/sampled/green_tripdata_2020-01.csv\r\n",
      "data/sampled/green_tripdata_2020-02.csv\r\n",
      "data/sampled/green_tripdata_2020-04.csv\r\n",
      "data/sampled/green_tripdata_2020-05.csv\r\n",
      "data/sampled/green_tripdata_2020-06.csv\r\n"
     ]
    }
   ],
   "source": [
    "print ('list of yellow tripdata files:')\n",
    "!ls data/sampled/yellow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
