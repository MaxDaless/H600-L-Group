{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040\n",
    "\n",
    "## FUNCTION DECLARATION\n",
    "\n",
    "# Creation of a function to convert lat-lon into location ID\n",
    "def convertlocID(lon, lat):\n",
    "    global locationID # access the outer scope variable by declaring it global\n",
    "    if lon != None and lat != None and lon < -73.0 and lon > -75.0 and lat > 40.0 and lat < 42.0:\n",
    "        query_point = Point( lon, lat)\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "        for i in range(0,len(possible_matches)) :\n",
    "            if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "                locationID = possible_matches[i]\n",
    "    else:\n",
    "        locationID = 9999\n",
    "    \n",
    "    return locationID\n",
    "\n",
    "# Check if the value is null or not\n",
    "def blank_as_null(x):\n",
    "\n",
    "    return f.when(col(x).isNull(), 0 ).otherwise(col(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory data/cleaned already exist\n",
      "The directory data/cleaned/yellow already exist\n",
      "The directory data/cleaned/green already exist\n",
      "The directory data/cleaned/fhv already exist\n",
      "The directory data/cleaned/fhvhv already exist\n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "isdir = os.path.isdir(\"data/cleaned\")  \n",
    "if isdir == False :\n",
    "    print (\"Need to create directory data/cleaned\")\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "else:\n",
    "    print (\"The directory data/cleaned already exist\")\n",
    "    \n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    isdir = os.path.isdir(path)\n",
    "    if isdir == False :\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path) \n",
    "    else:\n",
    "        print (\"The directory %s already exist\" % path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We then donc need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= '/data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleanned/fhvhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 (from 2015-1 to 2016-12): \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 (from 2017-1 to 2017-6): \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 (from 2017-7 to 2017-12): \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 (from 2018-1 to 2018-12):\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Final schema 5 (from 2019-1 to 2020-6):\n",
    "            NO change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        fhv_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                if nb_schema+1 == 1 :\n",
    "                    fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                           .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                           .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                           .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                            \"dropoff_datetime\",\n",
    "                            col(\"locationID\").alias(\"PULocationID\"),\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                    fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                            .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                            .select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    fhv3_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    fhv4_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::], index = False)\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::], index = False)\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "From previous analyse we decide to use as reference for the GREEN taxi files the following schema:\n",
    "\n",
    "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 (from 2013-8 to 2014-12): \n",
    "            a) Two new columns are add : congestion_surcharge and improvement_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "           \n",
    "- Change in schema 2 (from 2015-1 to 2016-7):\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Change in schema 3 (from 2016-7 to 2018-12):\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Final schema 4 (from 2019-1 to 2020-6):\n",
    "            NO change\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ca6213680244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnb_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# List the file from the same taxi company brand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/sampled/%s_*.csv\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_brand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnb_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_files\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Save in list the files name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='green'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        green_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    green_DF = green_DF.withColumn(\"Dropoff_longitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Dropoff_latitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Pickup_latitude\", blank_as_null(\"Pickup_latitude\"))\\\n",
    "                           .withColumn(\"Pickup_longitude\", blank_as_null(\"Pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green1_DF = DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                             .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2\")\n",
    "                    green_DF = green_DF.withColumn(\"Dropoff_longitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Dropoff_latitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Pickup_latitude\", blank_as_null(\"Pickup_latitude\"))\\\n",
    "                           .withColumn(\"Pickup_longitude\", blank_as_null(\"Pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green2_DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    green3_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    green4_DF = green_DF.select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "        if date_file == dating_schema[4].date() :\n",
    "            print(\"schema LAST year\")\n",
    "            green4_DF = green_DF.select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "            green4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow files\n",
    "\n",
    "Yellow files\n",
    "\n",
    "From previous analyse we decided to use the following schema as a reference for the YELLOW taxi files:\n",
    "\n",
    "['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "\n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 (from 2009-1 to 2009-12) :\n",
    "            a)Columns transformations:\n",
    "                  -'vendor_name' => 'vendorid'\n",
    "                  -'Trip_Pickup_DateTime' => 'tpep_pickup_datetime'\n",
    "                  -'Trip_Dropoff_DateTime' => 'tpep_dropoff_datetime'\n",
    "                  -'Passenger_Count' => 'passenger_count'\n",
    "                  -'Trip_Distance' => 'trip_distance'\n",
    "                  -'Rate_Code' => 'ratecodeid'\n",
    "                  -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                  -'Start_Lon','Start_Lat' => 'pulocationid'\n",
    "                  -'End_Lon','End_Lat' => 'dolocationid'\n",
    "                  -'Payment_Type' => 'payment_type'\n",
    "                  -'Fare_Amt' => 'fare_amount'\n",
    "                  -'surcharge' => 'extra'\n",
    "                  -'Tip_Amt' => 'tip_amount'\n",
    "                  -'Tolls_Amt' => 'tolls_amount'\n",
    "                  -'Total_Amt' => 'total_amount'     \n",
    "          b) Columns to add:\n",
    "                  -'congestion_surcharge'\n",
    "                  -'improvement_surcharge'\n",
    "\n",
    "- Change schema 2 (from 2010-1 to 2014-12):\n",
    "            a)Columns transformations:\n",
    "                  -'vendor_id' => 'VendorID'\n",
    "                  -'pickup_datetime' => 'tpep_pickup_datetime'\n",
    "                  -'dropoff_datetime' => 'tpep_dropoff_datetime'\n",
    "                  -'Trip_Distance' => 'trip_distance'\n",
    "                  -'rate_code' => 'ratecodeID'\n",
    "                  -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                  -'pickup_longitude','pickup_latitude' => 'pulocationid'\n",
    "                  -'dropoff_longitude','dropoff_latitude' => 'dolocationid'   \n",
    "                  -'surcharge' => 'extra'\n",
    "          b) Columns to add:\n",
    "                  -'congestion_surcharge'\n",
    "                  -'improvement_surcharge'\n",
    "\n",
    "- Change in schema 3 (from 2015-1 to 2016-7):\n",
    "          a)Columns transformations:\n",
    "              -'RateCodeID' => 'ratecodeid'\n",
    "              -'store_and_forward' => 'store_and_fwd_flag'\n",
    "              -'pickup_longitude','pickup_latitude' => 'puLocationid'\n",
    "              -'dropoff_longitude','dropoff_latitude' => 'DOLocationid                 \n",
    "          b) One new column to add : congestion_surcharge\n",
    "\n",
    "- Change in schema 4 (from 2016-7 to 2018-12):\n",
    "          a) One new column to add : congestion_surcharge\n",
    "\n",
    "- Final schema 5 (from 2019-1 to 2020-6):\n",
    "          a) Lowercasing header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-01.csv\n",
      "2009-02-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-02.csv\n",
      "2009-03-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-03.csv\n",
      "2009-04-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-04.csv\n",
      "2009-05-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-05.csv\n",
      "2009-06-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-06.csv\n",
      "2009-07-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-07.csv\n",
      "2009-08-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-08.csv\n",
      "2009-09-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-09.csv\n",
      "2009-10-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-10.csv\n",
      "2009-11-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-11.csv\n",
      "2009-12-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-12.csv\n",
      "2010-01-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-01.csv\n",
      "29739 29739\n",
      "2010-02-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-02.csv\n",
      "22291 22291\n",
      "2010-03-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-03.csv\n",
      "25768 25768\n",
      "2010-04-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-04.csv\n",
      "30298 30298\n",
      "2010-05-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-05.csv\n",
      "30969 30969\n",
      "2010-06-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-06.csv\n",
      "29658 29658\n",
      "2010-07-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-07.csv\n",
      "29321 29321\n",
      "2010-08-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-08.csv\n",
      "25056 25056\n",
      "2010-09-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-09.csv\n",
      "31085 31085\n",
      "2010-10-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-10.csv\n",
      "28401 28401\n",
      "2010-11-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-11.csv\n",
      "27826 27826\n",
      "2010-12-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2010-12.csv\n",
      "27638 27638\n",
      "2011-01-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-01.csv\n",
      "26932 26932\n",
      "2011-02-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-02.csv\n",
      "28411 28411\n",
      "2011-03-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-03.csv\n",
      "32150 32150\n",
      "2011-04-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-04.csv\n",
      "29443 29443\n",
      "2011-05-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-05.csv\n",
      "31115 31115\n",
      "2011-06-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-06.csv\n",
      "30201 30201\n",
      "2011-07-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-07.csv\n",
      "29490 29490\n",
      "2011-08-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-08.csv\n",
      "26529 26529\n",
      "2011-09-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-09.csv\n",
      "29261 29261\n",
      "2011-10-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-10.csv\n",
      "31423 31423\n",
      "2011-11-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-11.csv\n",
      "29062 29062\n",
      "2011-12-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2011-12.csv\n",
      "29861 29861\n",
      "2012-01-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-01.csv\n",
      "29943 29943\n",
      "2012-02-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-02.csv\n",
      "29971 29971\n",
      "2012-03-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-03.csv\n",
      "32300 32300\n",
      "2012-04-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-04.csv\n",
      "30959 30959\n",
      "2012-05-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-05.csv\n",
      "31142 31142\n",
      "2012-06-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-06.csv\n",
      "30197 30197\n",
      "2012-07-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-07.csv\n",
      "28767 28767\n",
      "2012-08-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-08.csv\n",
      "28771 28771\n",
      "2012-09-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-09.csv\n",
      "29098 29098\n",
      "2012-10-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-10.csv\n",
      "29056 29056\n",
      "2012-11-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-11.csv\n",
      "27554 27554\n",
      "2012-12-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2012-12.csv\n",
      "29401 29401\n",
      "2013-01-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-01.csv\n",
      "29556 29556\n",
      "2013-02-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-02.csv\n",
      "27988 27988\n",
      "2013-03-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-03.csv\n",
      "31501 31501\n",
      "2013-04-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-04.csv\n",
      "30202 30202\n",
      "2013-05-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-05.csv\n",
      "30575 30575\n",
      "2013-06-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-06.csv\n",
      "28767 28767\n",
      "2013-07-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-07.csv\n",
      "27642 27642\n",
      "2013-08-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-08.csv\n",
      "25190 25190\n",
      "2013-09-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-09.csv\n",
      "28221 28221\n",
      "2013-10-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-10.csv\n",
      "30008 30008\n",
      "2013-11-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-11.csv\n",
      "28786 28786\n",
      "2013-12-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2013-12.csv\n",
      "27941 27941\n",
      "2014-01-01\n",
      "schema 2 for file: data/sampled/yellow_tripdata_2014-01.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`dropoff_longitude`' given input columns: [ dropoff_longitude,  pickup_longitude,  passenger_count,  dropoff_datetime, vendor_id,  store_and_fwd_flag,  pickup_datetime,  rate_code,  fare_amount,  mta_tax,  pickup_latitude,  tip_amount,  trip_distance,  surcharge,  dropoff_latitude,  tolls_amount,  total_amount,  payment_type];;\\n'Project [vendor_id#72238,  pickup_datetime#72239,  dropoff_datetime#72240,  passenger_count#72241,  trip_distance#72242,  pickup_longitude#72243,  pickup_latitude#72244,  rate_code#72245,  store_and_fwd_flag#72246,  dropoff_longitude#72247,  dropoff_latitude#72248,  payment_type#72249,  fare_amount#72250,  surcharge#72251,  mta_tax#72252,  tip_amount#72253,  tolls_amount#72254,  total_amount#72255, CASE WHEN isnull('dropoff_longitude) THEN 0 ELSE 'dropoff_longitude END AS dropoff_longitude#72274]\\n+- Relation[vendor_id#72238, pickup_datetime#72239, dropoff_datetime#72240, passenger_count#72241, trip_distance#72242, pickup_longitude#72243, pickup_latitude#72244, rate_code#72245, store_and_fwd_flag#72246, dropoff_longitude#72247, dropoff_latitude#72248, payment_type#72249, fare_amount#72250, surcharge#72251, mta_tax#72252, tip_amount#72253, tolls_amount#72254, total_amount#72255] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o29429.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`dropoff_longitude`' given input columns: [ dropoff_longitude,  pickup_longitude,  passenger_count,  dropoff_datetime, vendor_id,  store_and_fwd_flag,  pickup_datetime,  rate_code,  fare_amount,  mta_tax,  pickup_latitude,  tip_amount,  trip_distance,  surcharge,  dropoff_latitude,  tolls_amount,  total_amount,  payment_type];;\n'Project [vendor_id#72238,  pickup_datetime#72239,  dropoff_datetime#72240,  passenger_count#72241,  trip_distance#72242,  pickup_longitude#72243,  pickup_latitude#72244,  rate_code#72245,  store_and_fwd_flag#72246,  dropoff_longitude#72247,  dropoff_latitude#72248,  payment_type#72249,  fare_amount#72250,  surcharge#72251,  mta_tax#72252,  tip_amount#72253,  tolls_amount#72254,  total_amount#72255, CASE WHEN isnull('dropoff_longitude) THEN 0 ELSE 'dropoff_longitude END AS dropoff_longitude#72274]\n+- Relation[vendor_id#72238, pickup_datetime#72239, dropoff_datetime#72240, passenger_count#72241, trip_distance#72242, pickup_longitude#72243, pickup_latitude#72244, rate_code#72245, store_and_fwd_flag#72246, dropoff_longitude#72247, dropoff_latitude#72248, payment_type#72249, fare_amount#72250, surcharge#72251, mta_tax#72252, tip_amount#72253, tolls_amount#72254, total_amount#72255] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:354)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:354)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3407)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1335)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2253)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2220)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-28e33e97b50f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mnb_schema\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"schema 2 for file:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0myellow_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myellow_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropoff_longitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank_as_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropoff_longitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropoff_latitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank_as_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropoff_latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pickup_latitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank_as_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pickup_latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \"\"\"\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`dropoff_longitude`' given input columns: [ dropoff_longitude,  pickup_longitude,  passenger_count,  dropoff_datetime, vendor_id,  store_and_fwd_flag,  pickup_datetime,  rate_code,  fare_amount,  mta_tax,  pickup_latitude,  tip_amount,  trip_distance,  surcharge,  dropoff_latitude,  tolls_amount,  total_amount,  payment_type];;\\n'Project [vendor_id#72238,  pickup_datetime#72239,  dropoff_datetime#72240,  passenger_count#72241,  trip_distance#72242,  pickup_longitude#72243,  pickup_latitude#72244,  rate_code#72245,  store_and_fwd_flag#72246,  dropoff_longitude#72247,  dropoff_latitude#72248,  payment_type#72249,  fare_amount#72250,  surcharge#72251,  mta_tax#72252,  tip_amount#72253,  tolls_amount#72254,  total_amount#72255, CASE WHEN isnull('dropoff_longitude) THEN 0 ELSE 'dropoff_longitude END AS dropoff_longitude#72274]\\n+- Relation[vendor_id#72238, pickup_datetime#72239, dropoff_datetime#72240, passenger_count#72241, trip_distance#72242, pickup_longitude#72243, pickup_latitude#72244, rate_code#72245, store_and_fwd_flag#72246, dropoff_longitude#72247, dropoff_latitude#72248, payment_type#72249, fare_amount#72250, surcharge#72251, mta_tax#72252, tip_amount#72253, tolls_amount#72254, total_amount#72255] csv\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='yellow'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        yellow_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        #test = yellow_DF.withColumn(\"dropoff_longitude\", f.when(col(\"dropoff_longitude\").isNull, 0).otherwise(col(\"dropoff_longitude\")))\n",
    "        #yellow_DF.printSchema()\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    yellow_DF = yellow_DF.withColumn(\"End_Lon\", blank_as_null(\"End_Lon\"))\\\n",
    "                             .withColumn(\"End_Lat\", blank_as_null(\"End_Lat\"))\\\n",
    "                             .withColumn(\"Start_Lat\", blank_as_null(\"Start_Lat\"))\\\n",
    "                             .withColumn(\"Start_Lon\", blank_as_null(\"Start_Lon\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('Start_Lat')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('Start_Lon')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('End_Lat')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('End_Lon')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow1_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                          f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                          f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                          .select(\n",
    "                                                               col(\"vendor_name\").alias(\"vendorid\"),\n",
    "                                                               col(\"Trip_Pickup_DateTime\").alias(\"tpep_pickup_datetime\"),\n",
    "                                                               col(\"Trip_Dropoff_DateTime\").alias(\"tpep_dropoff_datetime\"),\n",
    "                                                               col(\"Passenger_Count\").alias(\"passenger_count\"),\n",
    "                                                               col(\"Trip_Distance\").alias(\"trip_distance\"),\n",
    "                                                               col(\"Rate_Code\").alias(\"ratecodeid\"),\n",
    "                                                               col(\"store_and_forward\").alias(\"store_and_fwd_flag\"),\n",
    "                                                               \"pulocationid\",\n",
    "                                                               \"dolocationid\",\n",
    "                                                               col(\"Payment_Type\").alias(\"payment_type\"),\n",
    "                                                               col(\"Fare_Amt\").alias(\"fare_amount\"),\n",
    "                                                               col(\"surcharge\").alias(\"extra\"),\n",
    "                                                               \"mta_tax\",\n",
    "                                                               col(\"Tip_Amt\").alias(\"tip_amount\"),\n",
    "                                                               col(\"Tolls_Amt\").alias(\"tolls_amount\"),\n",
    "                                                               \"improvement_surcharge\",\n",
    "                                                               col(\"Total_Amt\").alias(\"total_amount\"),\n",
    "                                                               \"congestion_surcharge\")\n",
    "                    yellow1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2 for file:\",list_files[yr])\n",
    "                    yellow_DF = yellow_DF.withColumn(\"dropoff_longitude\", blank_as_null(\"dropoff_longitude\"))\\\n",
    "                             .withColumn(\"dropoff_latitude\", blank_as_null(\"dropoff_latitude\"))\\\n",
    "                             .withColumn(\"pickup_latitude\", blank_as_null(\"pickup_latitude\"))\\\n",
    "                             .withColumn(\"pickup_longitude\", blank_as_null(\"pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    print(len(Dropoff_list_lon),len(Dropoff_list_lat))\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        #print(i,Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow2_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"vendor_id\").alias(\"vendorid\"),\n",
    "                                                            col(\"pickup_datetime\").alias(\"tpep_pickup_datetime\"),\n",
    "                                                            col(\"dropoff_datetime\").alias(\"tpep_dropoff_datetime\"),\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"rate_code\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            col(\"surcharge\").alias(\"extra\"),\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\" ,\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\") \n",
    "                    yellow2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    yellow_DF = yellow_DF.withColumn(\"dropoff_longitude\", blank_as_null(\"dropoff_longitude\"))\\\n",
    "                             .withColumn(\"dropoff_latitude\", blank_as_null(\"dropoff_latitude\"))\\\n",
    "                             .withColumn(\"pickup_latitude\", blank_as_null(\"pickup_latitude\"))\\\n",
    "                             .withColumn(\"pickup_longitude\", blank_as_null(\"pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow3_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            \"tpep_pickup_datetime\",\n",
    "                                                            \"tpep_dropoff_datetime\",\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    yellow3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    # Create the new file\n",
    "                    yellow4_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            \"tpep_pickup_datetime\",\n",
    "                                                            \"tpep_dropoff_datetime\",\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                                            col(\"DOLocationID\").alias(\"dulocationid\"),\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")    \n",
    "\n",
    "                    yellow4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    print(\"schema LAST\")\n",
    "                    # Create the new file\n",
    "                    yellow5_DF = yellow_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                            \"tpep_pickup_datetime\",\n",
    "                                            \"tpep_dropoff_datetime\",\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"dolocationid\"),\n",
    "                                            \"payment_type\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"congestion_surcharge\")\n",
    "                    yellow5_DF = green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            print(\"schema LAST year\")\n",
    "            yellow5_DF = yellow_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                            \"tpep_pickup_datetime\",\n",
    "                                            \"tpep_dropoff_datetime\",\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"dolocationid\"),\n",
    "                                            \"payment_type\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"congestion_surcharge\")\n",
    "            yellow5_DF = green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
