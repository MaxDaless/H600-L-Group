{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory data/t2 failed\n",
      "Creation of the directory data/t2/yellow failed\n",
      "Creation of the directory data/t2/green failed\n",
      "Creation of the directory data/t2/fhv failed\n",
      "Creation of the directory data/t2/fhvhv failed\n"
     ]
    }
   ],
   "source": [
    "#create t2 directories\n",
    "try:\n",
    "    os.mkdir(\"data/t2\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory data/t2 failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/t2\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/t2/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.mkdir(path)  \n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We can then create a unified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of fhvhv tripdata files:\n",
      "fhvhv_tripdata_2019-02.csv  fhvhv_tripdata_2020-01.csv\n",
      "fhvhv_tripdata_2019-03.csv  fhvhv_tripdata_2020-03.csv\n",
      "fhvhv_tripdata_2019-04.csv  fhvhv_tripdata_2020-04.csv\n",
      "fhvhv_tripdata_2019-05.csv  fhvhv_tripdata_2020-05.csv\n",
      "fhvhv_tripdata_2019-06.csv  fhvhv_tripdata_2020-06.csv\n",
      "count of fhvhv tripdata files:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#export new csv file\n",
    "#fhvhv_DF.write.save(path='data/modified/fhvhv.csv', format='csv', mode='append', sep='\\t')\n",
    "#Ã§a fait un dossier ac plusieurs fichiers pourris\n",
    "\n",
    "#move files to t2 directory\n",
    "source_dir= 'data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/t2/fhvhv')\n",
    "\n",
    "print ('list of fhvhv tripdata files:')\n",
    "!ls data/t2/fhvhv\n",
    "\n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find data/t2/fhvhv -type f | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyses we saw that for fhv there are some adjustements:\n",
    "\n",
    "In 2017 - 1 :\n",
    "\n",
    "4 diff on a total of 5 col: ['pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid']\n",
    "           2/4 col add\n",
    "           2/4 name change\n",
    "\n",
    "In 2017 - 7 :\n",
    "\n",
    "1 diff on a total of 6 col: ['sr_flag']\n",
    "           1/1 col add\n",
    "\n",
    "In 2018 - 1 :\n",
    "\n",
    "1 diff on a total of 7 col: ['dispatching_base_number']\n",
    "           1/1 col add\n",
    "\n",
    "In 2019 - 1 :\n",
    "\n",
    "1 diff on a total of 6 col: ['dispatching_base_number']\n",
    "           1/1 col remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of fhvhv tripdata files:\n",
      "fhv_tripdata_2015-01.csv  fhv_tripdata_2016-11.csv  fhv_tripdata_2018-09.csv\n",
      "fhv_tripdata_2015-02.csv  fhv_tripdata_2016-12.csv  fhv_tripdata_2018-10.csv\n",
      "fhv_tripdata_2015-03.csv  fhv_tripdata_2017-01.csv  fhv_tripdata_2018-11.csv\n",
      "fhv_tripdata_2015-04.csv  fhv_tripdata_2017-02.csv  fhv_tripdata_2018-12.csv\n",
      "fhv_tripdata_2015-05.csv  fhv_tripdata_2017-03.csv  fhv_tripdata_2019-01.csv\n",
      "fhv_tripdata_2015-06.csv  fhv_tripdata_2017-04.csv  fhv_tripdata_2019-02.csv\n",
      "fhv_tripdata_2015-07.csv  fhv_tripdata_2017-05.csv  fhv_tripdata_2019-03.csv\n",
      "fhv_tripdata_2015-08.csv  fhv_tripdata_2017-06.csv  fhv_tripdata_2019-04.csv\n",
      "fhv_tripdata_2015-09.csv  fhv_tripdata_2017-07.csv  fhv_tripdata_2019-05.csv\n",
      "fhv_tripdata_2015-10.csv  fhv_tripdata_2017-08.csv  fhv_tripdata_2019-06.csv\n",
      "fhv_tripdata_2015-11.csv  fhv_tripdata_2017-09.csv  fhv_tripdata_2019-07.csv\n",
      "fhv_tripdata_2015-12.csv  fhv_tripdata_2017-10.csv  fhv_tripdata_2019-08.csv\n",
      "fhv_tripdata_2016-01.csv  fhv_tripdata_2017-11.csv  fhv_tripdata_2019-09.csv\n",
      "fhv_tripdata_2016-02.csv  fhv_tripdata_2017-12.csv  fhv_tripdata_2019-10.csv\n",
      "fhv_tripdata_2016-03.csv  fhv_tripdata_2018-01.csv  fhv_tripdata_2019-11.csv\n",
      "fhv_tripdata_2016-04.csv  fhv_tripdata_2018-02.csv  fhv_tripdata_2020-01.csv\n",
      "fhv_tripdata_2016-05.csv  fhv_tripdata_2018-03.csv  fhv_tripdata_2020-03.csv\n",
      "fhv_tripdata_2016-06.csv  fhv_tripdata_2018-04.csv  fhv_tripdata_2020-04.csv\n",
      "fhv_tripdata_2016-07.csv  fhv_tripdata_2018-05.csv  fhv_tripdata_2020-05.csv\n",
      "fhv_tripdata_2016-08.csv  fhv_tripdata_2018-06.csv  fhv_tripdata_2020-06.csv\n",
      "fhv_tripdata_2016-09.csv  fhv_tripdata_2018-07.csv\n",
      "fhv_tripdata_2016-10.csv  fhv_tripdata_2018-08.csv\n",
      "count of fhvhv tripdata files:\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'       \n",
    "#or filename in glob.glob(os.path.join(source_dir,'fhv_*.csv')):\n",
    "#   shutil.copy(filename, 'data/t2/fhv')\n",
    "\n",
    "#rint ('list of fhv tripdata files:')\n",
    "#ls data/t2/fhv\n",
    "\n",
    "#rint ('count of fhv tripdata files:')\n",
    "#find data/t2/fhv -type f | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pickup_DateTime: timestamp (nullable = true)\n",
      " |-- DropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Dispatching_base_number: string (nullable = true)\n",
      " |-- Dispatching_base_num: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+------------+------------+-------+-----------------------+--------------------+\n",
      "|    Pickup_DateTime|   DropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Dispatching_base_number|Dispatching_base_num|\n",
      "+-------------------+-------------------+------------+------------+-------+-----------------------+--------------------+\n",
      "|2018-01-20 20:07:22|2018-01-20 20:16:51|          62|          17|      1|                 B02510|                null|\n",
      "+-------------------+-------------------+------------+------------+-------+-----------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|              B02872|2019-01-23 09:17:46|2019-01-23 09:44:21|         181|         137|   null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read fhv files\n",
    "#fhv1_DF = (spark.read\n",
    "#           .option(\"sep\", \",\")\n",
    "#           .option(\"header\", True)\n",
    "#           .option(\"inferSchema\", True)\n",
    "#            .csv('data/sampled/fhv_tripdata_2015-01.csv'))\n",
    "#fhv1_DF.printSchema()\n",
    "#fhv1_DF.show(1)\n",
    "#fhv2_DF = (spark.read\n",
    "#           .option(\"sep\", \",\")\n",
    "#           .option(\"header\", True)\n",
    "#           .option(\"inferSchema\", True)\n",
    "#            .csv('data/sampled/fhv_tripdata_2017-01.csv'))\n",
    "#fhv2_DF.printSchema()\n",
    "##fhv2_DF.show(1)\n",
    "#fhv3_DF = (spark.read\n",
    "#           .option(\"sep\", \",\")\n",
    "#           .option(\"header\", True)\n",
    "#           .option(\"inferSchema\", True)\n",
    "#            .csv('data/sampled/fhv_tripdata_2017-07.csv'))\n",
    "#fhv3_DF.printSchema()\n",
    "#fhv3_DF.show(1)\n",
    "fhv4_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/fhv_tripdata_2018-01.csv'))\n",
    "fhv4_DF.printSchema()\n",
    "fhv4_DF.show(1)\n",
    "fhv5_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/fhv_tripdata_2019-01.csv'))\n",
    "fhv5_DF.printSchema()\n",
    "fhv5_DF.show(1)\n",
    "\n",
    "#avant 01/2015 il faut ajouter |dropoff_datetime||DOLocationID|SR_Flag\n",
    "# modifier Pickup_date en pickup_datetime, locationID en PULocationID, Dispatching_base_num en dispatching_base_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|              B02764|2015-01-09 23:28:06|            null|         186|        null|   null|\n",
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|              B02914|2017-01-19 22:09:39|            null|        null|        null|   null|\n",
      "+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|              B02879|2017-07-10 06:35:56|2017-07-10 06:45:17|         249|         163|   null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|                null|2018-01-20 20:07:22|2018-01-20 20:16:51|          62|          17|      1|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "#file_name='fhv_tripdata_2015-01.csv'\n",
    "#source_dir= 'data/sampled/' \n",
    "#dest_dir='data/t2/fhv'\n",
    "\n",
    "#1.modifing schema as needed\n",
    "fhv_201501 = fhv1_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                    .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                    .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                    .select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                        \"dropoff_datetime\",\n",
    "                        col(\"locationID\").alias(\"PULocationID\"),\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "fhv_201501.show(1)\n",
    "#saving new file                                     \n",
    "#export new csv file\n",
    "#fhv_201501.write.save(path='data/t2/fhv/fhv_tripdata_2015-01.csv', format='csv', mode='append', sep=',')\n",
    "#print ('list of fhv tripdata files:')\n",
    "#!ls data/t2/fhv\n",
    "\n",
    "#2.modifing schema as needed\n",
    "fhv_201701 = fhv2_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                    .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                    .select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                        col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                        \"PULocationID\",\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "fhv_201701.show(1)\n",
    "#3.modifing schema as needed\n",
    "fhv_201707 = fhv3_DF.select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                        col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                        \"PULocationID\",\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "fhv_201707.show(1)\n",
    "#2.modifing schema as needed\n",
    "fhv_201801 =  fhv4_DF.select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                        col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                        \"PULocationID\",\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "fhv_201801.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "For green there are 76 files:\n",
    "\n",
    "In 2015 - 1 :\n",
    "\n",
    "   1 diff on a total of 21 col: ['improvement_surcharge']\n",
    "           1/1 col add\n",
    "\n",
    "In 2016 - 7 :\n",
    "\n",
    "   2 diff on a total of 19 col: ['pulocationid', 'dolocationid']\n",
    "           2/2 col remove\n",
    "\n",
    "In 2019 - 1 :\n",
    "\n",
    "   1 diff on a total of 20 col: ['congestion_surcharge']\n",
    "           1/1 col add\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##.Yellow files\n",
    "\n",
    "For yellow there are 131 files:\n",
    "\n",
    " In 2010 - 1 :\n",
    " \n",
    "   12 diff on a total of 18 col: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "         12/12 column name have changed:\n",
    "         \n",
    " In 2015 - 1 :\n",
    " \n",
    "   6 diff on a total of 19 col: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
    "         1/6 col add\n",
    "         5/6 name change\n",
    "         \n",
    " In 2016 - 7 :\n",
    " \n",
    "   2 diff on a total of 17 col: ['pulocationid', 'dolocationid']\n",
    "         2/2 col remove\n",
    "         \n",
    " In 2019 - 1 :\n",
    " \n",
    "   1 diff on a total of 18 col: ['congestion_surcharge']\n",
    "         1/1 col add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
