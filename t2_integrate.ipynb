{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory data/t2 failed\n",
      "Creation of the directory data/t2/yellow failed\n",
      "Creation of the directory data/t2/green failed\n",
      "Creation of the directory data/t2/fhv failed\n",
      "Creation of the directory data/t2/fhvhv failed\n"
     ]
    }
   ],
   "source": [
    "#create t2 directories\n",
    "try:\n",
    "    os.mkdir(\"data/t2\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory data/t2 failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/t2\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/t2/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.mkdir(path)  \n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We can then create a unified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of fhvhv tripdata files:\n",
      "fhvhv_tripdata_2019-02.csv  fhvhv_tripdata_2020-01.csv\n",
      "fhvhv_tripdata_2019-03.csv  fhvhv_tripdata_2020-03.csv\n",
      "fhvhv_tripdata_2019-04.csv  fhvhv_tripdata_2020-04.csv\n",
      "fhvhv_tripdata_2019-05.csv  fhvhv_tripdata_2020-05.csv\n",
      "fhvhv_tripdata_2019-06.csv  fhvhv_tripdata_2020-06.csv\n",
      "count of fhvhv tripdata files:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#read fhvhv files\n",
    "#fhvhv_DF = (spark.read\n",
    "#           .option(\"sep\", \"\\t\")\n",
    "#           .option(\"header\", True)\n",
    "#           .option(\"inferSchema\", True)\n",
    "#            .csv('data/sampled/fhvhv_*.csv'))\n",
    "\n",
    "#fhvhv_DF.printSchema()\n",
    "#fhvhv_DF.show(5)\n",
    "\n",
    "#export new csv file\n",
    "#fhvhv_DF.write.save(path='data/modified/fhvhv.csv', format='csv', mode='append', sep='\\t')\n",
    "#Ã§a fait un dossier ac plusieurs fichiers pourris\n",
    "\n",
    "source_dir= 'data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/t2/fhvhv')\n",
    "\n",
    "print ('list of fhvhv tripdata files:')\n",
    "!ls data/t2/fhvhv\n",
    "\n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find data/t2/fhvhv -type f | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyses we saw that for fhv there are some adjustements:\n",
    "\n",
    "In 2017 - 1 :\n",
    "\n",
    "4 diff on a total of 5 col: ['pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid']\n",
    "           2/4 col add\n",
    "           2/4 name change\n",
    "\n",
    "In 2017 - 7 :\n",
    "\n",
    "1 diff on a total of 6 col: ['sr_flag']\n",
    "           1/1 col add\n",
    "\n",
    "In 2018 - 1 :\n",
    "\n",
    "1 diff on a total of 7 col: ['dispatching_base_number']\n",
    "           1/1 col add\n",
    "\n",
    "In 2019 - 1 :\n",
    "\n",
    "1 diff on a total of 6 col: ['dispatching_base_number']\n",
    "           1/1 col remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
