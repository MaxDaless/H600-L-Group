{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory data/cleaned\n",
      "Successfully created the directory data/cleaned/yellow \n",
      "Successfully created the directory data/cleaned/green \n",
      "Successfully created the directory data/cleaned/fhv \n",
      "Successfully created the directory data/cleaned/fhvhv \n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "try :  \n",
    "    os.path.isdir(\"data/cleaned\")\n",
    "except OSError:\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "    print (\"Creation of the directory data/cleaned failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/cleaned\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.path.isdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We then donc need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= '/data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleanned/fhvhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 : \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 : \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 :\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Final schema 5 :\n",
    "           No change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        fhv_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            print(date_file)\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                if nb_schema+1 == 1 :\n",
    "                    fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                           .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                           .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                           .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                            \"dropoff_datetime\",\n",
    "                            col(\"locationID\").alias(\"PULocationID\"),\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                    fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                            .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                            .select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    fhv3_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    fhv4_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "From previous analyse we decided to use the following schema as a reference for the GREEN taxi files:\n",
    "\n",
    "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    " \n",
    "We therefore need to apply some transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Two new columns to add : congestion_surcharge and improvement_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "           \n",
    "- Change in schema 2 :\n",
    "            a) One new column to add : congestion_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Change in schema 3 :\n",
    "            a) One new column to add : congestion_surcharge\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Final schema 4 :\n",
    "            lowercasing header\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-08-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-08.csv\n",
      "2013-09-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-09.csv\n",
      "2013-10-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-10.csv\n",
      "2013-11-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-11.csv\n",
      "2013-12-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-12.csv\n",
      "2014-01-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-01.csv\n",
      "2014-02-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-02.csv\n",
      "2014-03-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-03.csv\n",
      "2014-04-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-04.csv\n",
      "2014-05-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-05.csv\n",
      "2014-06-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-06.csv\n",
      "2014-07-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-07.csv\n",
      "2014-08-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-08.csv\n",
      "2014-09-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-09.csv\n",
      "2014-10-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-10.csv\n",
      "2014-11-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-11.csv\n",
      "2014-12-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-12.csv\n",
      "2015-01-01\n",
      "schema 2\n",
      "2015-02-01\n",
      "schema 2\n",
      "2015-03-01\n",
      "schema 2\n",
      "2015-04-01\n",
      "schema 2\n",
      "2015-05-01\n",
      "schema 2\n",
      "2015-06-01\n",
      "schema 2\n",
      "2015-07-01\n",
      "schema 2\n",
      "2015-08-01\n",
      "schema 2\n",
      "2015-09-01\n",
      "schema 2\n",
      "2015-10-01\n",
      "schema 2\n",
      "2015-11-01\n",
      "schema 2\n",
      "2015-12-01\n",
      "schema 2\n",
      "2016-01-01\n",
      "schema 2\n",
      "2016-02-01\n",
      "schema 2\n",
      "2016-03-01\n",
      "schema 2\n",
      "2016-04-01\n",
      "schema 2\n",
      "2016-05-01\n",
      "schema 2\n",
      "2016-06-01\n",
      "schema 2\n",
      "2016-07-01\n",
      "schema 3\n",
      "2016-08-01\n",
      "schema 3\n",
      "2016-09-01\n",
      "schema 3\n",
      "2016-10-01\n",
      "schema 3\n",
      "2016-11-01\n",
      "schema 3\n",
      "2016-12-01\n",
      "schema 3\n",
      "2017-01-01\n",
      "schema 3\n",
      "2017-02-01\n",
      "schema 3\n",
      "2017-03-01\n",
      "schema 3\n",
      "2017-04-01\n",
      "schema 3\n",
      "2017-05-01\n",
      "schema 3\n",
      "2017-06-01\n",
      "schema 3\n",
      "2017-07-01\n",
      "schema 3\n",
      "2017-08-01\n",
      "schema 3\n",
      "2017-09-01\n",
      "schema 3\n",
      "2017-10-01\n",
      "schema 3\n",
      "2017-11-01\n",
      "schema 3\n",
      "2017-12-01\n",
      "schema 3\n",
      "2018-01-01\n",
      "schema 3\n",
      "2018-02-01\n",
      "schema 3\n",
      "2018-03-01\n",
      "schema 3\n",
      "2018-04-01\n",
      "schema 3\n",
      "2018-05-01\n",
      "schema 3\n",
      "2018-06-01\n",
      "schema 3\n",
      "2018-07-01\n",
      "schema 3\n",
      "2018-08-01\n",
      "schema 3\n",
      "2018-09-01\n",
      "schema 3\n",
      "2018-10-01\n",
      "schema 3\n",
      "2018-11-01\n",
      "schema 3\n",
      "2018-12-01\n",
      "schema 3\n",
      "2019-01-01\n",
      "schema 4\n",
      "2019-02-01\n",
      "schema 4\n",
      "2019-03-01\n",
      "schema 4\n",
      "2019-04-01\n",
      "schema 4\n",
      "2019-05-01\n",
      "schema 4\n",
      "2019-06-01\n",
      "schema 4\n",
      "2020-01-01\n",
      "schema 4\n",
      "2020-02-01\n",
      "schema 4\n",
      "2020-04-01\n",
      "schema 4\n",
      "2020-05-01\n",
      "schema 4\n",
      "schema LAST\n",
      "All the 76 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='green'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Creation of a function to convert lat-lon into location ID\n",
    "def convertlocID(lon, lat):\n",
    "    global locationID # access the outer scope variable by declaring it global\n",
    "    if int(lon) != 0 and int(lat) != 0:\n",
    "        query_point = Point( lon, lat)\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "        for i in range(0,len(possible_matches)) :\n",
    "            if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "                locationID = possible_matches[i]\n",
    "    else:\n",
    "        locationID = 9999\n",
    "    \n",
    "    return locationID\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        green_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green1_DF = DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                             .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2\")\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green2_DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    green3_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    green4_DF = green_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                            \"lpep_pickup_datetime\",\n",
    "                                            \"lpep_dropoff_datetime\",\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeID\"),\n",
    "                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"dolocationid\"),\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"ehail_fee\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"payment_type\",\n",
    "                                            \"trip_type\",\n",
    "                                            \"congestion_surcharge\")\n",
    "                    green4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "        if date_file == dating_schema[4].date() :\n",
    "            print(\"schema LAST\")\n",
    "            green4_DF = green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow files\n",
    "\n",
    "\n",
    "From previous analyse we decided to use the following schema as a reference for the YELLOW taxi files:\n",
    "\n",
    "['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "\n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a)Columns transformations:\n",
    "                -'vendor_name' => 'vendorid'\n",
    "                -'Trip_Pickup_DateTime' => 'tpep_pickup_datetime'\n",
    "                -'Trip_Dropoff_DateTime' => 'tpep_dropoff_datetime'\n",
    "                -'Passenger_Count' => 'passenger_count'\n",
    "                -'Trip_Distance' => 'trip_distance'\n",
    "                -'Rate_Code' => 'ratecodeid'\n",
    "                -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                -'Start_Lon','Start_Lat' => 'pulocationid'\n",
    "                -'End_Lon','End_Lat' => 'dolocationid'\n",
    "                -'Payment_Type' => 'payment_type'\n",
    "                -'Fare_Amt' => 'fare_amount'\n",
    "                -'Tip_Amt' => 'tip_amount'\n",
    "                -'Tolls_Amt' => 'tolls_amount'\n",
    "                -'Total_Amt' => 'total_amount'     \n",
    "            b) Column to remove:\n",
    "                -'surcharge'\n",
    "            c) Columns to add:\n",
    "                -'congestion_surcharge'\n",
    "                -'improvement_surcharge'\n",
    "                -'extra'\n",
    "   \n",
    "- Change in schema 2 :\n",
    "            a)Columns transformations:\n",
    "                -'vendor_id' => 'VendorID'\n",
    "                -'pickup_datetime' => 'tpep_pickup_datetime'\n",
    "                -'dropoff_datetime' => 'tpep_dropoff_datetime'\n",
    "                -'Trip_Distance' => 'trip_distance'\n",
    "                -'rate_code' => 'ratecodeID'\n",
    "                -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                -'pickup_longitude','pickup_latitude' => 'pulocationid'\n",
    "                -'dropoff_longitude','dropoff_latitude' => 'dolocationid'   \n",
    "            b) Column to remove:\n",
    "                -'surcharge'\n",
    "            c) Columns to add:\n",
    "                -'congestion_surcharge'\n",
    "                -'improvement_surcharge'\n",
    "                -'extra'\n",
    "                \n",
    "- Change in schema 3 :\n",
    "            a)Columns transformations:\n",
    "                -'RateCodeID' => 'ratecodeid'\n",
    "                -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                -'pickup_longitude','pickup_latitude' => 'puLocationid'\n",
    "                -'dropoff_longitude','dropoff_latitude' => 'DOLocationid                 \n",
    "            b) One new column to add : congestion_surcharge\n",
    "\n",
    "- Change in schema 4:\n",
    "            b) One new column to add : congestion_surcharge\n",
    "            \n",
    "\n",
    "- Final schema 5 :\n",
    "            lowercasing header\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vendor_id,pickup_datetime,dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,rate_code,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,surcharge,mta_tax,tip_amount,tolls_amount,total_amount\n",
    "#yellow1_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#                    .withColumn(\"pulocationid\",\n",
    "#                                    f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                    .withColumn(\"dolocationid\",\n",
    "#                                    f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                    .withColumn(\"extra\",lit('null'))\\\n",
    "#                                    .select(\n",
    "#                                        col(\"vendor_name\").alias(\"vendorid\"),\n",
    "#                                        col(\"Trip_Pickup_DateTime\").alias(\"tpep_pickup_datetime\"),\n",
    "#                                        col(\"Trip_Dropoff_DateTime\").alias(\"tpep_dropoff_datetime\"),\n",
    "#                                        col(\"Passenger_Count\").alias(\"passenger_count\"),\n",
    "#                                        col(\"Trip_Distance\").alias(\"trip_distance\"),\n",
    "#                                        col(\"Rate_Code\").alias(\"ratecodeid\"),\n",
    "#                                        col(\"store_and_forward\").alias(\"store_and_fwd_flag\"),\n",
    "#                                        \"pulocationid\",\n",
    "#                                        \"dolocationid\",\n",
    "#                                        col(\"Payment_Type\").alias(\"payment_type\"),\n",
    "#                                        col(\"Fare_Amt\").alias(\"fare_amount\"),\n",
    "#                                        \"extra\",\n",
    "#                                        mta_tax\",\n",
    "#                                        col(\"Tip_Amt\").alias(\"tip_amount\"),\n",
    "#                                        col(\"Tolls_Amt\").alias(\"tolls_amount\"),\n",
    "#                                        \"improvement_surcharge\",\n",
    "#                                        col(\"Total_Amt\").alias(\"total_amount\"),\n",
    "#                                        \"congestion_surcharge\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#yellow2_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#                    .withColumn(\"pulocationid\",\n",
    "#                                    f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                    .withColumn(\"dolocationid\",\n",
    "#                                    f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                    .withColumn(\"extra\",lit('null'))\\\n",
    "#                                    .select(\n",
    "#                                        col(\"vendor_id\").alias(\"vendorid\")\n",
    "#                                        col(\"pickup_datetime\").alias(\"tpep_pickup_datetime\"),\n",
    "#                                        col(\"dropoff_datetime\").alias(\"tpep_dropoff_datetime\"),\n",
    "#                                        \"passenger_count\",\n",
    "#                                        \"trip_distance\",\n",
    "#                                        col(\"rate_code\").alias(\"ratecodeid\"),\n",
    "#                                        \"store_and_fwd_flag\",\n",
    "#                                        \"pulocationid\",\n",
    "#                                        \"dolocationid\",\n",
    "#                                        \"payment_type\",\n",
    "#                                        \"fare_amount\",\n",
    "#                                        \"extra\",\n",
    "#                                        \"mta_tax\",\n",
    "#                                        \"tip_amount\",\n",
    "#                                        \"tolls_amount\" ,\n",
    "#                                        \"improvement_surcharge\",\n",
    "#                                        \"total_amount\",\n",
    "#                                        \"congestion_surcharge\")   \n",
    "#\n",
    "#\n",
    "#yellow3_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#                    .withColumn(\"pulocationid\",\n",
    "#                                    f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                    .withColumn(\"dolocationid\",\n",
    "#                                    f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "#                                    .select(\n",
    "#                                        col(\"VendorID\").alias(\"vendorid\"),\n",
    "#                                        \"tpep_pickup_datetime\",\n",
    "#                                        \"tpep_dropoff_datetime\",\n",
    "#                                        \"passenger_count\",\n",
    "#                                        \"trip_distance\",\n",
    "#                                        col(\"RateCodeID\").alias(\"ratecodeid\"),\n",
    "#                                        \"store_and_fwd_flag\",\n",
    "#                                        \"pulocationid\",\n",
    "#                                        \"dolocationid\",\n",
    "#                                        \"payment_type\",\n",
    "#                                        \"fare_amount\",\n",
    "#                                        \"extra\",\n",
    "#                                        \"mta_tax\",\n",
    "#                                        \"tip_amount\",\n",
    "#                                        \"tolls_amount\",\n",
    "#                                        \"improvement_surcharge\",\n",
    "#                                        \"total_amount\",\n",
    "#                                        \"congestion_surcharge\")\n",
    "#    \n",
    "#yellow4_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#                                    .select(\n",
    "#                                        col(\"VendorID\").alias(\"vendorid\"),\n",
    "#                                        \"tpep_pickup_datetime\",\n",
    "#                                        \"tpep_dropoff_datetime\",\n",
    "#                                        \"passenger_count\",\n",
    "#                                        \"trip_distance\",\n",
    "#                                        col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "#                                        \"store_and_fwd_flag\",\n",
    "#                                        col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "#                                        col(\"DOLocationID\").alias(\"dulocationid\"),\n",
    "#                                        \"payment_type\",\n",
    "#                                        \"fare_amount\",\n",
    "#                                        \"extra\",\n",
    "#                                        \"mta_tax\",\n",
    "#                                        \"tip_amount\",\n",
    "#                                        \"tolls_amount\",\n",
    "#                                        \"improvement_surcharge\",\n",
    "#                                        \"total_amount\",\n",
    "#                                        \"congestion_surcharge\")    \n",
    "#\n",
    "#yellow5_DF = green_DF.select(\n",
    "#                        col(\"VendorID\").alias(\"vendorid\")\n",
    "#                        \"tpep_pickup_datetime\",\n",
    "#                        \"tpep_dropoff_datetime\",\n",
    "#                        \"passenger_count\",\n",
    "#                        \"trip_distance\",\n",
    "#                        col(\"RatecodeID\").alias(\"ratecodeid\")\n",
    "#                        \"store_and_fwd_flag\",\n",
    "#                        col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "#                        col(\"DOLocationID\").alias(\"dolocationid\"),\n",
    "#                        \"payment_type\",\n",
    "#                        \"fare_amount\",\n",
    "#                        \"extra\",\n",
    "#                        \"mta_tax\",\n",
    "#                        \"tip_amount\",\n",
    "#                        \"tolls_amount\",\n",
    "#                        \"improvement_surcharge\",\n",
    "#                        \"total_amount\",\n",
    "#                        \"congestion_surcharge\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01\n",
      "schema 1 for file: data/sampled/yellow_tripdata_2009-01.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`improvement_surcharge`' given input columns: [Trip_Distance, congestion_surcharge, surcharge, Tolls_Amt, Fare_Amt, Trip_Pickup_DateTime, End_Lat, dolocationid, store_and_forward, Start_Lat, vendor_name, extra, mta_tax, Start_Lon, Total_Amt, Rate_Code, Payment_Type, Tip_Amt, Trip_Dropoff_DateTime, pulocationid, End_Lon, Passenger_Count];;\\n'Project [vendor_name#27598 AS vendorid#27794, Trip_Pickup_DateTime#27599 AS tpep_pickup_datetime#27795, Trip_Dropoff_DateTime#27600 AS tpep_dropoff_datetime#27796, Passenger_Count#27601 AS passenger_count#27797, Trip_Distance#27602 AS trip_distance#27798, Rate_Code#27605 AS ratecodeid#27799, store_and_forward#27606 AS store_and_fwd_flag#27800, pulocationid#27727, dolocationid#27749, Payment_Type#27609 AS payment_type#27801, Fare_Amt#27610 AS fare_amount#27802, extra#27771, mta_tax#27612, Tip_Amt#27613 AS tip_amount#27803, Tolls_Amt#27614 AS tolls_amount#27804, 'improvement_surcharge, Total_Amt#27615 AS total_amount#27805, congestion_surcharge#27706]\\n+- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, dolocationid#27749, null AS extra#27771]\\n   +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, <lambda>(monotonically_increasing_id()) AS dolocationid#27749]\\n      +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, <lambda>(monotonically_increasing_id()) AS pulocationid#27727]\\n         +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, null AS congestion_surcharge#27706]\\n            +- Relation[vendor_name#27598,Trip_Pickup_DateTime#27599,Trip_Dropoff_DateTime#27600,Passenger_Count#27601,Trip_Distance#27602,Start_Lon#27603,Start_Lat#27604,Rate_Code#27605,store_and_forward#27606,End_Lon#27607,End_Lat#27608,Payment_Type#27609,Fare_Amt#27610,surcharge#27611,mta_tax#27612,Tip_Amt#27613,Tolls_Amt#27614,Total_Amt#27615] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o12744.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`improvement_surcharge`' given input columns: [Trip_Distance, congestion_surcharge, surcharge, Tolls_Amt, Fare_Amt, Trip_Pickup_DateTime, End_Lat, dolocationid, store_and_forward, Start_Lat, vendor_name, extra, mta_tax, Start_Lon, Total_Amt, Rate_Code, Payment_Type, Tip_Amt, Trip_Dropoff_DateTime, pulocationid, End_Lon, Passenger_Count];;\n'Project [vendor_name#27598 AS vendorid#27794, Trip_Pickup_DateTime#27599 AS tpep_pickup_datetime#27795, Trip_Dropoff_DateTime#27600 AS tpep_dropoff_datetime#27796, Passenger_Count#27601 AS passenger_count#27797, Trip_Distance#27602 AS trip_distance#27798, Rate_Code#27605 AS ratecodeid#27799, store_and_forward#27606 AS store_and_fwd_flag#27800, pulocationid#27727, dolocationid#27749, Payment_Type#27609 AS payment_type#27801, Fare_Amt#27610 AS fare_amount#27802, extra#27771, mta_tax#27612, Tip_Amt#27613 AS tip_amount#27803, Tolls_Amt#27614 AS tolls_amount#27804, 'improvement_surcharge, Total_Amt#27615 AS total_amount#27805, congestion_surcharge#27706]\n+- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, dolocationid#27749, null AS extra#27771]\n   +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, <lambda>(monotonically_increasing_id()) AS dolocationid#27749]\n      +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, <lambda>(monotonically_increasing_id()) AS pulocationid#27727]\n         +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, null AS congestion_surcharge#27706]\n            +- Relation[vendor_name#27598,Trip_Pickup_DateTime#27599,Trip_Dropoff_DateTime#27600,Passenger_Count#27601,Trip_Distance#27602,Start_Lon#27603,Start_Lat#27604,Rate_Code#27605,store_and_forward#27606,End_Lon#27607,End_Lat#27608,Payment_Type#27609,Fare_Amt#27610,surcharge#27611,mta_tax#27612,Tip_Amt#27613,Tolls_Amt#27614,Total_Amt#27615] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f5f878a69e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                                                \u001b[0;34m\"improvement_surcharge\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                                                \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total_Amt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                                                \"congestion_surcharge\")\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0myellow1_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtaxi_brand\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_brand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mnb_schema\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`improvement_surcharge`' given input columns: [Trip_Distance, congestion_surcharge, surcharge, Tolls_Amt, Fare_Amt, Trip_Pickup_DateTime, End_Lat, dolocationid, store_and_forward, Start_Lat, vendor_name, extra, mta_tax, Start_Lon, Total_Amt, Rate_Code, Payment_Type, Tip_Amt, Trip_Dropoff_DateTime, pulocationid, End_Lon, Passenger_Count];;\\n'Project [vendor_name#27598 AS vendorid#27794, Trip_Pickup_DateTime#27599 AS tpep_pickup_datetime#27795, Trip_Dropoff_DateTime#27600 AS tpep_dropoff_datetime#27796, Passenger_Count#27601 AS passenger_count#27797, Trip_Distance#27602 AS trip_distance#27798, Rate_Code#27605 AS ratecodeid#27799, store_and_forward#27606 AS store_and_fwd_flag#27800, pulocationid#27727, dolocationid#27749, Payment_Type#27609 AS payment_type#27801, Fare_Amt#27610 AS fare_amount#27802, extra#27771, mta_tax#27612, Tip_Amt#27613 AS tip_amount#27803, Tolls_Amt#27614 AS tolls_amount#27804, 'improvement_surcharge, Total_Amt#27615 AS total_amount#27805, congestion_surcharge#27706]\\n+- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, dolocationid#27749, null AS extra#27771]\\n   +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, pulocationid#27727, <lambda>(monotonically_increasing_id()) AS dolocationid#27749]\\n      +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, congestion_surcharge#27706, <lambda>(monotonically_increasing_id()) AS pulocationid#27727]\\n         +- Project [vendor_name#27598, Trip_Pickup_DateTime#27599, Trip_Dropoff_DateTime#27600, Passenger_Count#27601, Trip_Distance#27602, Start_Lon#27603, Start_Lat#27604, Rate_Code#27605, store_and_forward#27606, End_Lon#27607, End_Lat#27608, Payment_Type#27609, Fare_Amt#27610, surcharge#27611, mta_tax#27612, Tip_Amt#27613, Tolls_Amt#27614, Total_Amt#27615, null AS congestion_surcharge#27706]\\n            +- Relation[vendor_name#27598,Trip_Pickup_DateTime#27599,Trip_Dropoff_DateTime#27600,Passenger_Count#27601,Trip_Distance#27602,Start_Lon#27603,Start_Lat#27604,Rate_Code#27605,store_and_forward#27606,End_Lon#27607,End_Lat#27608,Payment_Type#27609,Fare_Amt#27610,surcharge#27611,mta_tax#27612,Tip_Amt#27613,Tolls_Amt#27614,Total_Amt#27615] csv\\n\""
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='yellow'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Creation of a function to convert lat-lon into location ID\n",
    "def convertlocID(lon, lat):\n",
    "    global locationID # access the outer scope variable by declaring it global\n",
    "    if int(lon) != 0 and int(lat) != 0:\n",
    "        query_point = Point( lon, lat)\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "        for i in range(0,len(possible_matches)) :\n",
    "            if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "                locationID = possible_matches[i]\n",
    "    else:\n",
    "        locationID = 9999\n",
    "    \n",
    "    return locationID\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        yellow_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('Start_Lat')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('Start_Lon')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('End_Lat')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('End_Lon')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow1_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                          f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                          f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"extra\",lit('null'))\\\n",
    "                                                          .select(\n",
    "                                                               col(\"vendor_name\").alias(\"vendorid\"),\n",
    "                                                               col(\"Trip_Pickup_DateTime\").alias(\"tpep_pickup_datetime\"),\n",
    "                                                               col(\"Trip_Dropoff_DateTime\").alias(\"tpep_dropoff_datetime\"),\n",
    "                                                               col(\"Passenger_Count\").alias(\"passenger_count\"),\n",
    "                                                               col(\"Trip_Distance\").alias(\"trip_distance\"),\n",
    "                                                               col(\"Rate_Code\").alias(\"ratecodeid\"),\n",
    "                                                               col(\"store_and_forward\").alias(\"store_and_fwd_flag\"),\n",
    "                                                               \"pulocationid\",\n",
    "                                                               \"dolocationid\",\n",
    "                                                               col(\"Payment_Type\").alias(\"payment_type\"),\n",
    "                                                               col(\"Fare_Amt\").alias(\"fare_amount\"),\n",
    "                                                               \"extra\",\n",
    "                                                               \"mta_tax\",\n",
    "                                                               col(\"Tip_Amt\").alias(\"tip_amount\"),\n",
    "                                                               col(\"Tolls_Amt\").alias(\"tolls_amount\"),\n",
    "                                                               \"improvement_surcharge\",\n",
    "                                                               col(\"Total_Amt\").alias(\"total_amount\"),\n",
    "                                                               \"congestion_surcharge\")\n",
    "                    yellow1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2\")\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow2_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                         .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                         .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                         .withColumn(\"extra\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"vendor_id\").alias(\"vendorid\"),\n",
    "                                                            col(\"pickup_datetime\").alias(\"tpep_pickup_datetime\"),\n",
    "                                                            col(\"dropoff_datetime\").alias(\"tpep_dropoff_datetime\"),\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"rate_code\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\" ,\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\") \n",
    "                    yellow2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow3_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                         .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                         .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            \"tpep_pickup_datetime\",\n",
    "                                                            \"tpep_dropoff_datetime\",\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    yellow3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    # Create the new file\n",
    "                    yellow4_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            \"tpep_pickup_datetime\",\n",
    "                                                            \"tpep_dropoff_datetime\",\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                                            col(\"DOLocationID\").alias(\"dulocationid\"),\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")    \n",
    "\n",
    "                    yellow4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    print(\"schema LAST\")\n",
    "                    # Create the new file\n",
    "                    yellow5_DF = yellow_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                            \"tpep_pickup_datetime\",\n",
    "                                            \"tpep_dropoff_datetime\",\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"PULocationID\").alias(\"pulocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"dolocationid\"),\n",
    "                                            \"payment_type\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"congestion_surcharge\")\n",
    "                    yellow5_DF = green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
