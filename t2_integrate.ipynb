{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040\n",
    "\n",
    "## FUNCTION DECLARATION\n",
    "\n",
    "# Creation of a function to convert lat-lon into location ID\n",
    "def convertlocID(lon, lat):\n",
    "    global locationID # access the outer scope variable by declaring it global\n",
    "    if lon != None and lat != None and lon < -70.0 and lon > -80.0 and lat > 35.0 and lat < 45.0:\n",
    "        query_point = Point( lon, lat)\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "        for i in range(0,len(possible_matches)) :\n",
    "            if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "                locationID = possible_matches[i]\n",
    "    else:\n",
    "        locationID = None\n",
    "    \n",
    "    return locationID\n",
    "\n",
    "# Check if the value is null or not\n",
    "def blank_as_null(x):\n",
    "\n",
    "    return f.when(col(x).isNull(), 0 ).otherwise(col(x))\n",
    "\n",
    "def create_files_list(brand,list_files):\n",
    "    global nb_files\n",
    "    nb_files = 0\n",
    "    for file in glob.glob(\"data/sampled/%s_*.csv\" %(brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files.append(file)\n",
    "        # Order by date the file list\n",
    "        list_files.sort()\n",
    "\n",
    "    return list_files, nb_files\n",
    "\n",
    "def remove_all_whitespace(col):\n",
    "    return f.regexp_replace(col, \"\\\\s+\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory data/cleaned already exist\n",
      "The directory data/cleaned/yellow already exist\n",
      "The directory data/cleaned/green already exist\n",
      "The directory data/cleaned/fhv already exist\n",
      "The directory data/cleaned/fhvhv already exist\n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "isdir = os.path.isdir(\"data/cleaned\")  \n",
    "if isdir == False :\n",
    "    print (\"Need to create directory data/cleaned\")\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "else:\n",
    "    print (\"The directory data/cleaned already exist\")\n",
    "    \n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    isdir = os.path.isdir(path)\n",
    "    if isdir == False :\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path) \n",
    "    else:\n",
    "        print (\"The directory %s already exist\" % path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We then donc need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= 'data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleaned/fhvhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 (from 2015-1 to 2016-12): \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 (from 2017-1 to 2017-6): \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 (from 2017-7 to 2017-12): \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 (from 2018-1 to 2018-12):\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Final schema 5 (from 2019-1 to 2020-6):\n",
    "            NO change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        fhv_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                if nb_schema+1 == 1 :\n",
    "                    fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                           .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                           .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                           .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                            \"dropoff_datetime\",\n",
    "                            col(\"locationID\").alias(\"PUlocationid\"),\n",
    "                            col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                            \"SR_Flag\")\n",
    "                    fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                            .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                            .select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                col(\"locationID\").alias(\"PUlocationid\"),\n",
    "                                col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                \"SR_Flag\")\n",
    "                    fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    fhv3_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                col(\"locationID\").alias(\"PUlocationid\"),\n",
    "                                col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                \"SR_Flag\")\n",
    "                    fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    fhv4_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                col(\"locationID\").alias(\"PUlocationid\"),\n",
    "                                col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                \"SR_Flag\")\n",
    "                    fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "From previous analyse we decide to use as reference for the GREEN taxi files the following schema:\n",
    "\n",
    "['vendorid', 'pickup_datetime', 'dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 (from 2013-8 to 2014-12): \n",
    "            a) Two new columns are add : congestion_surcharge and improvement_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "           \n",
    "- Change in schema 2 (from 2015-1 to 2016-7):\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Change in schema 3 (from 2016-7 to 2018-12):\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Final schema 4 (from 2019-1 to 2020-6):\n",
    "            NO change\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-08-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-08.csv\n",
      "2013-09-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-09.csv\n",
      "2013-10-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-10.csv\n",
      "2013-11-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-11.csv\n",
      "2013-12-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2013-12.csv\n",
      "2014-01-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-01.csv\n",
      "2014-02-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-02.csv\n",
      "2014-03-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-03.csv\n",
      "2014-04-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-04.csv\n",
      "2014-05-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-05.csv\n",
      "2014-06-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-06.csv\n",
      "2014-07-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-07.csv\n",
      "2014-08-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-08.csv\n",
      "2014-09-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-09.csv\n",
      "2014-10-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-10.csv\n",
      "2014-11-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-11.csv\n",
      "2014-12-01\n",
      "schema 1 for file: data/sampled/green_tripdata_2014-12.csv\n",
      "2015-01-01\n",
      "schema 2\n",
      "2015-02-01\n",
      "schema 2\n",
      "2015-03-01\n",
      "schema 2\n",
      "2015-04-01\n",
      "schema 2\n",
      "2015-05-01\n",
      "schema 2\n",
      "2015-06-01\n",
      "schema 2\n",
      "2015-07-01\n",
      "schema 2\n",
      "2015-08-01\n",
      "schema 2\n",
      "2015-09-01\n",
      "schema 2\n",
      "2015-10-01\n",
      "schema 2\n",
      "2015-11-01\n",
      "schema 2\n",
      "2015-12-01\n",
      "schema 2\n",
      "2016-01-01\n",
      "schema 2\n",
      "2016-02-01\n",
      "schema 2\n",
      "2016-03-01\n",
      "schema 2\n",
      "2016-04-01\n",
      "schema 2\n",
      "2016-05-01\n",
      "schema 2\n",
      "2016-06-01\n",
      "schema 2\n",
      "2016-07-01\n",
      "schema 3\n",
      "2016-08-01\n",
      "schema 3\n",
      "2016-09-01\n",
      "schema 3\n",
      "2016-10-01\n",
      "schema 3\n",
      "2016-11-01\n",
      "schema 3\n",
      "2016-12-01\n",
      "schema 3\n",
      "2017-01-01\n",
      "schema 3\n",
      "2017-02-01\n",
      "schema 3\n",
      "2017-03-01\n",
      "schema 3\n",
      "2017-04-01\n",
      "schema 3\n",
      "2017-05-01\n",
      "schema 3\n",
      "2017-06-01\n",
      "schema 3\n",
      "2017-07-01\n",
      "schema 3\n",
      "2017-08-01\n",
      "schema 3\n",
      "2017-09-01\n",
      "schema 3\n",
      "2017-10-01\n",
      "schema 3\n",
      "2017-11-01\n",
      "schema 3\n",
      "2017-12-01\n",
      "schema 3\n",
      "2018-01-01\n",
      "schema 3\n",
      "2018-02-01\n",
      "schema 3\n",
      "2018-03-01\n",
      "schema 3\n",
      "2018-04-01\n",
      "schema 3\n",
      "2018-05-01\n",
      "schema 3\n",
      "2018-06-01\n",
      "schema 3\n",
      "2018-07-01\n",
      "schema 3\n",
      "2018-08-01\n",
      "schema 3\n",
      "2018-09-01\n",
      "schema 3\n",
      "2018-10-01\n",
      "schema 3\n",
      "2018-11-01\n",
      "schema 3\n",
      "2018-12-01\n",
      "schema 3\n",
      "2019-01-01\n",
      "schema 4\n",
      "2019-02-01\n",
      "schema 4\n",
      "2019-03-01\n",
      "schema 4\n",
      "2019-04-01\n",
      "schema 4\n",
      "2019-05-01\n",
      "schema 4\n",
      "2019-06-01\n",
      "schema 4\n",
      "2020-01-01\n",
      "schema 4\n",
      "2020-02-01\n",
      "schema 4\n",
      "2020-04-01\n",
      "schema 4\n",
      "2020-05-01\n",
      "schema 4\n",
      "schema LAST year\n",
      "All the 76 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='green'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        green_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        green_DF = green_DF.select([f.col(col).alias(col.replace(' ', '')) for col in green_DF.columns])\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    green_DF = green_DF.withColumn(\"Dropoff_longitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Dropoff_latitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Pickup_latitude\", blank_as_null(\"Pickup_latitude\"))\\\n",
    "                           .withColumn(\"Pickup_longitude\", blank_as_null(\"Pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green1_DF = DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                             .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2\")\n",
    "                    green_DF = green_DF.withColumn(\"Dropoff_longitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Dropoff_latitude\", blank_as_null(\"Dropoff_longitude\"))\\\n",
    "                           .withColumn(\"Pickup_latitude\", blank_as_null(\"Pickup_latitude\"))\\\n",
    "                           .withColumn(\"Pickup_longitude\", blank_as_null(\"Pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green2_DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"lpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    green3_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    green4_DF = green_DF.select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "        if date_file == dating_schema[4].date() :\n",
    "            print(\"schema LAST year\")\n",
    "            green4_DF = green_DF.select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "            green4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Yellow files\n",
    "\n",
    "Yellow files\n",
    "\n",
    "From previous analyse we decided to use the following schema as a reference for the YELLOW taxi files:\n",
    "\n",
    "['vendorid','tpep_pickup_datetime','tpep_dropoff_datetime','passenger_count','trip_distance','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge','total_amount','congestion_surcharge']\n",
    "\n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 (from 2009-1 to 2009-12) :\n",
    "            a)Columns transformations:\n",
    "                  -'vendor_name' => 'vendorid'\n",
    "                  -'Trip_Pickup_DateTime' => 'pickup_datetime'\n",
    "                  -'Trip_Dropoff_DateTime' => 'dropoff_datetime'\n",
    "                  -'Passenger_Count' => 'passenger_count'\n",
    "                  -'Trip_Distance' => 'trip_distance'\n",
    "                  -'Rate_Code' => 'ratecodeid'\n",
    "                  -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                  -'Start_Lon','Start_Lat' => 'pulocationid'\n",
    "                  -'End_Lon','End_Lat' => 'dolocationid'\n",
    "                  -'Payment_Type' => 'payment_type'\n",
    "                  -'Fare_Amt' => 'fare_amount'\n",
    "                  -'surcharge' => 'extra'\n",
    "                  -'Tip_Amt' => 'tip_amount'\n",
    "                  -'Tolls_Amt' => 'tolls_amount'\n",
    "                  -'Total_Amt' => 'total_amount'     \n",
    "          b) Columns to add:\n",
    "                  -'congestion_surcharge'\n",
    "                  -'improvement_surcharge'\n",
    "\n",
    "- Change schema 2 (from 2010-1 to 2014-12):\n",
    "          a)Columns transformations:\n",
    "                  -'vendor_id' => 'VendorID'\n",
    "                  -'Trip_Distance' => 'trip_distance'\n",
    "                  -'rate_code' => 'ratecodeID'\n",
    "                  -'store_and_forward' => 'store_and_fwd_flag'\n",
    "                  -'pickup_longitude','pickup_latitude' => 'pulocationid'\n",
    "                  -'dropoff_longitude','dropoff_latitude' => 'dolocationid'   \n",
    "                  -'surcharge' => 'extra'\n",
    "          b) Columns to add:\n",
    "                  -'congestion_surcharge'\n",
    "                  -'improvement_surcharge'\n",
    "\n",
    "- Change in schema 3 (from 2015-1 to 2016-7):\n",
    "          a)Columns transformations:\n",
    "              -'Trip_Pickup_DateTime' => 'pickup_datetime'\n",
    "              -'Trip_Dropoff_DateTime' => 'dropoff_datetime'\n",
    "              -'RateCodeID' => 'ratecodeid'\n",
    "              -'store_and_forward' => 'store_and_fwd_flag'\n",
    "              -'pickup_longitude','pickup_latitude' => 'puLocationid'\n",
    "              -'dropoff_longitude','dropoff_latitude' => 'DOLocationid                 \n",
    "          b) One new column to add : congestion_surcharge\n",
    "\n",
    "- Change in schema 4 (from 2016-7 to 2018-12):          \n",
    "          a)Columns transformations:\n",
    "              -'Trip_Pickup_DateTime' => 'pickup_datetime'\n",
    "              -'Trip_Dropoff_DateTime' => 'dropoff_datetime'\n",
    "          b) One new column to add : congestion_surcharge\n",
    "\n",
    "- Final schema 5 (from 2019-1 to 2020-6):          \n",
    "          a)Columns transformations:\n",
    "              -'Trip_Pickup_DateTime' => 'pickup_datetime'\n",
    "              -'Trip_Dropoff_DateTime' => 'dropoff_datetime'\n",
    "          b) Lowercasing header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01\n",
      "2009-02-01\n",
      "2009-03-01\n",
      "2009-04-01\n",
      "2009-05-01\n",
      "2009-06-01\n",
      "2009-07-01\n",
      "2009-08-01\n",
      "2009-09-01\n",
      "2009-10-01\n",
      "2009-11-01\n",
      "2009-12-01\n",
      "2010-01-01\n",
      "29739 29739\n",
      "2010-02-01\n",
      "22291 22291\n",
      "2010-03-01\n",
      "25768 25768\n",
      "2010-04-01\n",
      "30298 30298\n",
      "2010-05-01\n",
      "30969 30969\n",
      "2010-06-01\n",
      "29658 29658\n",
      "2010-07-01\n",
      "29321 29321\n",
      "2010-08-01\n",
      "25056 25056\n",
      "2010-09-01\n",
      "31085 31085\n",
      "2010-10-01\n",
      "28401 28401\n",
      "2010-11-01\n",
      "27826 27826\n",
      "2010-12-01\n",
      "27638 27638\n",
      "2011-01-01\n",
      "26932 26932\n",
      "2011-02-01\n",
      "28411 28411\n",
      "2011-03-01\n",
      "32150 32150\n",
      "2011-04-01\n",
      "29443 29443\n",
      "2011-05-01\n",
      "31115 31115\n",
      "2011-06-01\n",
      "30201 30201\n",
      "2011-07-01\n",
      "29490 29490\n",
      "2011-08-01\n",
      "26529 26529\n",
      "2011-09-01\n",
      "29261 29261\n",
      "2011-10-01\n",
      "31423 31423\n",
      "2011-11-01\n",
      "29062 29062\n",
      "2011-12-01\n",
      "29861 29861\n",
      "2012-01-01\n",
      "29943 29943\n",
      "2012-02-01\n",
      "29971 29971\n",
      "2012-03-01\n",
      "32300 32300\n",
      "2012-04-01\n",
      "30959 30959\n",
      "2012-05-01\n",
      "31142 31142\n",
      "2012-06-01\n",
      "30197 30197\n",
      "2012-07-01\n",
      "28767 28767\n",
      "2012-08-01\n",
      "28771 28771\n",
      "2012-09-01\n",
      "29098 29098\n",
      "2012-10-01\n",
      "29056 29056\n",
      "2012-11-01\n",
      "27554 27554\n",
      "2012-12-01\n",
      "29401 29401\n",
      "2013-01-01\n",
      "29556 29556\n",
      "2013-02-01\n",
      "27988 27988\n",
      "2013-03-01\n",
      "31501 31501\n",
      "2013-04-01\n",
      "30202 30202\n",
      "2013-05-01\n",
      "30575 30575\n",
      "2013-06-01\n",
      "28767 28767\n",
      "2013-07-01\n",
      "27642 27642\n",
      "2013-08-01\n",
      "25190 25190\n",
      "2013-09-01\n",
      "28221 28221\n",
      "2013-10-01\n",
      "30008 30008\n",
      "2013-11-01\n",
      "28786 28786\n",
      "2013-12-01\n",
      "27941 27941\n",
      "2014-01-01\n",
      "27559 27559\n",
      "2014-02-01\n",
      "26134 26134\n",
      "2014-03-01\n",
      "30862 30862\n",
      "2014-04-01\n",
      "29250 29250\n",
      "2014-05-01\n",
      "29554 29554\n",
      "2014-06-01\n",
      "27623 27623\n",
      "2014-07-01\n",
      "26224 26224\n",
      "2014-08-01\n",
      "25378 25378\n",
      "2014-09-01\n",
      "26748 26748\n",
      "2014-10-01\n",
      "28465 28465\n",
      "2014-11-01\n",
      "26440 26440\n",
      "2014-12-01\n",
      "26029 26029\n",
      "2015-01-01\n",
      "schema 3\n",
      "2015-02-01\n",
      "schema 3\n",
      "2015-03-01\n",
      "schema 3\n",
      "2015-04-01\n",
      "schema 3\n",
      "2015-05-01\n",
      "schema 3\n",
      "2015-06-01\n",
      "schema 3\n",
      "2015-07-01\n",
      "schema 3\n",
      "2015-08-01\n",
      "schema 3\n",
      "2015-09-01\n",
      "schema 3\n",
      "2015-10-01\n",
      "schema 3\n",
      "2015-11-01\n",
      "schema 3\n",
      "2015-12-01\n",
      "schema 3\n",
      "2016-01-01\n",
      "schema 3\n",
      "2016-02-01\n",
      "schema 3\n",
      "2016-03-01\n",
      "schema 3\n",
      "2016-04-01\n",
      "schema 3\n",
      "2016-05-01\n",
      "schema 3\n",
      "2016-06-01\n",
      "schema 3\n",
      "2016-07-01\n",
      "schema 4\n",
      "2016-08-01\n",
      "schema 4\n",
      "2016-09-01\n",
      "schema 4\n",
      "2016-10-01\n",
      "schema 4\n",
      "2016-11-01\n",
      "schema 4\n",
      "2016-12-01\n",
      "schema 4\n",
      "2017-01-01\n",
      "schema 4\n",
      "2017-02-01\n",
      "schema 4\n",
      "2017-03-01\n",
      "schema 4\n",
      "2017-04-01\n",
      "schema 4\n",
      "2017-05-01\n",
      "schema 4\n",
      "2017-06-01\n",
      "schema 4\n",
      "2017-07-01\n",
      "schema 4\n",
      "2017-08-01\n",
      "schema 4\n",
      "2017-09-01\n",
      "schema 4\n",
      "2017-10-01\n",
      "schema 4\n",
      "2017-11-01\n",
      "schema 4\n",
      "2017-12-01\n",
      "schema 4\n",
      "2018-01-01\n",
      "schema 4\n",
      "2018-02-01\n",
      "schema 4\n",
      "2018-03-01\n",
      "schema 4\n",
      "2018-04-01\n",
      "schema 4\n",
      "2018-05-01\n",
      "schema 4\n",
      "2018-06-01\n",
      "schema 4\n",
      "2018-07-01\n",
      "schema 4\n",
      "2018-08-01\n",
      "schema 4\n",
      "2018-09-01\n",
      "schema 4\n",
      "2018-10-01\n",
      "schema 4\n",
      "2018-11-01\n",
      "schema 4\n",
      "2018-12-01\n",
      "schema 4\n",
      "2019-01-01\n",
      "schema LAST\n",
      "2019-02-01\n",
      "schema LAST\n",
      "2019-03-01\n",
      "schema LAST\n",
      "2019-04-01\n",
      "schema LAST\n",
      "2019-05-01\n",
      "schema LAST\n",
      "2019-06-01\n",
      "schema LAST\n",
      "2020-01-01\n",
      "schema LAST\n",
      "2020-02-01\n",
      "schema LAST\n",
      "2020-04-01\n",
      "schema LAST\n",
      "2020-05-01\n",
      "schema LAST\n",
      "All the 131 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='yellow'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        yellow_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        yellow_DF = yellow_DF.select([f.col(col).alias(col.replace(' ', '')) for col in yellow_DF.columns])\n",
    "        #test = yellow_DF.withColumn(\"dropoff_longitude\", f.when(col(\"dropoff_longitude\").isNull, 0).otherwise(col(\"dropoff_longitude\")))\n",
    "        #yellow_DF.printSchema()\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    # print(\"schema 1 for file:\",list_files[yr])\n",
    "                    yellow_DF = yellow_DF.withColumn(\"End_Lon\", blank_as_null(\"End_Lon\"))\\\n",
    "                             .withColumn(\"End_Lat\", blank_as_null(\"End_Lat\"))\\\n",
    "                             .withColumn(\"Start_Lat\", blank_as_null(\"Start_Lat\"))\\\n",
    "                             .withColumn(\"Start_Lon\", blank_as_null(\"Start_Lon\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('Start_Lat')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('Start_Lon')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('End_Lat')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('End_Lon')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow1_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                          f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                          f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                          .select(\n",
    "                                                               col(\"vendor_name\").alias(\"vendorid\"),\n",
    "                                                               col(\"Trip_Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                                               col(\"Trip_Dropoff_DateTime\").alias(\"dropoff_datetime\"),\n",
    "                                                               col(\"Passenger_Count\").alias(\"passenger_count\"),\n",
    "                                                               col(\"Trip_Distance\").alias(\"trip_distance\"),\n",
    "                                                               col(\"Rate_Code\").alias(\"ratecodeid\"),\n",
    "                                                               col(\"store_and_forward\").alias(\"store_and_fwd_flag\"),\n",
    "                                                               col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                               col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                               col(\"Payment_Type\").alias(\"payment_type\"),\n",
    "                                                               col(\"Fare_Amt\").alias(\"fare_amount\"),\n",
    "                                                               col(\"surcharge\").alias(\"extra\"),\n",
    "                                                               \"mta_tax\",\n",
    "                                                               col(\"Tip_Amt\").alias(\"tip_amount\"),\n",
    "                                                               col(\"Tolls_Amt\").alias(\"tolls_amount\"),\n",
    "                                                               \"improvement_surcharge\",\n",
    "                                                               col(\"Total_Amt\").alias(\"total_amount\"),\n",
    "                                                               \"congestion_surcharge\")\n",
    "                    yellow1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    # print(\"schema 2 for file:\",list_files[yr])\n",
    "                    yellow_DF = yellow_DF.withColumn(\"dropoff_longitude\", blank_as_null(\"dropoff_longitude\"))\\\n",
    "                             .withColumn(\"dropoff_latitude\", blank_as_null(\"dropoff_latitude\"))\\\n",
    "                             .withColumn(\"pickup_latitude\", blank_as_null(\"pickup_latitude\"))\\\n",
    "                             .withColumn(\"pickup_longitude\", blank_as_null(\"pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    print(len(Dropoff_list_lon),len(Dropoff_list_lat))\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow2_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"vendor_id\").alias(\"vendorid\"),\n",
    "                                                            \"pickup_datetime\",\n",
    "                                                            \"dropoff_datetime\",\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"rate_code\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            col(\"surcharge\").alias(\"extra\"),\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\" ,\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\") \n",
    "                    yellow2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    yellow_DF = yellow_DF.withColumn(\"dropoff_longitude\", blank_as_null(\"dropoff_longitude\"))\\\n",
    "                             .withColumn(\"dropoff_latitude\", blank_as_null(\"dropoff_latitude\"))\\\n",
    "                             .withColumn(\"pickup_latitude\", blank_as_null(\"pickup_latitude\"))\\\n",
    "                             .withColumn(\"pickup_longitude\", blank_as_null(\"pickup_longitude\"))\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = yellow_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = yellow_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = yellow_DF.select(f.collect_list('dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = yellow_DF.select(f.collect_list('dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    yellow3_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                          .withColumn(\"pulocationid\",\n",
    "                                                        f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                          .withColumn(\"dolocationid\",\n",
    "                                                        f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            col(\"tpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"tpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            col(\"pulocationid\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"dolocationid\").alias(\"DOlocationid\"),\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    yellow3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    # Create the new file\n",
    "                    yellow4_DF = yellow_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                                            col(\"tpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                                            col(\"tpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                                            \"passenger_count\",\n",
    "                                                            \"trip_distance\",\n",
    "                                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                                            \"store_and_fwd_flag\",\n",
    "                                                            col(\"PULocationID\").alias(\"PUlocationid\"),\n",
    "                                                            col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                                            \"payment_type\",\n",
    "                                                            \"fare_amount\",\n",
    "                                                            \"extra\",\n",
    "                                                            \"mta_tax\",\n",
    "                                                            \"tip_amount\",\n",
    "                                                            \"tolls_amount\",\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            \"total_amount\",\n",
    "                                                            \"congestion_surcharge\")    \n",
    "\n",
    "                    yellow4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    print(\"schema LAST\")\n",
    "                    # Create the new file\n",
    "                    yellow5_DF = yellow_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                            col(\"tpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                            col(\"tpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"PULocationID\").alias(\"PUlocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                            \"payment_type\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"congestion_surcharge\")\n",
    "                    yellow5_DF = yellow_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            # print(\"schema LAST year\")\n",
    "            yellow5_DF = yellow_DF.select(\n",
    "                                            col(\"VendorID\").alias(\"vendorid\"),\n",
    "                                            col(\"tpep_pickup_datetime\").alias(\"pickup_datetime\"),\n",
    "                                            col(\"tpep_dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                            \"passenger_count\",\n",
    "                                            \"trip_distance\",\n",
    "                                            col(\"RatecodeID\").alias(\"ratecodeid\"),\n",
    "                                            \"store_and_fwd_flag\",\n",
    "                                            col(\"PULocationID\").alias(\"PUlocationid\"),\n",
    "                                            col(\"DOLocationID\").alias(\"DOlocationid\"),\n",
    "                                            \"payment_type\",\n",
    "                                            \"fare_amount\",\n",
    "                                            \"extra\",\n",
    "                                            \"mta_tax\",\n",
    "                                            \"tip_amount\",\n",
    "                                            \"tolls_amount\",\n",
    "                                            \"improvement_surcharge\",\n",
    "                                            \"total_amount\",\n",
    "                                            \"congestion_surcharge\")\n",
    "            yellow5_DF = yellow_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(source_dir)::], index = False)\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
