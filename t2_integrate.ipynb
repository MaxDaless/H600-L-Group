{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from shutil import copyfile\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory data/cleaned\n",
      "Successfully created the directory data/cleaned/yellow \n",
      "Successfully created the directory data/cleaned/green \n",
      "Successfully created the directory data/cleaned/fhv \n",
      "Successfully created the directory data/cleaned/fhvhv \n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "try :  \n",
    "    os.path.isdir(\"data/cleaned\")\n",
    "except OSError:\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "    print (\"Creation of the directory data/cleaned failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/cleaned\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.path.isdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We can then create a unified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Documents/Master_BDGA\n",
      "list of fhvhv tripdata files:\n",
      "fhvhv_tripdata_2019-02.csv  fhvhv_tripdata_2020-01.csv\n",
      "fhvhv_tripdata_2019-03.csv  fhvhv_tripdata_2020-03.csv\n",
      "fhvhv_tripdata_2019-04.csv  fhvhv_tripdata_2020-04.csv\n",
      "fhvhv_tripdata_2019-05.csv  fhvhv_tripdata_2020-05.csv\n",
      "fhvhv_tripdata_2019-06.csv  fhvhv_tripdata_2020-06.csv\n",
      "count of fhvhv tripdata files:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#export new csv file\n",
    "#fhvhv_DF.write.save(path='data/modified/fhvhv.csv', format='csv', mode='append', sep='\\t')\n",
    "#Ã§a fait un dossier ac plusieurs fichiers pourris\n",
    "\n",
    "#move files to t2 directory\n",
    "\n",
    "source_dir= '/data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleanned/fhvhv')\n",
    "\n",
    "print ('list of fhvhv tripdata files:')\n",
    "!ls data/cleanned/fhvhv\n",
    "\n",
    "print ('count of fhvhv tripdata files:')\n",
    "!find data/cleanned/fhvhv -type f | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 : \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 : \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 :\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Final schema 5 :\n",
    "            NO change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01\n",
      "schema 1\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-01-01\n",
      "2015-02-01\n",
      "schema 1\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-02-01\n",
      "2015-03-01\n",
      "schema 1\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-03-01\n",
      "2015-04-01\n",
      "schema 1\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-04-01\n",
      "2015-05-01\n",
      "schema 1\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-05-01\n",
      "2015-06-01\n",
      "schema 1\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-06-01\n",
      "2015-07-01\n",
      "schema 1\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-07-01\n",
      "2015-08-01\n",
      "schema 1\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-08-01\n",
      "2015-09-01\n",
      "schema 1\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-09-01\n",
      "2015-10-01\n",
      "schema 1\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-10-01\n",
      "2015-11-01\n",
      "schema 1\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-11-01\n",
      "2015-12-01\n",
      "schema 1\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2015-12-01\n",
      "2016-01-01\n",
      "schema 1\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-01-01\n",
      "2016-02-01\n",
      "schema 1\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-02-01\n",
      "2016-03-01\n",
      "schema 1\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-03-01\n",
      "2016-04-01\n",
      "schema 1\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-04-01\n",
      "2016-05-01\n",
      "schema 1\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-05-01\n",
      "2016-06-01\n",
      "schema 1\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-06-01\n",
      "2016-07-01\n",
      "schema 1\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-07-01\n",
      "2016-08-01\n",
      "schema 1\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-08-01\n",
      "2016-09-01\n",
      "schema 1\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-09-01\n",
      "2016-10-01\n",
      "schema 1\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-10-01\n",
      "2016-11-01\n",
      "schema 1\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-11-01\n",
      "2016-12-01\n",
      "schema 1\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2016-12-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "schema 2\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-01-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "schema 2\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-02-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "schema 2\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-03-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "schema 2\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-04-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "schema 2\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-05-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "schema 2\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-06-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "schema 3\n",
      "2017-07-01\n",
      "2017-07-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "schema 3\n",
      "2017-08-01\n",
      "2017-08-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "schema 3\n",
      "2017-09-01\n",
      "2017-09-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "schema 3\n",
      "2017-10-01\n",
      "2017-10-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "schema 3\n",
      "2017-11-01\n",
      "2017-11-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "schema 3\n",
      "2017-12-01\n",
      "2017-12-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "2018-01-01\n",
      "schema 4\n",
      "2018-01-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "2018-02-01\n",
      "schema 4\n",
      "2018-02-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "2018-03-01\n",
      "schema 4\n",
      "2018-03-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "2018-04-01\n",
      "schema 4\n",
      "2018-04-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "2018-05-01\n",
      "schema 4\n",
      "2018-05-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "2018-06-01\n",
      "schema 4\n",
      "2018-06-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "2018-07-01\n",
      "schema 4\n",
      "2018-07-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "2018-08-01\n",
      "schema 4\n",
      "2018-08-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "2018-09-01\n",
      "schema 4\n",
      "2018-09-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "2018-10-01\n",
      "schema 4\n",
      "2018-10-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "2018-11-01\n",
      "schema 4\n",
      "2018-11-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "2018-12-01\n",
      "schema 4\n",
      "2018-12-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "2019-01-01\n",
      "schema 5\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "2019-02-01\n",
      "schema 5\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "2019-03-01\n",
      "schema 5\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "2019-04-01\n",
      "schema 5\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "2019-05-01\n",
      "schema 5\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "2019-06-01\n",
      "schema 5\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "2019-07-01\n",
      "schema 5\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "2019-08-01\n",
      "schema 5\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "2019-09-01\n",
      "schema 5\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "2019-10-01\n",
      "schema 5\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "2019-11-01\n",
      "schema 5\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "schema 5\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "2020-03-01\n",
      "schema 5\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "2020-04-01\n",
      "schema 5\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "2020-05-01\n",
      "schema 5\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "2020-06-01\n",
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "    month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "    date_file = date(year,month,1)\n",
    "    fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files[yr]) )\n",
    "    for nb_schema in range(0,len(dating_schema)-1):\n",
    "        print(date_file)\n",
    "        if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "            if nb_schema+1 == 1 :\n",
    "                fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                       .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                       .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                       .select(\n",
    "                        col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                        col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                        \"dropoff_datetime\",\n",
    "                        col(\"locationID\").alias(\"PULocationID\"),\n",
    "                        \"DOLocationID\",\n",
    "                        \"SR_Flag\")\n",
    "                fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 2 :\n",
    "                fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                        .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 3 :\n",
    "                fhv3_DF = fhv_DF.select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 4 :\n",
    "                fhv4_DF = fhv_DF.select(\n",
    "                            col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                            col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                            \"PULocationID\",\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 5 :\n",
    "                fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "    if date_file == dating_schema[5].date() :\n",
    "        fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/fhv'))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "########## \n",
    "#### TO KEEP FOR TEST\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "fhv1_DF, fhv2_DF, fhv3_DF, fhv4_DF, fhv5_DF = 0,0,0,0,0\n",
    "for yr in range(0,nb_files):\n",
    "    year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "    month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "    date_file = date(year,month,1)\n",
    "    for nb_schema in range(0,len(dating_schema)-1):\n",
    "        if date_file >= dating_schema[nb_schema].date() and date_file < dating_schema[nb_schema+1].date():\n",
    "            if nb_schema+1 == 1 :\n",
    "                fhv1_DF = fhv1_DF + 1\n",
    "            elif nb_schema+1 == 2 :\n",
    "                fhv2_DF = fhv2_DF + 1\n",
    "            elif nb_schema+1 == 3 :\n",
    "                fhv3_DF = fhv3_DF + 1\n",
    "            elif nb_schema+1 == 4 :\n",
    "                fhv4_DF = fhv4_DF + 1\n",
    "            elif nb_schema+1 == 5 :\n",
    "                fhv5_DF = fhv5_DF + 1\n",
    "    if date_file == dating_schema[5].date() :\n",
    "        fhv5_DF = fhv5_DF + 1\n",
    "#print(list_files[0:fhv1_DF])\n",
    "#print(list_files[fhv1_DF:fhv1_DF+fhv2_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF:fhv1_DF+fhv2_DF+fhv3_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF])\n",
    "#print(list_files[fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF:fhv1_DF+fhv2_DF+fhv3_DF+fhv4_DF+fhv5_DF])\n",
    "########## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "From previous analyse we decide to use as reference for the GREEN taxi files the following schema:\n",
    "\n",
    "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Two new columns are add : congestion_surcharge and improvement_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "           \n",
    "- Change in schema 2 :\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Change in schema 3 :\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Final schema 4 :\n",
    "            NO change\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of yellow tripdata files:\n",
      "data/sampled/green_tripdata_2013-08.csv\r\n",
      "data/sampled/green_tripdata_2013-09.csv\r\n",
      "data/sampled/green_tripdata_2013-10.csv\r\n",
      "data/sampled/green_tripdata_2013-11.csv\r\n",
      "data/sampled/green_tripdata_2013-12.csv\r\n",
      "data/sampled/green_tripdata_2014-01.csv\r\n",
      "data/sampled/green_tripdata_2014-02.csv\r\n",
      "data/sampled/green_tripdata_2014-03.csv\r\n",
      "data/sampled/green_tripdata_2014-04.csv\r\n",
      "data/sampled/green_tripdata_2014-05.csv\r\n",
      "data/sampled/green_tripdata_2014-06.csv\r\n",
      "data/sampled/green_tripdata_2014-07.csv\r\n",
      "data/sampled/green_tripdata_2014-08.csv\r\n",
      "data/sampled/green_tripdata_2014-09.csv\r\n",
      "data/sampled/green_tripdata_2014-10.csv\r\n",
      "data/sampled/green_tripdata_2014-11.csv\r\n",
      "data/sampled/green_tripdata_2014-12.csv\r\n",
      "data/sampled/green_tripdata_2015-01.csv\r\n",
      "data/sampled/green_tripdata_2015-02.csv\r\n",
      "data/sampled/green_tripdata_2015-03.csv\r\n",
      "data/sampled/green_tripdata_2015-04.csv\r\n",
      "data/sampled/green_tripdata_2015-05.csv\r\n",
      "data/sampled/green_tripdata_2015-06.csv\r\n",
      "data/sampled/green_tripdata_2015-07.csv\r\n",
      "data/sampled/green_tripdata_2015-08.csv\r\n",
      "data/sampled/green_tripdata_2015-09.csv\r\n",
      "data/sampled/green_tripdata_2015-10.csv\r\n",
      "data/sampled/green_tripdata_2015-11.csv\r\n",
      "data/sampled/green_tripdata_2015-12.csv\r\n",
      "data/sampled/green_tripdata_2016-01.csv\r\n",
      "data/sampled/green_tripdata_2016-02.csv\r\n",
      "data/sampled/green_tripdata_2016-03.csv\r\n",
      "data/sampled/green_tripdata_2016-04.csv\r\n",
      "data/sampled/green_tripdata_2016-05.csv\r\n",
      "data/sampled/green_tripdata_2016-06.csv\r\n",
      "data/sampled/green_tripdata_2016-07.csv\r\n",
      "data/sampled/green_tripdata_2016-08.csv\r\n",
      "data/sampled/green_tripdata_2016-09.csv\r\n",
      "data/sampled/green_tripdata_2016-10.csv\r\n",
      "data/sampled/green_tripdata_2016-11.csv\r\n",
      "data/sampled/green_tripdata_2016-12.csv\r\n",
      "data/sampled/green_tripdata_2017-01.csv\r\n",
      "data/sampled/green_tripdata_2017-02.csv\r\n",
      "data/sampled/green_tripdata_2017-03.csv\r\n",
      "data/sampled/green_tripdata_2017-04.csv\r\n",
      "data/sampled/green_tripdata_2017-05.csv\r\n",
      "data/sampled/green_tripdata_2017-06.csv\r\n",
      "data/sampled/green_tripdata_2017-07.csv\r\n",
      "data/sampled/green_tripdata_2017-08.csv\r\n",
      "data/sampled/green_tripdata_2017-09.csv\r\n",
      "data/sampled/green_tripdata_2017-10.csv\r\n",
      "data/sampled/green_tripdata_2017-11.csv\r\n",
      "data/sampled/green_tripdata_2017-12.csv\r\n",
      "data/sampled/green_tripdata_2018-01.csv\r\n",
      "data/sampled/green_tripdata_2018-02.csv\r\n",
      "data/sampled/green_tripdata_2018-03.csv\r\n",
      "data/sampled/green_tripdata_2018-04.csv\r\n",
      "data/sampled/green_tripdata_2018-05.csv\r\n",
      "data/sampled/green_tripdata_2018-06.csv\r\n",
      "data/sampled/green_tripdata_2018-07.csv\r\n",
      "data/sampled/green_tripdata_2018-08.csv\r\n",
      "data/sampled/green_tripdata_2018-09.csv\r\n",
      "data/sampled/green_tripdata_2018-10.csv\r\n",
      "data/sampled/green_tripdata_2018-11.csv\r\n",
      "data/sampled/green_tripdata_2018-12.csv\r\n",
      "data/sampled/green_tripdata_2019-01.csv\r\n",
      "data/sampled/green_tripdata_2019-02.csv\r\n",
      "data/sampled/green_tripdata_2019-03.csv\r\n",
      "data/sampled/green_tripdata_2019-04.csv\r\n",
      "data/sampled/green_tripdata_2019-05.csv\r\n",
      "data/sampled/green_tripdata_2019-06.csv\r\n",
      "data/sampled/green_tripdata_2020-01.csv\r\n",
      "data/sampled/green_tripdata_2020-02.csv\r\n",
      "data/sampled/green_tripdata_2020-04.csv\r\n",
      "data/sampled/green_tripdata_2020-05.csv\r\n",
      "data/sampled/green_tripdata_2020-06.csv\r\n"
     ]
    }
   ],
   "source": [
    "########## \n",
    "## NEED TO USE GEOPANDAS !!\n",
    "import geopandas as gpd\n",
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='green'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('../../Master_BDGA/data/metadata/taxi_zones.shp')\n",
    "    \n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "    month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "    date_file = date(year,month,1)\n",
    "    green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files[yr]) )\n",
    "    for nb_schema in range(0,len(dating_schema)-1):\n",
    "        print(date_file)\n",
    "        if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "            if nb_schema+1 == 1 :\n",
    "                # Transform LAT-LON in location ID\n",
    "                \n",
    "                # Create the new file\n",
    "                green1_DF = \n",
    "                green1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 2 :\n",
    "                fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 3 :\n",
    "                fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 4 :\n",
    "                fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "            elif nb_schema+1 == 5 :\n",
    "                fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "    if date_file == dating_schema[5].date() :\n",
    "        fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/fhv'))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POINT (-73.93002319335938 40.75640487670898)\n",
      "145\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2454.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 336, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-74-3018fbbce4a6>\", line 30, in convertlocID\nUnboundLocalError: local variable 'locationID' referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-74-3018fbbce4a6>\", line 30, in convertlocID\nUnboundLocalError: local variable 'locationID' referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-3018fbbce4a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mto_locID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconvertlocID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#green_DF = green_DF.withColumn('new_column', to_locID(green_DF['pickup_longitude'], green_DF['pickup_latitude']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreen_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickup_longitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pickup_latitude'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2454.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 336.0 failed 1 times, most recent failure: Lost task 0.0 in stage 336.0 (TID 336, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-74-3018fbbce4a6>\", line 30, in convertlocID\nUnboundLocalError: local variable 'locationID' referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/seb/Documents/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-74-3018fbbce4a6>\", line 30, in convertlocID\nUnboundLocalError: local variable 'locationID' referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pyspark.sql.functions as f\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import col, lit\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(\"data/sampled/green_tripdata_2013-09.csv\") )\n",
    "\n",
    "my_list_lat = green_DF.select(f.collect_list('pickup_latitude')).first()[0]\n",
    "\n",
    "my_list_lon = green_DF.select(f.collect_list('pickup_longitude')).first()[0]\n",
    "\n",
    "\n",
    "def convertlocID(lon, lat):\n",
    "    query_point = Point( lon, lat)\n",
    "    print(query_point)\n",
    "    possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "    for i in range(0,len(possible_matches)) :\n",
    "        if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "            locationID = possible_matches[i]\n",
    "    \n",
    "    return locationID \n",
    "\n",
    "convertlocID(my_list_lon[0],my_list_lat[0])\n",
    "print(locationID)\n",
    "\n",
    "apply_test = f.udf(convertlocID, IntegerType())\n",
    "to_locID = f.udf(lambda y, z: convertlocID(y,z), IntegerType()) \n",
    "#green_DF = green_DF.withColumn('new_column', to_locID(green_DF['pickup_longitude'], green_DF['pickup_latitude']))\n",
    "df = green_DF.withColumn('new_column', apply_test(col('pickup_longitude'), col('pickup_latitude'))).show()\n",
    "\n",
    "\n",
    "#spark.udf.register(\"to_locID\", to_locID)\n",
    "#green_DF.select(col(\"pickup_longitude\", \"pickup_latitude\" ), \\\n",
    "#    to_locID(col(\"LocationID\")).alias(\"LocationID\") ) \\\n",
    "#   .show(truncate=False)\n",
    "#green_DF.withColumn('LocationID', to_locID('pickup_longitude','pickup_latitude')).show()\n",
    "\n",
    "#df2 = green_DF.withColumn(['pickup_longitude','pickup_latitude'],to_locID('LocationID'))\n",
    "\n",
    "# Now let's find the zone of our first trip's pickup point.\n",
    "#for index, tup in zones.iterrows():\n",
    "#    if tup.geometry.contains(query_point):\n",
    "#        print(\"LocationID=%d, borough=%s, zone=%s\" % (tup.LocationID, tup.borough, tup.zone))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- Pickup_longitude: double (nullable = true)\n",
      " |-- Pickup_latitude: double (nullable = true)\n",
      " |-- Dropoff_longitude: double (nullable = true)\n",
      " |-- Dropoff_latitude: double (nullable = true)\n",
      " |-- Passenger_count: integer (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- Fare_amount: double (nullable = true)\n",
      " |-- Extra: double (nullable = true)\n",
      " |-- MTA_tax: double (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Tolls_amount: double (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      " |-- Payment_type: integer (nullable = true)\n",
      " |-- Trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|Lpep_dropoff_datetime|Store_and_fwd_flag|RateCodeID|  Pickup_longitude|   Pickup_latitude| Dropoff_longitude| Dropoff_latitude|Passenger_count|Trip_distance|Fare_amount|Extra|MTA_tax|Tip_amount|Tolls_amount|Ehail_fee|Total_amount|Payment_type|Trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "|       2| 2014-12-19 18:56:47|  2014-12-19 19:16:21|                 N|         1|-73.87925720214844|40.740623474121094|-73.91719055175781|40.74149703979492|              2|         2.18|       13.5|  1.0|    0.5|       0.0|         0.0|     null|        15.0|           2|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+------------------+------------------+-----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+------------+------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- Pickup_longitude: double (nullable = true)\n",
      " |-- Pickup_latitude: double (nullable = true)\n",
      " |-- Dropoff_longitude: double (nullable = true)\n",
      " |-- Dropoff_latitude: double (nullable = true)\n",
      " |-- Passenger_count: integer (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- Fare_amount: double (nullable = true)\n",
      " |-- Extra: double (nullable = true)\n",
      " |-- MTA_tax: double (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Tolls_amount: double (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      " |-- Payment_type: integer (nullable = true)\n",
      " |-- Trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|Lpep_dropoff_datetime|Store_and_fwd_flag|RateCodeID|  Pickup_longitude|  Pickup_latitude| Dropoff_longitude|Dropoff_latitude|Passenger_count|Trip_distance|Fare_amount|Extra|MTA_tax|Tip_amount|Tolls_amount|Ehail_fee|improvement_surcharge|Total_amount|Payment_type|Trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|       2| 2015-01-21 20:07:58|  2015-01-21 20:19:47|                 N|         1|-73.95250701904297|40.81092071533203|-73.94770812988281|40.7846794128418|              1|         2.48|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|           1|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------------+-----------------+------------------+----------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "|       2| 2016-07-24 18:50:57|  2016-07-24 18:56:09|                 N|         1|          95|         102|              1|         1.49|        6.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         7.3|           2|        1|\n",
      "|       2| 2016-07-27 08:40:44|  2016-07-27 09:06:41|                 N|         1|         244|         236|              1|         4.34|       19.0|  0.0|    0.5|       4.0|         0.0|     null|                  0.3|        23.8|           1|        1|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read fhv files\n",
    "green1_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2014-12.csv'))\n",
    "green1_DF.printSchema()\n",
    "green1_DF.show(1)\n",
    "green2_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2015-01.csv'))\n",
    "green2_DF.printSchema()\n",
    "green2_DF.show(1)\n",
    "green3_DF = (spark.read\n",
    "           .option(\"sep\", \",\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "            .csv('data/sampled/green_tripdata_2016-07.csv'))\n",
    "green3_DF.printSchema()\n",
    "green3_DF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2015-01-21 20:07:58|  2015-01-21 20:19:47|                 N|         1|              1|         2.48|       11.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        12.3|           1|        1|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##1.modifing schema as needed\n",
    "green_201412 = green1_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"VendorID\").alias(\"VendorID\"),\n",
    "                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                            col(\"RateCodeID\").alias(\"RatecodeID\"),\n",
    "                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                            col(\"Extra\").alias(\"extra\"),\n",
    "                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                            col(\"improvement_surcharge\").alias(\"improvement_surcharge\"),\n",
    "                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                            \"congestion_surcharge\")\n",
    "#green_201412.show(1)\n",
    "#\n",
    "#2.modifing schema as needed (with pu/do location id)\n",
    "green_201501 = green2_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                        .select(\n",
    "                            col(\"VendorID\").alias(\"VendorID\"),\n",
    "                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                            col(\"RateCodeID\").alias(\"RatecodeID\"),\n",
    "                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                            col(\"Extra\").alias(\"extra\"),\n",
    "                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                            col(\"improvement_surcharge\").alias(\"improvement_surcharge\"),\n",
    "                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                            \"congestion_surcharge\")\n",
    "\n",
    "green_201501.show(1)\n",
    "\n",
    "#3.modifing schema as needed\n",
    "green_201707 = green3_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "#green_201707.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for geo location\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this subdf into a GeoDF, adding a \"geometry\" column to pinpoint the pickup location\n",
    "geometry = [Point(xy) for xy in zip(subdf['pickup_longitude'], subdf['pickup_latitude'])]\n",
    "geo_subdf = gpd.GeoDataFrame(subdf, geometry=geometry)\n",
    "geo_subdf\n",
    "# A geopanda dataframe has the possibility to create an R-tree index on it's geometry\n",
    "rtree = zones.sindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 236, 42]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By means of the intersection() method we can query for all the entries in the zones dataframe that \n",
    "# *can* intersect with a query point\n",
    "# Note: this mentod can return false positives; the actual zone is part of the result.\n",
    "# The method returns a generator. We use the list(.) constructor to convert this to a list.\n",
    "\n",
    "query_point = Point( df1.iloc[0].pickup_longitude, df1.iloc[0].pickup_latitude)\n",
    "possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "possible_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>141</td>\n",
       "      <td>0.041514</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>141</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.96178 40.75988, -73.96197 40.759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>Upper East Side South</td>\n",
       "      <td>237</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.96613 40.76218, -73.96658 40.761...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>0.099739</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>Central Park</td>\n",
       "      <td>43</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.97255 40.76490, -73.97301 40.764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                   zone  LocationID  \\\n",
       "140       141    0.041514    0.000077        Lenox Hill West         141   \n",
       "236       237    0.042213    0.000096  Upper East Side South         237   \n",
       "42         43    0.099739    0.000380           Central Park          43   \n",
       "\n",
       "       borough                                           geometry  \n",
       "140  Manhattan  POLYGON ((-73.96178 40.75988, -73.96197 40.759...  \n",
       "236  Manhattan  POLYGON ((-73.96613 40.76218, -73.96658 40.761...  \n",
       "42   Manhattan  POLYGON ((-73.97255 40.76490, -73.97301 40.764...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones.iloc[ possible_matches ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow files\n",
    "\n",
    "For yellow there are 131 files:\n",
    "\n",
    " In 2010 - 1 :\n",
    " \n",
    "   12 diff on a total of 18 col: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "         12/12 column name have changed:\n",
    "         \n",
    " In 2015 - 1 :\n",
    " \n",
    "   6 diff on a total of 19 col: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
    "         1/6 col add\n",
    "         5/6 name change\n",
    "         \n",
    " In 2016 - 7 :\n",
    " \n",
    "   2 diff on a total of 17 col: ['pulocationid', 'dolocationid']\n",
    "         2/2 col remove\n",
    "         \n",
    " In 2019 - 1 :\n",
    " \n",
    "   1 diff on a total of 18 col: ['congestion_surcharge']\n",
    "         1/1 col add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of fhv tripdata files:\n",
      "data/sampled/green_tripdata_2013-08.csv\r\n",
      "data/sampled/green_tripdata_2013-09.csv\r\n",
      "data/sampled/green_tripdata_2013-10.csv\r\n",
      "data/sampled/green_tripdata_2013-11.csv\r\n",
      "data/sampled/green_tripdata_2013-12.csv\r\n",
      "data/sampled/green_tripdata_2014-01.csv\r\n",
      "data/sampled/green_tripdata_2014-02.csv\r\n",
      "data/sampled/green_tripdata_2014-03.csv\r\n",
      "data/sampled/green_tripdata_2014-04.csv\r\n",
      "data/sampled/green_tripdata_2014-05.csv\r\n",
      "data/sampled/green_tripdata_2014-06.csv\r\n",
      "data/sampled/green_tripdata_2014-07.csv\r\n",
      "data/sampled/green_tripdata_2014-08.csv\r\n",
      "data/sampled/green_tripdata_2014-09.csv\r\n",
      "data/sampled/green_tripdata_2014-10.csv\r\n",
      "data/sampled/green_tripdata_2014-11.csv\r\n",
      "data/sampled/green_tripdata_2014-12.csv\r\n",
      "data/sampled/green_tripdata_2015-01.csv\r\n",
      "data/sampled/green_tripdata_2015-02.csv\r\n",
      "data/sampled/green_tripdata_2015-03.csv\r\n",
      "data/sampled/green_tripdata_2015-04.csv\r\n",
      "data/sampled/green_tripdata_2015-05.csv\r\n",
      "data/sampled/green_tripdata_2015-06.csv\r\n",
      "data/sampled/green_tripdata_2015-07.csv\r\n",
      "data/sampled/green_tripdata_2015-08.csv\r\n",
      "data/sampled/green_tripdata_2015-09.csv\r\n",
      "data/sampled/green_tripdata_2015-10.csv\r\n",
      "data/sampled/green_tripdata_2015-11.csv\r\n",
      "data/sampled/green_tripdata_2015-12.csv\r\n",
      "data/sampled/green_tripdata_2016-01.csv\r\n",
      "data/sampled/green_tripdata_2016-02.csv\r\n",
      "data/sampled/green_tripdata_2016-03.csv\r\n",
      "data/sampled/green_tripdata_2016-04.csv\r\n",
      "data/sampled/green_tripdata_2016-05.csv\r\n",
      "data/sampled/green_tripdata_2016-06.csv\r\n",
      "data/sampled/green_tripdata_2016-07.csv\r\n",
      "data/sampled/green_tripdata_2016-08.csv\r\n",
      "data/sampled/green_tripdata_2016-09.csv\r\n",
      "data/sampled/green_tripdata_2016-10.csv\r\n",
      "data/sampled/green_tripdata_2016-11.csv\r\n",
      "data/sampled/green_tripdata_2016-12.csv\r\n",
      "data/sampled/green_tripdata_2017-01.csv\r\n",
      "data/sampled/green_tripdata_2017-02.csv\r\n",
      "data/sampled/green_tripdata_2017-03.csv\r\n",
      "data/sampled/green_tripdata_2017-04.csv\r\n",
      "data/sampled/green_tripdata_2017-05.csv\r\n",
      "data/sampled/green_tripdata_2017-06.csv\r\n",
      "data/sampled/green_tripdata_2017-07.csv\r\n",
      "data/sampled/green_tripdata_2017-08.csv\r\n",
      "data/sampled/green_tripdata_2017-09.csv\r\n",
      "data/sampled/green_tripdata_2017-10.csv\r\n",
      "data/sampled/green_tripdata_2017-11.csv\r\n",
      "data/sampled/green_tripdata_2017-12.csv\r\n",
      "data/sampled/green_tripdata_2018-01.csv\r\n",
      "data/sampled/green_tripdata_2018-02.csv\r\n",
      "data/sampled/green_tripdata_2018-03.csv\r\n",
      "data/sampled/green_tripdata_2018-04.csv\r\n",
      "data/sampled/green_tripdata_2018-05.csv\r\n",
      "data/sampled/green_tripdata_2018-06.csv\r\n",
      "data/sampled/green_tripdata_2018-07.csv\r\n",
      "data/sampled/green_tripdata_2018-08.csv\r\n",
      "data/sampled/green_tripdata_2018-09.csv\r\n",
      "data/sampled/green_tripdata_2018-10.csv\r\n",
      "data/sampled/green_tripdata_2018-11.csv\r\n",
      "data/sampled/green_tripdata_2018-12.csv\r\n",
      "data/sampled/green_tripdata_2019-01.csv\r\n",
      "data/sampled/green_tripdata_2019-02.csv\r\n",
      "data/sampled/green_tripdata_2019-03.csv\r\n",
      "data/sampled/green_tripdata_2019-04.csv\r\n",
      "data/sampled/green_tripdata_2019-05.csv\r\n",
      "data/sampled/green_tripdata_2019-06.csv\r\n",
      "data/sampled/green_tripdata_2020-01.csv\r\n",
      "data/sampled/green_tripdata_2020-02.csv\r\n",
      "data/sampled/green_tripdata_2020-04.csv\r\n",
      "data/sampled/green_tripdata_2020-05.csv\r\n",
      "data/sampled/green_tripdata_2020-06.csv\r\n"
     ]
    }
   ],
   "source": [
    "print ('list of yellow tripdata files:')\n",
    "!ls data/sampled/yellow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
