{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration\n",
    "\n",
    "For each sub-dataset, write (and execute) code that converts a file (using possibly an old schema) into a file that has the new, latest schema version.\n",
    "\n",
    "Your conversion code should not modify the original files, but instead create a new file. Be sure to explain the design behind your conversion functions!\n",
    "\n",
    "The data integration step is highly parallellizable. Therefore, your solution on this part\n",
    "**must** be written in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: this notebook assumes that:\n",
    "\n",
    "- The data are in \"MY_PARENT_FOLDER/data/sampled/\" folder. You can run the bash script \"download_metadata.sh\" to download data and metadata in the correct folders to execute the jupyter notebooks.\n",
    "- The data are sampled to be run on a personnal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "import pyspark.sql.functions as f\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory data/cleaned\n",
      "Successfully created the directory data/cleaned/yellow \n",
      "Successfully created the directory data/cleaned/green \n",
      "Successfully created the directory data/cleaned/fhv \n",
      "Successfully created the directory data/cleaned/fhvhv \n"
     ]
    }
   ],
   "source": [
    "#create cleaned data directories\n",
    "try :  \n",
    "    os.path.isdir(\"data/cleaned\")\n",
    "except OSError:\n",
    "    os.mkdir(\"data/cleaned\")\n",
    "    print (\"Creation of the directory data/cleaned failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory data/cleaned\")\n",
    "\n",
    "list_taxi = [\"yellow\", \"green\", \"fhv\", \"fhvhv\"]\n",
    "#list_taxi = [\"green\"]\n",
    "for taxi_brand in list_taxi :\n",
    "    path = \"data/cleaned/%s\" %(taxi_brand)\n",
    "    # List the file from the same taxi company brand \n",
    "    try:\n",
    "        os.path.isdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s\" % path)\n",
    "        os.mkdir(path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FHVHV files\n",
    "\n",
    "From previous analyses we saw that header was consistent across all then fhvhv files.\n",
    "We then donc need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir= '/data/sampled/'       \n",
    "for filename in glob.glob(os.path.join(source_dir,'fhvhv_*.csv')):\n",
    "    shutil.copy(filename, 'data/cleanned/fhvhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.FHV files\n",
    "\n",
    "From previous analyse we decide to use as reference for the FHV taxi files the following schema:\n",
    "\n",
    "['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'pulocationid', 'dolocationid', 'sr_flag'] \n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_fhv.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Add to the files empty columns for 'dropoff_datetime', 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'pickup_datetime', 'locationID' by 'PULocationID',        \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "\n",
    "- Change schema 2 : \n",
    "            a) Add to the files empty columns for 'DOLocationID' and 'SR_Flag'. \n",
    "            b) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 3 : \n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_num\" by \"dispatching_base_num\".\n",
    "            \n",
    "- Change schema 4 :\n",
    "            a) Change the columns name 'Pickup_date' by 'Pickup_DateTime', 'Dropoff_datetime' by 'dropoff_datetime', \"Dispatching_base_number\" by \"dispatching_base_num\".\n",
    "            b) Remove the double column Dispatching_base_num with no value\n",
    "          \n",
    "- Final schema 5 :\n",
    "            NO change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 64 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='fhv'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        fhv_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            print(date_file)\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                if nb_schema+1 == 1 :\n",
    "                    fhv1_DF = fhv_DF.withColumn(\"dropoff_datetime\",lit('null'))\\\n",
    "                           .withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                           .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                           .select(\n",
    "                            col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                            col(\"Pickup_date\").alias(\"pickup_datetime\"),\n",
    "                            \"dropoff_datetime\",\n",
    "                            col(\"locationID\").alias(\"PULocationID\"),\n",
    "                            \"DOLocationID\",\n",
    "                            \"SR_Flag\")\n",
    "                    fhv1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    fhv2_DF = fhv_DF.withColumn(\"DOLocationID\",lit('null'))\\\n",
    "                            .withColumn(\"SR_Flag\",lit('null'))\\\n",
    "                            .select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    fhv3_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_num\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    fhv4_DF = fhv_DF.select(\n",
    "                                col(\"Dispatching_base_number\").alias(\"dispatching_base_num\"),\n",
    "                                col(\"Pickup_DateTime\").alias(\"pickup_datetime\"),\n",
    "                                col(\"Dropoff_datetime\").alias(\"dropoff_datetime\"),\n",
    "                                \"PULocationID\",\n",
    "                                \"DOLocationID\",\n",
    "                                \"SR_Flag\")\n",
    "                    fhv4_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 5 :\n",
    "                    fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "        if date_file == dating_schema[5].date() :\n",
    "            fhv5_DF = fhv_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Green files\n",
    "\n",
    "From previous analyse we decide to use as reference for the GREEN taxi files the following schema:\n",
    "\n",
    "['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    " \n",
    "We therefore need to apply somes transformations for creating new uniform files according to the time period previously defined and saved in the file Change_date_green.csv:\n",
    "\n",
    "- Change schema 1 : \n",
    "            a) Two new columns are add : congestion_surcharge and improvement_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "           \n",
    "- Change in schema 2 :\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) The columns 'pickup_longitude', 'pickup_latitude' and 'dropoff_longitude', 'dropoff_latitude' are respectively changed by 'pulocationid' and 'dolocationid'. The transformation use geopandas to transform lat-lon position to location id.\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Change in schema 3 :\n",
    "            a) One new column is add : congestion_surcharge\n",
    "            b) For all the others columns the upper case format letters are changed by lower case format.\n",
    "\n",
    "- Final schema 4 :\n",
    "            NO change\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the 76 files are well integrated !\n"
     ]
    }
   ],
   "source": [
    "source_dir= 'data/sampled/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='green'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/sampled/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "\n",
    "# Creation of a function to convert lat-lon into location ID\n",
    "def convertlocID(lon, lat):\n",
    "    global locationID # access the outer scope variable by declaring it global\n",
    "    if int(lon) != 0 and int(lat) != 0:\n",
    "        query_point = Point( lon, lat)\n",
    "        possible_matches = list(rtree.intersection( query_point.bounds ))\n",
    "        for i in range(0,len(possible_matches)) :\n",
    "            if zones.iloc[possible_matches[i]].geometry.contains(query_point) == True :\n",
    "                locationID = possible_matches[i]\n",
    "    else:\n",
    "        locationID = 9999\n",
    "    \n",
    "    return locationID\n",
    "\n",
    "# Load the shapefile, this yields a GeoDataFrame that has a row for each zone\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "rtree = zones.sindex\n",
    "\n",
    "# Open the date change file\n",
    "df = pd.read_csv(\"data/Change_date_%s.csv\" %(taxi_brand), sep=',', header=None)\n",
    "dating_schema = [ datetime.strptime(x, '%Y-%m-%d') for x in df[1] ]\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::]) == False :\n",
    "        year = int(list_files[yr][len(taxi_brand)+23:len(taxi_brand)+27])\n",
    "        month = int(list_files[yr][len(taxi_brand)+28:len(taxi_brand)+30])\n",
    "        date_file = date(year,month,1)\n",
    "        green_DF = (spark.read\n",
    "                    .option(\"sep\", \",\")\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"inferSchema\", True)\n",
    "                    .csv(list_files[yr]) )\n",
    "        for nb_schema in range(0,len(dating_schema)-1):\n",
    "            Drop_ID = []\n",
    "            Pick_ID = []\n",
    "            if date_file >= dating_schema[nb_schema].date() and  date_file < dating_schema[nb_schema+1].date():\n",
    "                print(date_file)\n",
    "                if nb_schema+1 == 1 :\n",
    "                    print(\"schema 1 for file:\",list_files[yr])\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green1_DF = DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                             .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                             .withColumn(\"improvement_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green1_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 2 :\n",
    "                    print(\"schema 2\")\n",
    "                    # Transform LAT-LON in location ID\n",
    "                    Pickup_list_lat = green_DF.select(f.collect_list('Pickup_latitude')).first()[0]\n",
    "                    Pickup_list_lon = green_DF.select(f.collect_list('Pickup_longitude')).first()[0]\n",
    "                    Dropoff_list_lat = green_DF.select(f.collect_list('Dropoff_latitude')).first()[0]\n",
    "                    Dropoff_list_lon = green_DF.select(f.collect_list('Dropoff_longitude')).first()[0]\n",
    "                    for i in range(0,len(Pickup_list_lat)):\n",
    "                        a = convertlocID(Pickup_list_lon[i],Pickup_list_lat[i])\n",
    "                        Pick_ID.append(a) \n",
    "                    for i in range(0,len(Dropoff_list_lat)):\n",
    "                        a = convertlocID(Dropoff_list_lon[i],Dropoff_list_lat[i])\n",
    "                        Drop_ID.append(a)\n",
    "                    # Create the new file\n",
    "                    green2_DF = green_DF.withColumn(\"pulocationid\",\n",
    "                                                            f.udf(lambda id: Pick_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"dolocationid\",\n",
    "                                                            f.udf(lambda id: Drop_ID[id])(f.monotonically_increasing_id()))\\\n",
    "                                        .withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green2_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 3 :\n",
    "                    print(\"schema 3\")\n",
    "                    green3_DF = green_DF.withColumn(\"congestion_surcharge\",lit('null'))\\\n",
    "                                                        .select(\n",
    "                                                            col(\"VendorID\").alias(\"vendorID\"),\n",
    "                                                            col(\"lpep_pickup_datetime\").alias(\"lpep_pickup_datetime\"),\n",
    "                                                            col(\"Lpep_dropoff_datetime\").alias(\"lpep_dropoff_datetime\"),\n",
    "                                                            col(\"Store_and_fwd_flag\").alias(\"store_and_fwd_flag\"),\n",
    "                                                            col(\"RateCodeID\").alias(\"ratecodeID\"),\n",
    "                                                            \"pulocationid\",\n",
    "                                                            \"dolocationid\",\n",
    "                                                            col(\"Passenger_count\").alias(\"passenger_count\"),\n",
    "                                                            col(\"Trip_distance\").alias(\"trip_distance\"),\n",
    "                                                            col(\"Fare_amount\").alias(\"fare_amount\"),\n",
    "                                                            col(\"Extra\").alias(\"extra\"),\n",
    "                                                            col(\"MTA_tax\").alias(\"mta_tax\"),\n",
    "                                                            col(\"Tip_amount\").alias(\"tip_amount\"),\n",
    "                                                            col(\"Tolls_amount\").alias(\"tolls_amount\"),\n",
    "                                                            col(\"Ehail_fee\").alias(\"ehail_fee\"),\n",
    "                                                            \"improvement_surcharge\",\n",
    "                                                            col(\"Total_amount\").alias(\"total_amount\"),\n",
    "                                                            col(\"Payment_type\").alias(\"payment_type\"),\n",
    "                                                            col(\"Trip_type\").alias(\"trip_type\"),\n",
    "                                                            \"congestion_surcharge\")\n",
    "                    green3_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "                elif nb_schema+1 == 4 :\n",
    "                    print(\"schema 4\")\n",
    "                    green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "        if date_file == dating_schema[4].date() :\n",
    "            print(\"schema LAST\")\n",
    "            green4_DF = green_DF.toPandas().to_csv(clean_dir+taxi_brand+'/'+list_files[yr][len(taxi_brand)+14::])\n",
    "new_files = len(os.listdir('data/cleaned/'+taxi_brand))\n",
    "if new_files == nb_files :\n",
    "    print(\"All the %i files are well integrated !\" %(new_files))\n",
    "else :\n",
    "    print(\"[ERROR] %i files on %i files have been integrated ...\" %(new_files, nb_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow files\n",
    "\n",
    "For yellow there are 131 files:\n",
    "\n",
    " In 2010 - 1 :\n",
    " \n",
    "   12 diff on a total of 18 col: ['vendor_id', 'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'tip_amount', 'tolls_amount', 'total_amount']\n",
    "         12/12 column name have changed:\n",
    "         \n",
    " In 2015 - 1 :\n",
    " \n",
    "   6 diff on a total of 19 col: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'ratecodeid', 'extra', 'improvement_surcharge']\n",
    "         1/6 col add\n",
    "         5/6 name change\n",
    "         \n",
    " In 2016 - 7 :\n",
    " \n",
    "   2 diff on a total of 17 col: ['pulocationid', 'dolocationid']\n",
    "         2/2 col remove\n",
    "         \n",
    " In 2019 - 1 :\n",
    " \n",
    "   1 diff on a total of 18 col: ['congestion_surcharge']\n",
    "         1/1 col add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
