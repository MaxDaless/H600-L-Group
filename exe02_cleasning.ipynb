{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook is inteded to integrate and clean the project data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Integration\n",
    "-If a column exists in the old schema version, but not in the new one, drop its contents.\n",
    "\n",
    "-If a column exists both in the old schema version and the new (but possibly under a\n",
    "different name, or the order of the column in the new schema version is different), copy\n",
    "the value.\n",
    "\n",
    "-If a column does not exist in the old, but does exist in the new, give it a blank value\n",
    "(indicating that the record does not have such a value).\n",
    "\n",
    "-If a column does not exists in the old schema, but does exist in the new, and it is possible\n",
    "to compute the value of the new column given information in the old schema, then all in\n",
    "this computed value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "sql_c = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0002|              B02914|2019-02-25 15:29:31|2019-02-25 15:42:55|         161|         164|   null|\n",
      "|           HV0004|              B02800|2019-02-12 07:21:27|2019-02-12 07:59:23|         255|         170|      1|\n",
      "|           HV0003|              B02878|2019-02-23 09:16:03|2019-02-23 09:32:50|         168|         243|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualize a specific file\n",
    "df_green = sql_c.read.csv('testspark/fhvhv_tripdata_2019-02.csv', header=True)\n",
    "df_green.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize header\n",
    "df_green.columns[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------+\n",
      "|    pickup_datetime|PULocationID|SR_Flag|\n",
      "+-------------------+------------+-------+\n",
      "|2019-02-25 15:29:31|         161|   null|\n",
      "|2019-02-12 07:21:27|         255|      1|\n",
      "|2019-02-23 09:16:03|         168|   null|\n",
      "+-------------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualize only specific columns in a specific order\n",
    "df_test3column=df_green.select('pickup_datetime','PULocationID','SR_Flag').show(3)\n",
    "\n",
    "#try to export that visu into a new csv file\n",
    "#df_test3column.write.format('com.databricks.spark.csv').save('testspark/modified/3columns.csv')\n",
    "#df_test3column.save('testspark/modified/3columns.csv', 'com.databricks.spark.csv')\n",
    "#df_test3column.write.csv('testspark/modified/3columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|    pickup_datetime|PULocationID|\n",
      "+-------------------+------------+\n",
      "|2019-02-25 15:29:31|         161|\n",
      "|2019-02-12 07:21:27|         255|\n",
      "|2019-02-23 09:16:03|         168|\n",
      "+-------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop a column\n",
    "df_green.select('pickup_datetime','PULocationID').drop('SR_Flag').show(3)\n",
    "#df_test3column.drop('pickup_datetime').collect(3)\n",
    "df_green.columns[:7]\n",
    "#SR8Flag is still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fileName = 'Data/green_tripdata_2020-01.csv'\n",
    "GreenRDD = sc.textFile(fileName)\n",
    "GreenRDD.take(5)\n",
    "df = GreenRDD\n",
    "df.drop('congestion_surcharge')\n",
    "#drop column:\n",
    "#    df = dataframe\n",
    "#    df.drop('age').collect()\n",
    "#    \n",
    "#change order:\n",
    "#    https://www.datasciencemadesimple.com/re-arrange-or-re-order-column-in-pyspark/#:~:text=In%20order%20to%20Rearrange%20or,rearrange%20the%20column%20by%20position.\n",
    "#    \n",
    "#add column:\n",
    "#    df.withColumn(\"location\", lit())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
