{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import Row\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Create list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_list(path, brand, list_files):\n",
    "    \"\"\"\n",
    "    This function create the files list of specify taxi brand (brand) from the specify folder (path). \n",
    "    \n",
    "    Input: the path where are the files -> /data/cleaned or data/sampled\n",
    "           the name of the taxi company -> fhv, fhvfh, green, yellow\n",
    "           the empty file name list in which each file will be append\n",
    "    Output: number of files in the list and the list of files name.\n",
    "    \"\"\"  \n",
    "    global nb_files\n",
    "    nb_files = 0\n",
    "    for file in glob.glob(\"%s/%s/*.csv\" %(path,brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files.append(file)\n",
    "        # Order by date the file list\n",
    "        list_files.sort()\n",
    "\n",
    "    return list_files, nb_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds(df):\n",
    "    bounds = {\n",
    "        c: dict(\n",
    "            zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "        )\n",
    "        for c,d in zip(df.columns, df.dtypes) if d[1] == \"int\"\n",
    "    }\n",
    "\n",
    "    for c in bounds:\n",
    "        iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "        bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "        bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df, id_col):\n",
    "    bounds = calculate_bounds(df)\n",
    "    outliers = {}\n",
    "\n",
    "    return df.select(c, id_col,\n",
    "            *[\n",
    "                f.when(\n",
    "                    ~f.col(c).between(bounds[c]['min'], bounds[c]['max']),\n",
    "                    \"yes\"\n",
    "                ).otherwise(\"no\").alias(c+'_outlier')\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'green_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-25732369903c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#flag_outliers(green_DF,'Tip_amount')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#green_DF.show(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'green_DF' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_bounds(green_DF)\n",
    "#flag_outliers(green_DF,'Tip_amount')\n",
    "#green_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 handle negative values\n",
    "def neg_val_treat(df, features):\n",
    "    \"\"\"\n",
    "    This function handles negative values for specific columns where we absolutely\n",
    "    do not want negative values. In this case, we want to drop the rows that are concerned by\n",
    "    such values. The features targeted are provided in a list of their names.\n",
    "    \n",
    "    Input: the dataframe to prepare and the name of the features to analyse\n",
    "        \n",
    "    Output: the dataframe without the rows for which any of the targeted features has\n",
    "    negative values.\n",
    "    \"\"\"\n",
    "    \n",
    "    #for each columns listed as an input, we drop the rows that have negative values\n",
    "    for feature in features:\n",
    "        neg_condition = df[df[feature] < 0].index\n",
    "        df = df.drop(neg_condition)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. identify non consistent data\n",
    "#check consistency between total amount and all fees (yellow and green)\n",
    "#check consistency regarding datatype\n",
    "\n",
    "#4 fill in with data\n",
    "# for numeric or fees => mean of all the rest\n",
    "# for other => delete the row ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_no_passenger(df):\n",
    "    \"\"\"\n",
    "    This function splits the dataframe into two dataframes:\n",
    "    - one with only the trips with no passengers\n",
    "    - one with all the trips that have at least one passenger\n",
    "\n",
    "    Input: the dataframe to prepare\n",
    "        \n",
    "    Output: two dataframes, one with passengers, and the other one with only the 'empty trips'\n",
    "    \"\"\"\n",
    "    \n",
    "    #We create two dataframes, one with only the trips with no passengers, the other with passengers\n",
    "    no_pass_condition = df[df['passenger_count'] == 0].index\n",
    "    df_full = df.drop(no_pass_condition)\n",
    "    df_empty = df[df['passenger_count']==0]\n",
    "    \n",
    "    return df_empty, df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Yellow taxi records are records that record trip information of New York's famous yellow\n",
    "taxi cars.\n",
    "\n",
    "• Green taxi records are records that record trip information by so-called 'boro' taxis a\n",
    "newer service introduced in August of 2013 to improve taxi service and availability in the\n",
    "boroughs\n",
    "\n",
    "• FHV records (short for 'For Hire Vehicles') record information from services that o\u001ber\n",
    "for-hire vehicles (such as Uber, Lyft, Via, and Juno), but also luxury limousine bases.\n",
    "\n",
    "• High volume FHV (FHVHV for short) are FHV records o\u001bered by services that make\n",
    "more than 10,000 trips per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime||\n",
    "|PULocationID|Zone in which the trip began|Integer|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer||\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','PULocationID'. Therefore, we want these columns to be not null and under good format.\n",
    "The column 'dropoff_datetime' will aslo be used. But as there is a substantial gap in values (between 2015-01 and 2016-12), we decided to remove the constraint 'Not Null' for that one.\n",
    "\n",
    "Action to be taken on fhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####HELP NEEDED\n",
    "\n",
    "source_dir= 'data/inegrated/'\n",
    "clean_dir = 'data/cleaned/'\n",
    "taxi_brand='yellow'\n",
    "list_files = []\n",
    "nb_files=0\n",
    "# List the file from the same taxi company brand \n",
    "for file in glob.glob(\"data/integrated/%s_*.csv\" %(taxi_brand)):\n",
    "    nb_files = nb_files+1\n",
    "    # Save in list the files name\n",
    "    list_files.append(file)\n",
    "    # Order by date the file list\n",
    "    list_files.sort()\n",
    "for yr in range(0,nb_files):\n",
    "    if os.path.isfile(clean_dir+taxi_brand+'/'+list_files[len(source_dir)::]) == False :\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (1389608, 6)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (1107322, 3)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (1064909, 3)\n",
      "-Changing DataTypes\n",
      "---DONE---\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (1064909, 3)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID']\n",
    "list_files_fhv = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"fhv\",list_files_fhv)\n",
    "fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhv))\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhv_DF= fhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhv_DF=fhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "fhv_DF= fhv_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhv_DF= fhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhv_DF = fhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhv_DF = fhv_DF.withColumn(\"dropoff_datetime\",fhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime (as we there is a substantial gap in value for dropoff_datetime) )\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2015-01-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhv_DF = fhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\n",
    "#                    .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhv_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Hvfhs_license_num|TLC license number of the HVFHS base or business|String||\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime|Not Null|\n",
    "|PULocationID|Zone in which the trip began|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer(smallint)|Not Null|\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| \n",
    "\n",
    "\n",
    "*Hvfhs_license_num possible values:\n",
    "• HV0002: Juno\n",
    "• HV0003: Uber\n",
    "• HV0004: Via\n",
    "• HV0005: Lyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID',DULocationID. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on fhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (321819, 7)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (321819, 4)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (311874, 4)\n",
      "-Changing DataTypes\n",
      "---DONE---\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (311867, 4)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "list_files_fhvhv = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"fhvhv\",list_files_fhvhv)\n",
    "fhvhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhvhv))\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhvhv_DF= fhvhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhvhv_DF=fhvhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "fhvhv_DF= fhvhv_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhvhv_DF= fhvhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhvhv_DF = fhvhv_DF.withColumn(\"dropoff_datetime\",fhvhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2019-02-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhvhv_DF = fhvhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhvhv_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)||\n",
    "|pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|Not Null|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|Not Null|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal||\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal||\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|Not Null|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|Not Null|\n",
    "|Trip_type|A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver|Boolean|1 or 2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorID: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- ratecodeID: double (nullable = true)\n",
      " |-- PUlocationid: integer (nullable = true)\n",
      " |-- DOlocationid: integer (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here all columns that must be NOT NULL\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','Passenger_count','Trip_distance','Tip_amount','Total_amount']\n",
    "list_files_green = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"green\",list_files_green)\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_green))\n",
    "\n",
    "green_DF.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)|1 or 2, Not Null|\n",
    "|lpep_pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|lpep_dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|???|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|   \t|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N, Not Null|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal|Not Null|\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal|   \t|\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|   \t|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "yellow_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(\"data/cleaned/yellow/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
