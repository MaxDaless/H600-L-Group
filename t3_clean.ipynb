{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import Row\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Create list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_list(path, brand, list_files):\n",
    "    \"\"\"\n",
    "    This function create the files list of specify taxi brand (brand) from the specify folder (path). \n",
    "    \n",
    "    Input: the path where are the files -> /data/cleaned or data/sampled\n",
    "           the name of the taxi company -> fhv, fhvfh, green, yellow\n",
    "           the empty file name list in which each file will be append\n",
    "    Output: number of files in the list and the list of files name.\n",
    "    \"\"\"  \n",
    "    global nb_files\n",
    "    nb_files = 0\n",
    "    for file in glob.glob(\"%s/%s/*.csv\" %(path,brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files.append(file)\n",
    "        # Order by date the file list\n",
    "        list_files.sort()\n",
    "\n",
    "    return list_files, nb_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Passenger_count': {'q1': 1.0, 'q3': 1.0, 'min': 1.0, 'max': 1.0},\n",
       " 'Trip_distance': {'q1': 1.02, 'q3': 3.55, 'min': -2.775, 'max': 7.345},\n",
       " 'Tip_amount': {'q1': 0.0, 'q3': 1.86, 'min': -2.79, 'max': 4.65},\n",
       " 'Total_amount': {'q1': 8.15,\n",
       "  'q3': 17.8,\n",
       "  'min': -6.325000000000001,\n",
       "  'max': 32.275000000000006}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_bounds(df):\n",
    "    bounds = {\n",
    "        c: dict(\n",
    "            zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "        )\n",
    "        for c,d in zip(df.columns, df.dtypes) if d[1] == \"double\"\n",
    "    }\n",
    "\n",
    "    for c in bounds:\n",
    "        iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "        bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "        bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "\n",
    "    return bounds\n",
    "\n",
    "    d\n",
    "\n",
    "calculate_bounds(green_DF)\n",
    "calculate_bounds(yellow_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df, id_col):\n",
    "    bounds = calculate_bounds(df)\n",
    "    outliers = {}\n",
    "\n",
    "    return df.select(c, id_col,\n",
    "            *[\n",
    "                f.when(\n",
    "                    ~f.col(c).between(bounds[c]['min'], bounds[c]['max']),\n",
    "                    \"yes\"\n",
    "                ).otherwise(\"no\").alias(c+'_outlier')\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle negative values\n",
    "from pyspark.sql.functions import abs\n",
    "def abs_neg_val(df, feature):\n",
    "    #for each columns listed as an input, we drop the rows that have negative values\n",
    "    df = df.withColumn('feature',abs(df.feature))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#yellow_DF = yellow_DF.withColumn(\"only_positive\", f.when(f.col(\"Tip_amount\") > 0, f.col(\"Tip_amount\")).otherwise('null'))\n",
    "#yellow_DF = yellow_DF.withColumn(\"only_positive\", f.when(f.col(\"Total_amount\") > 0, f.col(\"Total_amount\")).otherwise('null'))\n",
    "#yellow_DF = yellow_DF.withColumn(\"only_positive\", f.when(f.col(\"Tip_amount\") > 0, f.col(\"Tip_amount\")).otherwise('null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating abs for numeric columns\n",
      "---DONE---\n",
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+-------------+\n",
      "|    pickup_datetime|   dropoff_datetime|Passenger_count|Trip_distance|PULocationID|DOLocationID|Tip_amount|Total_amount|only_positive|\n",
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+-------------+\n",
      "|2016-07-24 16:37:56|2016-07-24 16:48:14|            1.0|         2.54|          66|         144|      2.26|       13.56|         2.26|\n",
      "|2016-07-20 07:46:43|2016-07-20 07:59:04|            1.0|         2.59|         152|          75|       0.0|        11.8|         null|\n",
      "|2016-07-26 22:15:21|2016-07-26 22:40:10|            1.0|         8.39|          42|          79|       0.0|        27.8|         null|\n",
      "|2016-07-01 16:44:20|2016-07-01 16:47:47|            1.0|         0.58|         145|         145|       0.0|         6.3|         null|\n",
      "|2016-07-23 02:52:49|2016-07-23 03:02:13|            1.0|         2.44|          41|         244|      3.39|       14.69|         3.39|\n",
      "|2016-07-28 19:00:04|2016-07-28 19:34:02|            1.0|         7.95|         244|         186|       0.0|        31.8|         null|\n",
      "|2016-07-29 18:41:41|2016-07-29 19:13:42|            1.0|        11.46|          49|         132|      7.36|       44.16|         7.36|\n",
      "|2016-07-25 16:12:11|2016-07-25 16:21:16|            1.0|          1.1|          75|         263|      1.65|        9.95|         1.65|\n",
      "|2016-07-12 15:06:18|2016-07-12 15:27:47|            6.0|         3.33|         129|         226|       0.0|        17.3|         null|\n",
      "|2016-07-27 15:33:06|2016-07-27 15:39:26|            1.0|         0.33|          43|          75|       0.9|         7.2|          0.9|\n",
      "|2016-07-15 10:42:37|2016-07-15 11:06:58|            1.0|         3.33|          66|         234|      4.58|       22.88|         4.58|\n",
      "|2016-07-26 11:54:19|2016-07-26 12:07:43|            1.0|         1.51|          97|          65|      2.16|       12.96|         2.16|\n",
      "|2016-07-06 08:24:07|2016-07-06 08:33:35|            1.0|         1.43|          75|         263|       1.7|        11.0|          1.7|\n",
      "|2016-07-17 15:18:29|2016-07-17 15:35:45|            1.0|          2.9|          42|         244|       1.0|        15.8|          1.0|\n",
      "|2016-07-16 03:29:35|2016-07-16 03:41:22|            1.0|         2.11|         256|          37|      2.95|       14.75|         2.95|\n",
      "|2016-07-08 22:14:24|2016-07-08 22:32:46|            1.0|         5.47|          74|         248|       0.0|        20.8|         null|\n",
      "|2016-07-24 01:38:13|2016-07-24 01:58:55|            1.0|          4.0|         255|         113|      3.56|       21.36|         3.56|\n",
      "|2016-10-16 12:10:24|2016-10-16 12:25:32|            1.0|          2.6|          65|         211|      2.75|       16.55|         2.75|\n",
      "|2016-10-21 20:18:12|2016-10-21 20:22:43|            1.0|         0.84|          56|         173|       0.0|         6.3|         null|\n",
      "|2016-10-05 13:52:37|2016-10-05 14:00:05|            1.0|          1.8|         225|          17|       0.2|         9.0|          0.2|\n",
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#abs_neg_val(yellow_DF, 'Tip_amount')\n",
    "#calculate_bounds(yellow_DF)\n",
    "from pyspark.sql.functions import abs\n",
    "# calculate absolute value for some columns\n",
    "print (\"Calculating abs for numeric columns\")\n",
    "yellow_DF = yellow_DF.withColumn('Trip_distance',abs(yellow_DF.Trip_distance))\\\n",
    "                    .withColumn('Total_amount',abs(yellow_DF.Total_amount))\\\n",
    "                    .withColumn('Tip_amount',abs(yellow_DF.Tip_amount))\n",
    "print(\"---DONE---\")\n",
    "#calculate_bounds(yellow_DF)\n",
    "yellow_DF.where(col('Trip_distance')<0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-c9779fd6199c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhandle_no_passenger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-c9779fd6199c>\u001b[0m in \u001b[0;36mhandle_no_passenger\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#We create two dataframes, one with only the trips with no passengers, the other with passengers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mno_pass_condition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passenger_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdf_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_pass_condition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'passenger_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1304\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# handle negative values\n",
    "def handle_no_passenger(df):\n",
    "    #We create two dataframes, one with only the trips with no passengers, the other with passengers\n",
    "    no_pass_condition = df[df['passenger_count'] == 0].index\n",
    "    df_full = df.drop(no_pass_condition)\n",
    "    df_empty = df[df['passenger_count']==0]\n",
    "    \n",
    "    return df_empty, df_full\n",
    "handle_no_passenger(yellow_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|Passenger_count|Trip_distance|PULocationID|DOLocationID|Tip_amount|Total_amount|\n",
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+\n",
      "|2016-07-24 16:37:56|2016-07-24 16:48:14|            1.0|         2.54|          66|         144|      2.26|       13.56|\n",
      "|2016-07-20 07:46:43|2016-07-20 07:59:04|            1.0|         2.59|         152|          75|       0.0|        11.8|\n",
      "|2016-07-26 22:15:21|2016-07-26 22:40:10|            1.0|         8.39|          42|          79|       0.0|        27.8|\n",
      "|2016-07-01 16:44:20|2016-07-01 16:47:47|            1.0|         0.58|         145|         145|       0.0|         6.3|\n",
      "|2016-07-23 02:52:49|2016-07-23 03:02:13|            1.0|         2.44|          41|         244|      3.39|       14.69|\n",
      "|2016-07-28 19:00:04|2016-07-28 19:34:02|            1.0|         7.95|         244|         186|       0.0|        31.8|\n",
      "|2016-07-29 18:41:41|2016-07-29 19:13:42|            1.0|        11.46|          49|         132|      7.36|       44.16|\n",
      "|2016-07-25 16:12:11|2016-07-25 16:21:16|            1.0|          1.1|          75|         263|      1.65|        9.95|\n",
      "|2016-07-12 15:06:18|2016-07-12 15:27:47|            6.0|         3.33|         129|         226|       0.0|        17.3|\n",
      "|2016-07-27 15:33:06|2016-07-27 15:39:26|            1.0|         0.33|          43|          75|       0.9|         7.2|\n",
      "|2016-07-15 10:42:37|2016-07-15 11:06:58|            1.0|         3.33|          66|         234|      4.58|       22.88|\n",
      "|2016-07-26 11:54:19|2016-07-26 12:07:43|            1.0|         1.51|          97|          65|      2.16|       12.96|\n",
      "|2016-07-06 08:24:07|2016-07-06 08:33:35|            1.0|         1.43|          75|         263|       1.7|        11.0|\n",
      "|2016-07-17 15:18:29|2016-07-17 15:35:45|            1.0|          2.9|          42|         244|       1.0|        15.8|\n",
      "|2016-07-16 03:29:35|2016-07-16 03:41:22|            1.0|         2.11|         256|          37|      2.95|       14.75|\n",
      "|2016-07-08 22:14:24|2016-07-08 22:32:46|            1.0|         5.47|          74|         248|       0.0|        20.8|\n",
      "|2016-07-24 01:38:13|2016-07-24 01:58:55|            1.0|          4.0|         255|         113|      3.56|       21.36|\n",
      "|2016-10-16 12:10:24|2016-10-16 12:25:32|            1.0|          2.6|          65|         211|      2.75|       16.55|\n",
      "|2016-10-21 20:18:12|2016-10-21 20:22:43|            1.0|         0.84|          56|         173|       0.0|         6.3|\n",
      "|2016-10-05 13:52:37|2016-10-05 14:00:05|            1.0|          1.8|         225|          17|       0.2|         9.0|\n",
      "+-------------------+-------------------+---------------+-------------+------------+------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of records at this stage: (63281, 8)\n"
     ]
    }
   ],
   "source": [
    "green_DF_empty = green_DF[green_DF['passenger_count']!=0]\n",
    "green_DF_empty.show()\n",
    "print(\"Number of records at this stage:\",(green_DF_empty.count(), len(green_DF_empty.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Yellow taxi records are records that record trip information of New York's famous yellow\n",
    "taxi cars.\n",
    "\n",
    "• Green taxi records are records that record trip information by so-called 'boro' taxis a\n",
    "newer service introduced in August of 2013 to improve taxi service and availability in the\n",
    "boroughs\n",
    "\n",
    "• FHV records (short for 'For Hire Vehicles') record information from services that o\u001ber\n",
    "for-hire vehicles (such as Uber, Lyft, Via, and Juno), but also luxury limousine bases.\n",
    "\n",
    "• High volume FHV (FHVHV for short) are FHV records o\u001bered by services that make\n",
    "more than 10,000 trips per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime||\n",
    "|PULocationID|Zone in which the trip began|Integer|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer||\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','PULocationID'. Therefore, we want these columns to be not null and under good format.\n",
    "The column 'dropoff_datetime' will aslo be used. But as there is a substantial gap in values (between 2015-01 and 2016-12), we decided to remove the constraint 'Not Null' for that one.\n",
    "\n",
    "Action to be taken on fhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (360447, 3)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (233412, 3)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (233412, 3)\n",
      "-Changing DataTypes\n",
      "---DONE---\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (233412, 3)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID']\n",
    "list_files_fhv = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"fhv\",list_files_fhv)\n",
    "fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhv))\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhv_DF= fhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhv_DF=fhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "fhv_DF= fhv_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhv_DF= fhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhv_DF = fhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhv_DF = fhv_DF.withColumn(\"dropoff_datetime\",fhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime (as we there is a substantial gap in value for dropoff_datetime) )\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2015-01-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhv_DF = fhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\n",
    "#                    .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhv_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-66851238e5b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfhv_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhv_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/cleaned/fhv/fhv_cleaned.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "fhv_DF = fhv_DF.toPandas().to_csv('data/cleaned/fhv/fhv_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Hvfhs_license_num|TLC license number of the HVFHS base or business|String||\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime|Not Null|\n",
    "|PULocationID|Zone in which the trip began|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer(smallint)|Not Null|\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| \n",
    "\n",
    "\n",
    "*Hvfhs_license_num possible values:\n",
    "• HV0002: Juno\n",
    "• HV0003: Uber\n",
    "• HV0004: Via\n",
    "• HV0005: Lyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID',DULocationID. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on fhvhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (321819, 7)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (321819, 4)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (311874, 4)\n",
      "-Changing DataTypes\n",
      "---DONE---\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (311867, 4)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "list_files_fhvhv = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"fhvhv\",list_files_fhvhv)\n",
    "fhvhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhvhv))\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhvhv_DF= fhvhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhvhv_DF=fhvhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "fhvhv_DF= fhvhv_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhvhv_DF= fhvhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhvhv_DF = fhvhv_DF.withColumn(\"dropoff_datetime\",fhvhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2019-02-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhvhv_DF = fhvhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhvhv_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bb02bb400cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfhvhv_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhvhv_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/cleaned/fhvhv/fhvhv_cleaned.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "fhvhv_DF = fhvhv_DF.toPandas().to_csv('data/cleaned/fhvhv/fhvhv_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)||\n",
    "|pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|Not Null|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|Not Null|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal||\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal||\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|Not Null|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|Not Null|\n",
    "|Trip_type|A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver|Boolean|1 or 2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','Passenger_count','Trip_distance','Tip_amount','Total_amount'. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on green files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency\n",
    "- check and remove outliers in numeric columns\n",
    "- change negative values into positive one\n",
    "- remove trip with no passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (154014, 20)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (63647, 8)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (63330, 8)\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (63329, 8)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Passenger_count: double (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here all columns that must be NOT NULL\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','Passenger_count','Trip_distance','Tip_amount','Total_amount']\n",
    "list_files_green = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"green\",list_files_green)\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_green))\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "green_DF= green_DF.select('pickup_datetime','dropoff_datetime','Passenger_count','Trip_distance','PULocationID','DOLocationID','Tip_amount','Total_amount')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "green_DF=green_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "green_DF= green_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "green_DF= green_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "green_DF = green_DF.where(col(\"PULocationid\").isin(locID))\n",
    "green_DF = green_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2013-08-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "green_DF = green_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "green_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Removing rows with no passenger\n",
      "---DONE---\n",
      "Number of records at this stage: (63281, 8)\n",
      "-Removing rows with no passenger\n",
      "---DONE---\n",
      "Number of records at this stage: (63281, 8)\n",
      "-Changing negative values into positive ones\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'Trip_distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-ec54a1366b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#negative values into positive ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-Changing negative values into positive ones\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgreen_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreen_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trip_distance'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrip_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total_amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTotal_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tip_amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myellow_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTip_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Trip_distance'"
     ]
    }
   ],
   "source": [
    "#remove trip with no passenger\n",
    "print(\"-Removing rows with no passenger\")\n",
    "green_DF = green_DF[green_DF['passenger_count']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove outliers\n",
    "print(\"-Removing rows with no passenger\")\n",
    "\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "\n",
    "#negative values into positive ones\n",
    "print(\"-Changing negative values into positive ones\")\n",
    "green_DF = green_DF.withColumn('Total_amount',abs(yellow_DF.Total_amount))\n",
    "\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_DF = green_DF.withColumn('Total_amount',abs(yellow_DF.Total_amount))\n",
    "yellow_DF.where(col('Trip_distance')<0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_DF = green_DF.toPandas().to_csv('data/cleaned/green/green_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)|1 or 2, Not Null|\n",
    "|lpep_pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|lpep_dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|???|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|   \t|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N, Not Null|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal|Not Null|\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal|   \t|\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|   \t|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','Passenger_count','Trip_distance','Tip_amount','Total_amount'. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on yellow files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency\n",
    "- check and remove outliers in numeric columns\n",
    "- change negative values into positive one\n",
    "- remove trip with no passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records at this stage: (154014, 20)\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values\n",
      "---DONE---\n",
      "Number of records at this stage: (63647, 8)\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "Number of records at this stage: (63330, 8)\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "Number of records at this stage: (63330, 8)\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Passenger_count: double (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here all columns that must be NOT NULL\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','Passenger_count','Trip_distance','Tip_amount','Total_amount']\n",
    "list_files_yellow = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"green\",list_files_yellow)\n",
    "yellow_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_yellow))\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "yellow_DF= yellow_DF.select('pickup_datetime','dropoff_datetime','Passenger_count','Trip_distance','PULocationID','DOLocationID','Tip_amount','Total_amount')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "yellow_DF=yellow_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values\")\n",
    "yellow_DF= yellow_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "yellow_DF= yellow_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "yellow_DF = yellow_DF.where(col(\"PULocationid\").isin(locID))\n",
    "yellow_DF = yellow_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2009-01-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "yellow_DF = yellow_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "yellow_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove trip with no passenger\n",
    "print(\"-Removing rows with no passenger\")\n",
    "yellow_DF = yellow_DF[yellow_DF['passenger_count']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "#remove outliers\n",
    "print(\"-Removing rows with no passenger\")\n",
    "\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Number of records at this stage:\",(yellow_DF.count(), len(yellow_DF.columns)))\n",
    "\n",
    "#negative values into positive ones\n",
    "print(\"-Changing negative values into positive ones\")\n",
    "yellow_DF = yellow_DF.withColumn('Trip_distance',abs(yellow_DF.Trip_distance))\\\n",
    "                    .withColumn('Total_amount',abs(yellow_DF.Total_amount))\\\n",
    "                    .withColumn('Tip_amount',abs(yellow_DF.Tip_amount))\n",
    "\n",
    "yellow_DF.where(col('Trip_distance')<0).show()\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------+-------------+------------+------------+----------+------------+\n",
      "|pickup_datetime|dropoff_datetime|Passenger_count|Trip_distance|PULocationID|DOLocationID|Tip_amount|Total_amount|\n",
      "+---------------+----------------+---------------+-------------+------------+------------+----------+------------+\n",
      "+---------------+----------------+---------------+-------------+------------+------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_DF = green_DF.withColumn('Trip_distance',abs(yellow_DF.Trip_distance))\\\n",
    "                    .withColumn('Total_amount',abs(yellow_DF.Total_amount))\\\n",
    "                    .withColumn('Tip_amount',abs(yellow_DF.Tip_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_DF = yellow_DF.toPandas().to_csv('data/cleaned/yellow/yellow_cleaned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
