{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import Row\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Create list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_list(path, brand, list_files):\n",
    "    \"\"\"\n",
    "    This function create the files list of specify taxi brand (brand) from the specify folder (path). \n",
    "    \n",
    "    Input: the path where are the files -> /data/cleaned or data/sampled\n",
    "           the name of the taxi company -> fhv, fhvfh, green, yellow\n",
    "           the empty file name list in which each file will be append\n",
    "    Output: number of files in the list and the list of files name.\n",
    "    \"\"\"  \n",
    "    global nb_files\n",
    "    nb_files = 0\n",
    "    for file in glob.glob(\"%s/%s/*.csv\" %(path,brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files.append(file)\n",
    "        # Order by date the file list\n",
    "        list_files.sort()\n",
    "\n",
    "    return list_files, nb_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicates(df):\n",
    "    df.groupBy(df.columns)\\\n",
    "    .count()\\\n",
    "    .where(f.col('count')>1)\\\n",
    "    .select(f.sum('count'))\\\n",
    "    .show()\n",
    "\n",
    "def drop_duplicates(df,column_list):   \n",
    "    df.dropDuplicates([column_list]).show()\n",
    "    df.collect()        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: get columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(df, col_type=\"relevant\"):\n",
    "    cols = []\n",
    "    dfcols = list(df.columns)\n",
    "    if col_type == \"relevant\":\n",
    "        subs_to_check = ['time', 'location', 'passenger','distance', \n",
    "                         'ratecode', 'fare', \"longitude\", \"latitude\"]\n",
    "        for sub in subs_to_check:\n",
    "            for col in dfcols:\n",
    "                if sub.lower() in col.lower():\n",
    "                    cols.append(col)\n",
    "    \n",
    "    elif col_type == \"geolocation\":\n",
    "        subs_to_check = [\"location\", \"longitude\", \"latitude\"]\n",
    "        for sub in subs_to_check:\n",
    "            for col in dfcols:\n",
    "                if sub.lower() in col.lower():\n",
    "                    cols.append(col)\n",
    "    return cols\n",
    "        \n",
    "    return list_files, nb_files\n",
    "\n",
    "#df = pd.read_csv('data/cleaned/fhv/fhv_tripdata_2020-06.csv')\n",
    "#get_columns(df,col_type=\"geolocation\")\n",
    "#\n",
    "#df = pd.read_csv('data/cleaned/fhv/fhv_tripdata_2020-06.csv')\n",
    "#get_columns(df,col_type=\"relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: remove nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(df):\n",
    "    df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "    print(df.shape)\n",
    "    df = df[(df != 0).all(1)]\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: fix datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_datatypes(df):\n",
    "    date_columns = ['pickup_datetime', 'dropoff_datetime']\n",
    "    numeric_columns = ['passenger_count', 'trip_distance', 'fare_amount']\n",
    "    df[date_columns] = df[date_columns].apply(pd.to_datetime)\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds(df):\n",
    "    bounds = {\n",
    "        c: dict(\n",
    "            zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "        )\n",
    "        for c,d in zip(df.columns, df.dtypes) if d[1] == \"int\"\n",
    "    }\n",
    "\n",
    "    for c in bounds:\n",
    "        iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "        bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "        bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df, id_col):\n",
    "    bounds = calculate_bounds(df)\n",
    "    outliers = {}\n",
    "\n",
    "    return df.select(c, id_col,\n",
    "            *[\n",
    "                f.when(\n",
    "                    ~f.col(c).between(bounds[c]['min'], bounds[c]['max']),\n",
    "                    \"yes\"\n",
    "                ).otherwise(\"no\").alias(c+'_outlier')\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pulocationid': {'q1': 50.0, 'q3': 178.0, 'min': -142.0, 'max': 370.0},\n",
       " 'dolocationid': {'q1': 151.0, 'q3': 9999.0, 'min': -14621.0, 'max': 24771.0}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bounds(green_DF)\n",
    "#flag_outliers(green_DF,'Tip_amount')\n",
    "#green_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?. fonction to remove useless columns and create new file (last one it creates new files)\n",
    "    def drop_columns(input_file,output_file,cols_to_remove):\n",
    "    \"\"\"\n",
    "    This function removes all columns that were considered not useful for the analysis performed later on. \n",
    "    The idea is to have the dataset reduced to simplify our analysis.\n",
    "    \n",
    "    Input: the csv file to prepare and the name of the columns to drop      \n",
    "    Output: the csv file without the drop columns.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\") as source:\n",
    "        reader = csv.reader(source)\n",
    "        with open(output_file, \"w\", newline='') as result:\n",
    "            writer = csv.writer(result)\n",
    "            for row in reader:\n",
    "                row_count += 1\n",
    "                print('\\r{0}'.format(row_count), end='') # Print rows processed\n",
    "                for col_index in cols_to_remove:\n",
    "                    del row[col_index]\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 handle negative values\n",
    "def neg_val_treat(df, features):\n",
    "    \"\"\"\n",
    "    This function handles negative values for specific columns where we absolutely\n",
    "    do not want negative values. In this case, we want to drop the rows that are concerned by\n",
    "    such values. The features targeted are provided in a list of their names.\n",
    "    \n",
    "    Input: the dataframe to prepare and the name of the features to analyse\n",
    "        \n",
    "    Output: the dataframe without the rows for which any of the targeted features has\n",
    "    negative values.\n",
    "    \"\"\"\n",
    "    \n",
    "    #for each columns listed as an input, we drop the rows that have negative values\n",
    "    for feature in features:\n",
    "        neg_condition = df[df[feature] < 0].index\n",
    "        df = df.drop(neg_condition)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 calculate trip duration\n",
    "def trip_duration_calc(pu_time, do_time):\n",
    "    # computes the duration of the trip in seconds\n",
    "    trip_duration = do_time - pu_time\n",
    "    trip_duration = trip_duration/np.timedelta64(1,'s')\n",
    "    \n",
    "    return trip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. identify non consistent data\n",
    "#check consistency between total amount and all fees (yellow and green)\n",
    "#check consistency regarding datatype\n",
    "\n",
    "#4 fill in with data\n",
    "# for numeric or fees => mean of all the rest\n",
    "# for other => delete the row ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o443.csv.\n: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.String\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:42)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-bd0b43bf4263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 .csv([list_files_green]))\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#print('Proportion of trips witout any passenger')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#ratio = green_DF[green_DF['passenger_count']==0].shape[0]/green_DF.shape[0]*100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o443.csv.\n: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.String\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:42)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "list_files_green = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"green\",list_files_green)\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv([list_files_green]))\n",
    "\n",
    "#print('Proportion of trips witout any passenger')\n",
    "#ratio = green_DF[green_DF['passenger_count']==0].shape[0]/green_DF.shape[0]*100\n",
    "#print(\"{:.2f}\".format(ratio),'%')\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_no_passenger(df):\n",
    "    \"\"\"\n",
    "    This function splits the dataframe into two dataframes:\n",
    "    - one with only the trips with no passengers\n",
    "    - one with all the trips that have at least one passenger\n",
    "\n",
    "    Input: the dataframe to prepare\n",
    "        \n",
    "    Output: two dataframes, one with passengers, and the other one with only the 'empty trips'\n",
    "    \"\"\"\n",
    "    \n",
    "    #We create two dataframes, one with only the trips with no passengers, the other with passengers\n",
    "    no_pass_condition = df[df['passenger_count'] == 0].index\n",
    "    df_full = df.drop(no_pass_condition)\n",
    "    df_empty = df[df['passenger_count']==0]\n",
    "    \n",
    "    return df_empty, df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Yellow taxi records are records that record trip information of New York's famous yellow\n",
    "taxi cars.\n",
    "\n",
    "• Green taxi records are records that record trip information by so-called 'boro' taxis a\n",
    "newer service introduced in August of 2013 to improve taxi service and availability in the\n",
    "boroughs\n",
    "\n",
    "• FHV records (short for 'For Hire Vehicles') record information from services that o\u001ber\n",
    "for-hire vehicles (such as Uber, Lyft, Via, and Juno), but also luxury limousine bases.\n",
    "\n",
    "• High volume FHV (FHVHV for short) are FHV records o\u001bered by services that make\n",
    "more than 10,000 trips per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime|Not Null|\n",
    "|PULocationID|Zone in which the trip began|Integer|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer|Not Null|\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: string, PULocationID: double, DOLocationID: string, SR_Flag: string]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: string, PULocationID: double, DOLocationID: string, SR_Flag: string]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. adjust schema and datatypes\n",
    "2. remove useless columns \n",
    "3. remove rows containing null values for analysis central columns\n",
    "4. remove duplicate values\n",
    "5. check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before any cleaning step: (1389608, 6)\n",
      "Number of records after first cleaning step: (24625, 4)\n",
      "Number of records after second cleaning step: (24624, 4)\n",
      "Base schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n",
      "Number of records after LocId check and cleaning step: (3184, 4)\n"
     ]
    }
   ],
   "source": [
    "fhvSchema= StructType([\n",
    "    StructField(\"Dispatching_base_num\",StringType(),True),\n",
    "    StructField(\"Pickup_datetime\",TimestampType(),True),\n",
    "    StructField(\"DropOff_datetime\",TimestampType(),True),\n",
    "    StructField(\"PULocationID\",IntegerType(), True),\n",
    "    StructField(\"DOLocationID\",IntegerType(),True),\n",
    "    StructField(\"SR_Flag\",BooleanType(),True)\n",
    "])\n",
    "\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "list_files_fhv = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"fhv\",list_files_fhv)\n",
    "fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .schema(userDefinedSchema)\n",
    "                .csv(list_files_fhv))\n",
    "\n",
    "print(\"Number of records before any cleaning step:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "fhv_DF= fhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID')\n",
    "\n",
    "#remove null values\n",
    "fhv_DF= fhv_DF.dropna()\n",
    "\n",
    "print(\"Number of records after first cleaning step:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "fhv_DF= fhv_DF.dropDuplicates(column_list)\n",
    "\n",
    "print(\"Number of records after second cleaning step:\",(fhv_DF.count(), len(fhv_DF.columns)))\n",
    "\n",
    "print(\"Base schema:\")\n",
    "fhv_DF.printSchema()\n",
    "\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhv_DF = fhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "fhv_DF = fhv_DF.where(col(\"DOLocationid\").isin(locID)) \n",
    "print(\"Number of records after LocId check and cleaning step:\",(fhv_DF.count(), len(fhv_DF.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|Hvfhs_license_num|TLC license number of the HVFHS base or business|String|Not Null|\n",
    "|Dispatching_base_num|License Number of the base that dispatched the trip|String|Not Null|\n",
    "|Pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|DropOff_datetime|The date and time of the trip dropoff|Datetime|Not Null|\n",
    "|PULocationID|Zone in which the trip began|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer(smallint)|Not Null|\n",
    "|SR_Flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|Not Null| \n",
    "\n",
    "\n",
    "*Hvfhs_license_num possible values:\n",
    "• HV0002: Juno\n",
    "• HV0003: Uber\n",
    "• HV0004: Via\n",
    "• HV0005: Lyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. adjust schema and datatypes\n",
    "2. remove useless columns \n",
    "3. remove rows containing null values for analysis central columns\n",
    "4. remove duplicate values\n",
    "5. check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before any cleaning step: (321819, 7)\n",
      "Number of records after first cleaning step: (321819, 4)\n",
      "Number of records after second cleaning step: (321819, 4)\n",
      "Base schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n",
      "Number of records after LocId check and cleaning step: (311874, 4)\n"
     ]
    }
   ],
   "source": [
    "fhvhvSchema= StructType([\n",
    "    StructField(\"Hvfhs_license_num\",StringType(),True),\n",
    "    StructField(\"Dispatching_base_num\",StringType(),True),\n",
    "    StructField(\"Pickup_datetime\",TimestampType(),True),\n",
    "    StructField(\"DropOff_datetime\",TimestampType(),True),\n",
    "    StructField(\"PULocationID\",IntegerType(), True),\n",
    "    StructField(\"DOLocationID\",IntegerType(),True),\n",
    "    StructField(\"SR_Flag\",BooleanType(),True)\n",
    "])\n",
    "\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "list_files_fhvhv = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"fhvhv\",list_files_fhvhv)\n",
    "fhvhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .schema(fhvhvSchema)\n",
    "                .csv(list_files_fhvhv))\n",
    "\n",
    "print(\"Number of records before any cleaning step:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "fhvhv_DF= fhvhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID')\n",
    "\n",
    "#remove null values\n",
    "fhvhv_DF= fhvhv_DF.dropna()\n",
    "\n",
    "print(\"Number of records after first cleaning step:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "fhvhv_DF= fhvhv_DF.dropDuplicates(column_list)\n",
    "\n",
    "print(\"Number of records after second cleaning step:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))\n",
    "\n",
    "print(\"Base schema:\")\n",
    "fhvhv_DF.printSchema()\n",
    "\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"DOLocationid\").isin(locID)) \n",
    "print(\"Number of records after LocId check and cleaning step:\",(fhvhv_DF.count(), len(fhvhv_DF.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)|1 or 2, Not Null|\n",
    "|lpep_pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|lpep_dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|???|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|   \t|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N, Not Null|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal|Not Null|\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal|   \t|\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|   \t|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|   \t|\n",
    "|Trip_type|A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver|Boolean|1 or 2, Not Null|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records before any cleaning step: (1595304, 18)\n",
      "Number of records after first cleaning step: (0, 15)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"Total_amount\" among (pickup_datetime, dropoff_datetime, PULocationID, DOLocationID);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4060.dropDuplicates.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"Total_amount\" among (pickup_datetime, dropoff_datetime, PULocationID, DOLocationID);\n\tat org.apache.spark.sql.Dataset$$anonfun$41.apply(Dataset.scala:2405)\n\tat org.apache.spark.sql.Dataset$$anonfun$41.apply(Dataset.scala:2400)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n\tat org.apache.spark.sql.Dataset.dropDuplicates(Dataset.scala:2400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-270-0467ad3380cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#remove duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mfhvhv_DF\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfhvhv_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of records after second cleaning step:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreen_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdropDuplicates\u001b[0;34m(self, subset)\u001b[0m\n\u001b[1;32m   1576\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"Total_amount\" among (pickup_datetime, dropoff_datetime, PULocationID, DOLocationID);'"
     ]
    }
   ],
   "source": [
    "greenSchema= StructType([\n",
    "        StructField(\"VendorID\",IntegerType(),True),\n",
    "        StructField(\"pickup_datetime\",TimestampType(),True),\n",
    "        StructField(\"dropoff_datetime\",TimestampType(),True),\n",
    "        StructField(\"Passenger_count\",IntegerType(),True),\n",
    "        StructField(\"Trip_distance\",FloatType(),True),\n",
    "        StructField(\"PULocationID\",IntegerType(),True),\n",
    "        StructField(\"DOLocationID\",IntegerType(),True),\n",
    "        StructField(\"RateCodeID\",IntegerType(),True),\n",
    "        StructField(\"Store_and_fwd_flag\",BooleanType(),True),\n",
    "        StructField(\"Payment_type\",IntegerType(),True),\n",
    "        StructField(\"Fare_amount\",FloatType(),True),\n",
    "        StructField(\"Extra\",FloatType(),True), \n",
    "        StructField(\"MTA_tax\",FloatType(),True),\n",
    "        StructField(\"Improvement_surcharge\",FloatType(),True),\n",
    "        StructField(\"Tip_amount\",FloatType(),True),\n",
    "        StructField(\"Tolls_amount\",FloatType(),True),\n",
    "        StructField(\"Total_amount\",FloatType(),True),\n",
    "        StructField(\"Trip_type\",FloatType(),True)\n",
    "])\n",
    "                 \n",
    "column_list=['VendorID','pickup_datetime','dropoff_datetime','Passenger_count','Trip_distance','PULocationID','DOLocationID','RateCodeID','Store_and_fwd_flag','Payment_type','Fare_amount','Extra','MTA_tax','Improvement_surcharge','Tip_amount','Tolls_amount','Total_amount','Trip_type']\n",
    "list_files_fhvhv = []\n",
    "path=\"data/cleaned\"\n",
    "create_files_list(path,\"green\",list_files_green)\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .schema(greenSchema)\n",
    "                .csv(list_files_green))\n",
    "\n",
    "print(\"Number of records before any cleaning step:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove useless columns\n",
    "green_DF= green_DF.select('pickup_datetime','dropoff_datetime','Passenger_count','Trip_distance','PULocationID','DOLocationID','Payment_type','Fare_amount','Extra','MTA_tax','Improvement_surcharge','Tip_amount','Tolls_amount','Total_amount','Trip_type')\n",
    "\n",
    "#remove null values\n",
    "green_DF= green_DF.dropna()\n",
    "\n",
    "print(\"Number of records after first cleaning step:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "#remove duplicates\n",
    "fhvhv_DF= fhvhv_DF.dropDuplicates(column_list)\n",
    "\n",
    "print(\"Number of records after second cleaning step:\",(green_DF.count(), len(green_DF.columns)))\n",
    "\n",
    "print(\"Base schema:\")\n",
    "green_DF.printSchema()\n",
    "\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "green_DF = green_DF.where(col(\"PULocationid\").isin(locID))\n",
    "green_DF = green_DF.where(col(\"DOLocationid\").isin(locID)) \n",
    "print(\"Number of records after LocId check and cleaning step:\",(green_DF.count(), len(green_DF.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|VendorID|A code indicating the LPEP provider that provided the record.|Integer(tinyint)|1 or 2, Not Null|\n",
    "|lpep_pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|lpep_dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|Passenger_count|The number of passengers in the vehicle|Integer(tinyint)|???|\n",
    "|Trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|   \t|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|RateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N, Not Null|\n",
    "|Payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6, Not Null|\n",
    "|Fare_amount|The time-and-distance fare calculated by the meter|Decimal|Not Null|\n",
    "|Extra|Miscellaneous extras and surcharges|Decimal|   \t|\n",
    "|MTA_tax|0.50 MTA tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|Tip_amount|Tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|   \t|\n",
    "|Tolls_amount|Total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|Total_amount|The total amount charged to passengers|Decimal|   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "yellow_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(\"data/cleaned/yellow/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
