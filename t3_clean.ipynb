{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "# Imports go here\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os \n",
    "import shutil\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import Row\n",
    "from shutil import copyfile\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType, BooleanType, TimestampType, FloatType, DoubleType\n",
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=3g  pyspark-shell\"\n",
    "from pyspark.sql import SparkSession\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "# Create a new spark session (note, the * indicates to use all available CPU cores)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"H600 L-Group\") \\\n",
    "    .getOrCreate()\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext\n",
    "#in local mode, you will be able to access the Spark GUI at http://localhost:4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /home/bigdata/anaconda3/lib/python3.7/site-packages (0.8.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Yellow taxi records are records that record trip information of New York's famous yellow\n",
    "taxi cars.\n",
    "\n",
    "• Green taxi records are records that record trip information by so-called 'boro' taxis a\n",
    "newer service introduced in August of 2013 to improve taxi service and availability in the\n",
    "boroughs\n",
    "\n",
    "• FHV records (short for 'For Hire Vehicles') record information from services that o\u001ber\n",
    "for-hire vehicles (such as Uber, Lyft, Via, and Juno), but also luxury limousine bases.\n",
    "\n",
    "• High volume FHV (FHVHV for short) are FHV records o\u001bered by services that make\n",
    "more than 10,000 trips per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_list(path, brand, list_files):\n",
    "    global nb_files\n",
    "    nb_files = 0\n",
    "    for file in glob.glob(\"%s/%s/*.csv\" %(path,brand)):\n",
    "        nb_files = nb_files+1\n",
    "        # Save in list the files name\n",
    "        list_files.append(file)\n",
    "        # Order by date the file list\n",
    "        list_files.sort()\n",
    "\n",
    "    return list_files, nb_files\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "def total_amount_comp(df):\n",
    "    df = df.withColumn('total_amount', f.when(col('total_amount')==0, col('fare_amount')+col('mta_tax')+col('tip_amount')+0.29).otherwise('total_amount'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the FHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|pickup_datetime|The date and time of the trip pick-up|Datetime||\n",
    "|dropoff_datetime|The date and time of the trip dropoff|Datetime||\n",
    "|PULocationID|Zone in which the trip began|Integer|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer||\n",
    "|sr_flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'PULocationID'. Therefore, we want that column to be not null and under good format.\n",
    "The columns 'pickup-datetime' and 'dropoff_datetime' will aslo be used. But as there is a substantial gap in values (between 2015-01 and 2016-12) for the second one, we decided to remove the constraint 'Not Null' for that one.\n",
    "\n",
    "Action to be taken on fhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Number of records at this stage: 1389608 6\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values (PULocationID)\n",
      "---DONE---\n",
      "2.Number of records at this stage: 1107322 3\n",
      "-Removing duplicate values\n",
      "---DONE---\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID']\n",
    "list_files_fhv = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"fhv\",list_files_fhv)\n",
    "fhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhv))\n",
    "\n",
    "nb_r_step1=fhv_DF.count()\n",
    "nb_c_step1=len(fhv_DF.columns)\n",
    "print(\"1.Number of records at this stage:\",nb_r_step1, nb_c_step1)\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhv_DF= fhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhv_DF=fhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values (PULocationID)\")\n",
    "#fhv_DF= fhv_DF.na.drop()\n",
    "fhv_DF = fhv_DF.filter(fhv_DF.PULocationID. isNotNull())\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step2=fhv_DF.count()\n",
    "nb_c_step2=len(fhv_DF.columns)\n",
    "print(\"2.Number of records at this stage:\",nb_r_step2, nb_c_step2)\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhv_DF= fhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step3=fhv_DF.count()\n",
    "nb_c_step3=len(fhv_DF.columns)\n",
    "print(\"3.Number of records at this stage:\",nb_r_step3, nb_c_step3)\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhv_DF = fhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step4=fhv_DF.count()\n",
    "nb_c_step4=len(fhv_DF.columns)\n",
    "print(\"4.Number of records at this stage:\",nb_r_step4, nb_c_step4)\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhv_DF = fhv_DF.withColumn(\"dropoff_datetime\",fhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime (as we there is a substantial gap in value for dropoff_datetime) )\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2015-01-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhv_DF = fhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\n",
    "#                    .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step5=fhv_DF.count()\n",
    "nb_c_step5=len(fhv_DF.columns)\n",
    "print(\"5.Number of records at this stage:\",nb_r_step5, nb_c_step5)\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhv_DF.printSchema()\n",
    "\n",
    "print(tabulate([\n",
    "    ['Removing useless columns', 0,nb_c_step2-nb_c_step1],\n",
    "    ['Removing rows with null values', nb_r_step2-nb_r_step1,0],\n",
    "    ['Removing duplicate values', nb_r_step3-nb_r_step2,0],\n",
    "    ['Removing rows with wrong locID', nb_r_step4-nb_r_step3,0],\n",
    "    ['Removing rows with wrong dates', nb_r_step5-nb_r_step4,0]\n",
    "], \n",
    "               \n",
    "               headers=['Actions', 'Rows','Columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_DF = fhv_DF.toPandas().to_csv('data/cleaned/fhv/fhv_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the FHVHV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|hvfhs_license_num|TLC license number of the HVFHS base or business|String||\n",
    "|dispatching_base_num|License Number of the base that dispatched the trip|String||\n",
    "|pickup_datetime|The date and time of the trip pick-up|Datetime|Not Null|\n",
    "|dropoff_datetime|The date and time of the trip dropoff|Datetime|Not Null|\n",
    "|PULocationID|Zone in which the trip began|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the trip ended|Integer(smallint)|Not Null|\n",
    "|sr_flag|Indicates if the trip was a part of a shared ride chain offered by a High Volume FHV company (e.g. Uber Pool, Lyft Line); share=1, nonshared=0|Boolean|| \n",
    "\n",
    "\n",
    "*hvfhs_license_num possible values:\n",
    "• HV0002: Juno\n",
    "• HV0003: Uber\n",
    "• HV0004: Via\n",
    "• HV0005: Lyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID',DULocationID. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on fhvhv files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Number of records at this stage: 321819 7\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values (PULocationID)\n",
      "---DONE---\n",
      "2.Number of records at this stage: 321819 4\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "3.Number of records at this stage: 321819 4\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "4.Number of records at this stage: 153367 8\n",
      "-Changing DataTypes\n",
      "---DONE---\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "5.Number of records at this stage: 311867 4\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      "\n",
      "Actions                           Rows    Columns\n",
      "------------------------------  ------  ---------\n",
      "Removing useless columns             0         -3\n",
      "Removing rows with null values       0          0\n",
      "Removing duplicate values            0          0\n",
      "Removing rows with wrong locID   -9945          0\n",
      "Removing rows with wrong dates      -7          0\n"
     ]
    }
   ],
   "source": [
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID']\n",
    "list_files_fhvhv = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"fhvhv\",list_files_fhvhv)\n",
    "fhvhv_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_fhvhv))\n",
    "\n",
    "nb_r_step1=fhvhv_DF.count()\n",
    "nb_c_step1=len(fhvhv_DF.columns)\n",
    "print(\"1.Number of records at this stage:\",nb_r_step1, nb_c_step1)\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "fhvhv_DF= fhvhv_DF.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "fhvhv_DF=fhvhv_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values (PULocationID)\")\n",
    "fhvhv_DF = fhvhv_DF.filter(fhvhv_DF.PULocationID. isNotNull())\n",
    "#fhvhv_DF= fhvhv_DF.na.drop()\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step2=fhvhv_DF.count()\n",
    "nb_c_step2=len(fhvhv_DF.columns)\n",
    "print(\"2.Number of records at this stage:\",nb_r_step2, nb_c_step2)\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "fhvhv_DF= fhvhv_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step3=fhvhv_DF.count()\n",
    "nb_c_step3=len(fhvhv_DF.columns)\n",
    "print(\"3.Number of records at this stage:\",nb_r_step3, nb_c_step3)\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"PULocationid\").isin(locID))\n",
    "fhvhv_DF = fhvhv_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step4=fhvhv_DF.count()\n",
    "nb_c_step4=len(fhvhv_DF.columns)\n",
    "print(\"4.Number of records at this stage:\",nb_r_step5, nb_c_step5)\n",
    "\n",
    "print(\"-Changing DataTypes\")\n",
    "fhvhv_DF = fhvhv_DF.withColumn(\"dropoff_datetime\",fhvhv_DF[\"dropoff_datetime\"].cast(TimestampType()))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2019-02-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "fhvhv_DF = fhvhv_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step5=fhvhv_DF.count()\n",
    "nb_c_step5=len(fhvhv_DF.columns)\n",
    "print(\"5.Number of records at this stage:\",nb_r_step5, nb_c_step5)\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "fhvhv_DF.printSchema()\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([\n",
    "    ['Removing useless columns', 0,nb_c_step2-nb_c_step1],\n",
    "    ['Removing rows with null values', nb_r_step2-nb_r_step1,0],\n",
    "    ['Removing duplicate values', nb_r_step3-nb_r_step2,0],\n",
    "    ['Removing rows with wrong locID', nb_r_step4-nb_r_step3,0],\n",
    "    ['Removing rows with wrong dates', nb_r_step5-nb_r_step4,0]\n",
    "], \n",
    "               \n",
    "               headers=['Actions', 'Rows','Columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bb02bb400cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfhvhv_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhvhv_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/cleaned/fhvhv/fhvhv_cleaned.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "fhvhv_DF = fhvhv_DF.toPandas().to_csv('data/cleaned/fhvhv/fhvhv_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the GREEN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|vendorid|A code indicating the LPEP provider that provided the record.|Integer(tinyint)||\n",
    "|pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|dropoff_datetime|The date and time when the meter was disengaged|Datetime|Not Null|\n",
    "|passenger_count|The number of passengers in the vehicle|Integer(tinyint)|Not Null|\n",
    "|trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|Not Null|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)|Not Null|\n",
    "|rateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6|\n",
    "|store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N|\n",
    "|payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6|\n",
    "|fare_amount|The time-and-distance fare calculated by the meter|Decimal||\n",
    "|extra|Miscellaneous extras and surcharges|Decimal||\n",
    "|mta_tax|0.50 mta tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|tip_amount|tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|Not Null|\n",
    "|tolls_amount|total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|total_amount|The total amount charged to passengers|Decimal|Not Null|\n",
    "|trip_type|A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver|Boolean|1 or 2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount','total_amount'. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on green files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency\n",
    "- check and remove outliers in numeric columns\n",
    "- change negative values into positive one\n",
    "- remove trip with no passenger\n",
    "- compute total_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Number of records at this stage: 154014 20\n",
      "-Computing total_amount\n",
      "---DONE---\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values(PULocationID & trip_distance)\n",
      "---DONE---\n",
      "2.Number of records at this stage: 153872 8\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "3.Number of records at this stage: 153872 8\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "4.Number of records at this stage: 153368 8\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "5.Number of records at this stage: 153367 8\n",
      "-Removing rows with no passenger\n",
      "---DONE---\n",
      "6.Number of records at this stage: 152785 8\n",
      "-Removing rows with 0 as trip distance\n",
      "---DONE---\n",
      "7.Number of records at this stage: 150756 8\n",
      "-Changing negative values into positive ones\n",
      "---DONE---\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "Actions                                  Rows    Columns\n",
      "-------------------------------------  ------  ---------\n",
      "Removing useless columns                    0        -12\n",
      "Removing rows with null values           -142          0\n",
      "Removing duplicate values                   0          0\n",
      "Removing rows with wrong locID           -504          0\n",
      "Removing rows with wrong dates             -1          0\n",
      "Removing rows with no passenger          -582          0\n",
      "Removing rows with 0 as trip distance   -2029          0\n"
     ]
    }
   ],
   "source": [
    "#here all columns that must be NOT NULL\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount','total_amount']\n",
    "list_files_green = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"green\",list_files_green)\n",
    "green_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_green))\n",
    "\n",
    "nb_r_step1=green_DF.count()\n",
    "nb_c_step1=len(green_DF.columns)\n",
    "print(\"1.Number of records at this stage:\",nb_r_step1, nb_c_step1)\n",
    "\n",
    "#compute total amount\n",
    "print(\"-Computing total_amount\")\n",
    "total_amount_comp(green_DF)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "green_DF= green_DF.select('pickup_datetime','dropoff_datetime','passenger_count','trip_distance','PULocationID','DOLocationID','tip_amount','total_amount')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "green_DF=green_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values(PULocationID & trip_distance)\")\n",
    "#green_DF= green_DF.na.drop(\"pickup_datetime\")\n",
    "green_DF = green_DF.filter(green_DF.PULocationID. isNotNull())\n",
    "green_DF = green_DF.filter(green_DF.trip_distance. isNotNull())\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step2=green_DF.count()\n",
    "nb_c_step2=len(green_DF.columns)\n",
    "print(\"2.Number of records at this stage:\",nb_r_step2, nb_c_step2)\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "green_DF= green_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step3=green_DF.count()\n",
    "nb_c_step3=len(green_DF.columns)\n",
    "print(\"3.Number of records at this stage:\",nb_r_step3, nb_c_step3)\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "green_DF = green_DF.where(col(\"PULocationID\").isin(locID))\n",
    "#green_DF = green_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step4=green_DF.count()\n",
    "nb_c_step4=len(green_DF.columns)\n",
    "print(\"4.Number of records at this stage:\",nb_r_step4, nb_c_step4)\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2013-08-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "green_DF = green_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step5=green_DF.count()\n",
    "nb_c_step5=len(green_DF.columns)\n",
    "print(\"5.Number of records at this stage:\",nb_r_step5, nb_c_step5)\n",
    "\n",
    "#remove trip with no passenger\n",
    "print(\"-Removing rows with no passenger\")\n",
    "green_DF = green_DF[green_DF['passenger_count']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step6=green_DF.count()\n",
    "nb_c_step6=len(green_DF.columns)\n",
    "print(\"6.Number of records at this stage:\",nb_r_step6, nb_c_step6)\n",
    "\n",
    "#remove trip with 0 as trip distance\n",
    "print(\"-Removing rows with 0 as trip distance\")\n",
    "green_DF = green_DF[green_DF['trip_distance']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step7=green_DF.count()\n",
    "nb_c_step7=len(green_DF.columns)\n",
    "print(\"7.Number of records at this stage:\",nb_r_step7, nb_c_step7)\n",
    "\n",
    "#absolute value for negative value\n",
    "print(\"-Changing negative values into positive ones\")\n",
    "from  pyspark.sql.functions import abs\n",
    "green_DF = green_DF.withColumn('passenger_count',abs(green_DF.passenger_count))\n",
    "green_DF = green_DF.withColumn('trip_distance',abs(green_DF.trip_distance))\n",
    "green_DF = green_DF.withColumn('tip_amount',abs(green_DF.tip_amount))\n",
    "green_DF = green_DF.withColumn('total_amount',abs(green_DF.total_amount))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "green_DF.printSchema()\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([\n",
    "    ['Removing useless columns', 0,nb_c_step2-nb_c_step1],\n",
    "    ['Removing rows with null values', nb_r_step2-nb_r_step1,0],\n",
    "    ['Removing duplicate values', nb_r_step3-nb_r_step2,0],\n",
    "    ['Removing rows with wrong locID', nb_r_step4-nb_r_step3,0],\n",
    "    ['Removing rows with wrong dates', nb_r_step5-nb_r_step4,0],\n",
    "    ['Removing rows with no passenger', nb_r_step6-nb_r_step5,0],\n",
    "    ['Removing rows with 0 as trip distance', nb_r_step7-nb_r_step6,0], \n",
    "], \n",
    "               \n",
    "               headers=['Actions', 'Rows','Columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_DF = green_DF.toPandas().to_csv('data/cleaned/green/green_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the YELLOW dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of valid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Column Value|Description|Data Type|Constraints|\n",
    "|---\t|---\t|---\t|---\t|\n",
    "|vendorid|A code indicating the LPEP provider that provided the record.|Integer(tinyint)|1 or 2|\n",
    "|pickup_datetime|The date and time when the meter was engaged|Datetime|Not Null|\n",
    "|dropoff_datetime|The date and time when the meter was disengaged|Datetime||\n",
    "|passenger_count|The number of passengers in the vehicle|Integer(tinyint)|Not Null|\n",
    "|trip_distance|The elapsed trip distance in miles reported by the taximeter|Decimal|   \t|\n",
    "|PULocationID|Zone in which the taximeter was engaged|Integer(smallint)|Not Null|\n",
    "|DOLocationID|Zone in which the taximeter was disengaged|Integer(smallint)||\n",
    "|rateCodeID|The final rate code in effect at the end of the trip|Integer(tinyint)|1 to 6|\n",
    "|store_and_fwd_flag|This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,”because the vehicle did not have a connection to the server|Boolean|Y or N|\n",
    "|payment_type|A numeric code signifying how the passenger paid for the trip|Integer(tinyint)|1 to 6|\n",
    "|fare_amount|The time-and-distance fare calculated by the meter|Decimal||\n",
    "|extra|Miscellaneous extras and surcharges|Decimal|   \t|\n",
    "|mta_tax|0.50 mta tax that is automatically triggered based on the metered rate in use|Decimal|   \t|\n",
    "|Improvement_surcharge|0.30 improvement surcharge assessed on hailed trips at the flag drop|Decimal|   \t|\n",
    "|tip_amount|tip amount – This field is automatically populated for credit card tips. Cash tips are not included|Decimal|   \t|\n",
    "|tolls_amount|total amount of all tolls paid in trip|Decimal|   \t|\n",
    "|total_amount|The total amount charged to passengers|Decimal|   \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T4, the analysis will rely mainly on 'pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount','total_amount'. Therefore, we want these columns to be not null and under good format.\n",
    "\n",
    "Action to be taken on yellow files:\n",
    "- adjust schema and datatypes\n",
    "- remove useless columns \n",
    "- remove rows containing null values for analysis central columns\n",
    "- remove duplicate values\n",
    "- check locationID consistency\n",
    "- check and remove outliers in numeric columns\n",
    "- change negative values into positive one\n",
    "- remove trip with no passenger\n",
    "- compute total_amount\n",
    "- remove rows with trip_distance=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying dirty records, data repairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Number of records at this stage: 154014 20\n",
      "-Computing total_amount\n",
      "---DONE---\n",
      "-Removing useless columns\n",
      "---DONE---\n",
      "-Replacing null values by nothing\n",
      "---DONE---\n",
      "-Removing rows with null values (PULocationID & Trip_distance)\n",
      "---DONE---\n",
      "2.Number of records at this stage: 153872 8\n",
      "-Removing duplicate values\n",
      "---DONE---\n",
      "3.Number of records at this stage: 153872 8\n",
      "-Removing rows with wrong locID\n",
      "---DONE---\n",
      "4.Number of records at this stage: 153368 8\n",
      "-Removing rows with wrong dates\n",
      "---DONE---\n",
      "5.Number of records at this stage: 153368 8\n",
      "-Removing rows with no passenger\n",
      "---DONE---\n",
      "6.Number of records at this stage: 152786 8\n",
      "-Removing rows with 0 as trip distance\n",
      "---DONE---\n",
      "7.Number of records at this stage: 150757 8\n",
      "-Changing negative values into positive ones\n",
      "---DONE---\n",
      "Final Schema:\n",
      "root\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- Passenger_count: double (nullable = true)\n",
      " |-- Trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- Tip_amount: double (nullable = true)\n",
      " |-- Total_amount: double (nullable = true)\n",
      "\n",
      "Actions                                  Rows    Columns\n",
      "-------------------------------------  ------  ---------\n",
      "Removing useless columns                    0        -12\n",
      "Removing rows with null values           -142          0\n",
      "Removing duplicate values                   0          0\n",
      "Removing rows with wrong locID           -504          0\n",
      "Removing rows with wrong dates              0          0\n",
      "Removing rows with no passenger          -582          0\n",
      "Removing rows with 0 as trip distance   -2029          0\n"
     ]
    }
   ],
   "source": [
    "#here all columns that must be NOT NULL\n",
    "column_list=['pickup_datetime','dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','tip_amount','total_amount']\n",
    "list_files_yellow = []\n",
    "path=\"data/integrated\"\n",
    "create_files_list(path,\"green\",list_files_yellow)\n",
    "yellow_DF = (spark.read\n",
    "                .option(\"sep\", \",\")\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", True)\n",
    "                .csv(list_files_yellow))\n",
    "\n",
    "nb_r_step1=yellow_DF.count()\n",
    "nb_c_step1=len(yellow_DF.columns)\n",
    "print(\"1.Number of records at this stage:\",nb_r_step1, nb_c_step1)\n",
    "\n",
    "#compute total amount\n",
    "print(\"-Computing total_amount\")\n",
    "total_amount_comp(yellow_DF)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove useless columns\n",
    "print(\"-Removing useless columns\")\n",
    "yellow_DF= yellow_DF.select('pickup_datetime','dropoff_datetime','passenger_count','trip_distance','PULocationID','DOLocationID','tip_amount','total_amount')\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#replace 'null' by nothing\n",
    "print(\"-Replacing null values by nothing\")\n",
    "yellow_DF=yellow_DF.na.fill(\"\")\n",
    "print(\"---DONE---\")\n",
    "\n",
    "#remove null values\n",
    "print(\"-Removing rows with null values (PULocationID & trip_distance)\")\n",
    "#green_DF= green_DF.na.drop(\"pickup_datetime\")\n",
    "yellow_DF = yellow_DF.filter(yellow_DF.PULocationID. isNotNull())\n",
    "yellow_DF = yellow_DF.filter(yellow_DF.trip_distance. isNotNull())\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step2=yellow_DF.count()\n",
    "nb_c_step2=len(yellow_DF.columns)\n",
    "print(\"2.Number of records at this stage:\",nb_r_step2, nb_c_step2)\n",
    "\n",
    "#remove duplicates\n",
    "print(\"-Removing duplicate values\")\n",
    "yellow_DF= yellow_DF.dropDuplicates(column_list)\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step3=yellow_DF.count()\n",
    "nb_c_step3=len(yellow_DF.columns)\n",
    "print(\"3.Number of records at this stage:\",nb_r_step3, nb_c_step3)\n",
    "\n",
    "#remove wrong location id\n",
    "print(\"-Removing rows with wrong locID\")\n",
    "zones = gpd.read_file('data/metadata/taxi_zones.shp')\n",
    "zones_df = spark.createDataFrame(zones).cache()\n",
    "locID = zones_df.select(col(\"LocationID\"))\n",
    "locID = [row[0] for row in locID.select(\"LocationID\").collect()]\n",
    "yellow_DF = yellow_DF.where(col(\"PULocationid\").isin(locID))\n",
    "#yellow_DF = yellow_DF.where(col(\"DOLocationID\").isin(locID))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step4=yellow_DF.count()\n",
    "nb_c_step4=len(yellow_DF.columns)\n",
    "print(\"4.Number of records at this stage:\",nb_r_step4, nb_c_step4)\n",
    "\n",
    "#remove dates out of the range solely for pickup_datetime,dropoff_datetime\n",
    "print(\"-Removing rows with wrong dates\")\n",
    "date_in = pd.to_datetime('2009-01-01 00:00:00')\n",
    "date_out =  pd.to_datetime('2020-07-01 00:00:00' )\n",
    "yellow_DF = yellow_DF.filter(f.col(\"pickup_datetime\").between(date_in,date_out))\\\n",
    "                .filter(f.col(\"dropoff_datetime\").between(date_in,date_out))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step5=yellow_DF.count()\n",
    "nb_c_step5=len(yellow_DF.columns)\n",
    "print(\"5.Number of records at this stage:\",nb_r_step5, nb_c_step5)\n",
    "\n",
    "#remove trip with no passenger\n",
    "print(\"-Removing rows with no passenger\")\n",
    "yellow_DF = yellow_DF[yellow_DF['passenger_count']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step6=yellow_DF.count()\n",
    "nb_c_step6=len(yellow_DF.columns)\n",
    "print(\"6.Number of records at this stage:\",nb_r_step6, nb_c_step6)\n",
    "\n",
    "#remove trip with 0 as trip distance\n",
    "print(\"-Removing rows with 0 as trip distance\")\n",
    "yellow_DF = yellow_DF[yellow_DF['trip_distance']!=0]\n",
    "print(\"---DONE---\")\n",
    "\n",
    "nb_r_step7=yellow_DF.count()\n",
    "nb_c_step7=len(yellow_DF.columns)\n",
    "print(\"7.Number of records at this stage:\",nb_r_step7, nb_c_step7)\n",
    "\n",
    "#absolute value for negative value\n",
    "print(\"-Changing negative values into positive ones\")\n",
    "from  pyspark.sql.functions import abs\n",
    "yellow_DF = yellow_DF.withColumn('passenger_count',abs(yellow_DF.passenger_count))\n",
    "yellow_DF = yellow_DF.withColumn('trip_distance',abs(yellow_DF.trip_distance))\n",
    "yellow_DF = yellow_DF.withColumn('tip_amount',abs(yellow_DF.tip_amount))\n",
    "yellow_DF = yellow_DF.withColumn('total_amount',abs(yellow_DF.total_amount))\n",
    "print(\"---DONE---\")\n",
    "\n",
    "print(\"Final Schema:\")\n",
    "yellow_DF.printSchema()\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([\n",
    "    ['Removing useless columns', 0,nb_c_step2-nb_c_step1],\n",
    "    ['Removing rows with null values', nb_r_step2-nb_r_step1,0],\n",
    "    ['Removing duplicate values', nb_r_step3-nb_r_step2,0],\n",
    "    ['Removing rows with wrong locID', nb_r_step4-nb_r_step3,0],\n",
    "    ['Removing rows with wrong dates', nb_r_step5-nb_r_step4,0],\n",
    "    ['Removing rows with no passenger', nb_r_step6-nb_r_step5,0],\n",
    "    ['Removing rows with 0 as trip distance', nb_r_step7-nb_r_step6,0], \n",
    "], \n",
    "               \n",
    "               headers=['Actions', 'Rows','Columns']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
